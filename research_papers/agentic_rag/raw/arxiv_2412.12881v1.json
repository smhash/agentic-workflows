{
  "source": "arxiv",
  "query": "Agentic RAG",
  "fetched_at": "2025-11-21T17:17:57.827885",
  "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement",
  "url": "http://arxiv.org/abs/2412.12881v1",
  "content": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented\nVerification and Refinement\nJinhao Jiang1\u2217, Jiayi Chen2\u2217, Junyi Li4\u2217, Ruiyang Ren1, Shijie Wang3\nWayne Xin Zhao1\u2020, Yang Song5\u2020, Tao Zhang5\n1Gaoling School of Artificial Intelligence, Renmin University of China.\n2Wuhan University of Science and Technology.3Northeastern University at Qinhuangdao.\n4Department of Computer Science, National University of Singapore. 5BOSS Zhipin, Beijing, China.\njiangjinhao@ruc.edu.cn, batmanfly@gmail.com\nAbstract\nExisting large language models (LLMs) show\nexceptional problem-solving capabilities but\nmight struggle with complex reasoning tasks.\nDespite the successes of chain-of-thought and\ntree-based search methods, they mainly depend\non the internal knowledge of LLMs to search\nover intermediate reasoning steps, limited to\ndealing with simple tasks involving fewer rea-\nsoning steps. In this paper, we propose RAG-\nStar, a novel RAG approach that integrates the\nretrieved information to guide the tree-based\ndeliberative reasoning process that relies on\nthe inherent knowledge of LLMs. By lever-\naging Monte Carlo Tree Search, RAG-Star it-\neratively plans intermediate sub-queries and\nanswers for reasoning based on the LLM itself.\nTo consolidate internal and external knowledge,\nwe propose an retrieval-augmented verification\nthat utilizes query- and answer-aware reward\nmodeling to provide feedback for the inherent\nreasoning of LLMs. Our experiments involv-\ning Llama-3.1-8B-Instruct and GPT-4o demon-\nstrate that RAG-Star significantly outperforms\nprevious RAG and reasoning methods.\n1 Introduction\nDespite the excellent capabilities of large language\nmodels (LLMs) (Zhao et al., 2023b), they still face\nsignificant challenges in complex reasoning tasks\n(e.g., multi-hop question answering), which often\ngo beyond simple, single-step problem-solving,\ndemanding a deeper level of cognitive reasoning\nacross multiple facts, sources, or contexts (Huang\net al., 2024; Suzgun et al., 2023). Great efforts have\nbeen made to improve the reasoning effectiveness\nof LLMs by conducing step-by-step reasoning, ex-\nemplified by chain-of-thought (CoT) (Wei et al.,\n2022). However, as the number of reasoning steps\ngrows, LLMs are often prone to introduce logical\n\u2217 Equal contributions.\n\u2020 Corresponding author.\nerrors, factual hallucinations, or inconsistent state-\nments (Wei et al., 2022; Lyu et al., 2023).\nIn fact, step-by-step reasoning in the auto-\nregressive generation paradigm can be described\nas akin to \u201cSystem 1\u201d, a mode of thinking which is\nfast, instinctive but less accurate (Kahneman, 2011).\nConversely, solving complex reasoning problems\nrequires more in-depth, deliberative, and logical\nthinking, known as the \u201cSystem 2\u201d mode, which re-\nquires conscious effort to conduct massive strategic\ndecision-making (Kahneman, 2011). To enhance\nthe \u201cSystem 2\u201d reasoning capabilities of LLMs,\nprior studies have proposed to conduct deliberative\ngeneration by leveraging basic tree search algo-\nrithms (e.g., Monte Carlo Tree Search (Silver et al.,\n2017)). However, LLMs in these studies mainly\ndepend on their internal knowledge to search over\nintermediate reasoning steps, limited to handling\nproblems with relatively simple reasoning process.\nTo leverage external knowledge in model reasoning,\nextensive research has sought to augment LLMs\nwith external information sources (a.k.a. retrieval-\naugmented generation, RAG) (Lewis et al., 2020b;\nYao et al., 2022), while existing efforts mainly con-\nsider sequential reasoning structure, which cannot\nnaturally support more complex reasoning structure\nlike MCTS. Thus, we raise the following research\nquestion: Can RAG enhance the deliberative rea-\nsoning capabilities of LLMs?\nIn light of this, in this paper, we propose RAG-\nStar, a novel RAG-enhanced framework designed\nto improve multi-step reasoning capabilities of\nLLMs with deliberative planning. As the major\ntechnical contribution, RAG-Star can fully exploit\nthe internal knowledge of LLMs to plan the multi-\nstep reasoning, and meanwhile integrating the ex-\nternal retrieval to guide the internal reasoning pro-\ncess. To achieve this goal, we first introduce a\ntree-based search algorithm (i.e., Monte Carlo Tree\nSearch, MCTS) with LLMs to search over possible\nplans for solving the problem at hand where a com-\narXiv:2412.12881v1  [cs.CL]  17 Dec 2024\n\nplete plan is composed of a sequence of sub-queries\nand corresponding answers. Starting from the input\nquestion (root node), RAG-Star iteratively gener-\nates and selects an appropriate sub-query and its an-\nswer (intermediate node), which aims to maximally\nexplore the optimal sub-query path towards the fi-\nnal answer solely based on the inherent knowledge\nof LLMs. Second, different from existing delibera-\ntion methods (Wang et al., 2024a; Yao et al., 2023),\nRAG-Star proposes retrieval-augmented verifica-\ntion that involves both query- and answer-aware\nreward modeling, fully exploiting external sources\nto guide the internal deliberative reasoning. In our\napproach, instead of directly interfering in the rea-\nsoning process of LLMs, we consider employing\nRAG to refine the derived reasoning steps in MCTS,\nwhich can effectively reduce the conflicts between\ninherent and external knowledge, which has been\na common issue when using RAG methods (Wang\net al., 2024b; Gao et al., 2023).\nWe conduct extensive experiments to verify the\neffectiveness of RAG-Star based on Llama-3.1-8B-\nInstruct and GPT-4o. Our method outperforms the\nbaselines by up to 18.98% and 16.19% on average\nfor Llama-3.1-8B and GPT-4o, respectively.\nOur main contributions can be summarized as:\n\u2022 We propose RAG-Star that leverages external\nretrieval to enahnce the deliberative reasoning of\nLLMs based on their internal knowledge.\n\u2022 We design an effective retrieval-augmented\nverification and refinement to evaluate and correct\nthe inherent reasoning process.\n\u2022 We conduct extensive experiments on sev-\neral datasets, where RAG-Star significantly out-\nperforms existing RAG and reasoning methods.\n2 Related Work\nRetrieval-Augmented LLMs. Augmenting large\nlanguage models (LLMs) with retrieval has been\nextensively studied in existing literature (Lewis\net al., 2020a; Borgeaud et al., 2022; Guu et al.,\n2020), which incorporates a differentiable retriever\nto provide external sources for LLMs. Further-\nmore, LLMs have made significant advancements\nin many reasoning tasks, such as code genera-\ntion (OpenAI, 2023), math word problems (Zhu\net al., 2023) and question answering (Brown et al.,\n2020). Chain-of-thought (CoT) has been reported\nas an emergent ability of LLMs when they are large\nenough (Wei et al., 2022), which encourages LLMs\nto generate explicit intermediate reasoning steps\nin reasoning rather than simply providing answers\ndirectly. To elicit or improve the multi-step reason-\ning capability of LLMs, several approaches seek\nto harness the strengths of both CoT and retrieval\non knowledge-intensive complex reasoning tasks,\nsuch as multi-hop question answering (Yao et al.,\n2022; Zhao et al., 2023a). The rationales gained\nfrom reasoning enhance the retrieval of more rel-\nevant information, while the retrieved knowledge\nimproves the factuality of intermediate reasoning\nsteps. However, these approaches primarily take\nretrieved documents as direct input to the model,\neasily suffering from knowledge conflicts between\nthe parametric knowledge of LLMs and the exter-\nnal sources. In contrast, our RAG-Star framework\nintegrates tree-based search to fully explore the so-\nlution space and repurpose the retrieval information\nas external guidance to the reasoning process.\nEnhancing LLMs with Search. Applying search\non top of LLMs has been a topic of much inter-\nest. Several recent works have explored search\nalgorithms to improve the performance of LLMs\nduring the inference stage (Wang et al., 2024a;\nZhang et al., 2024). The bitter lesson (Sutton,\n2019) famously suggests that two forms of scal-\ning, i.e., learning and search, supersede all other\napproaches. Many studies have proven that scal-\ning the inference-time computation can lead to\nsubstantial improvements in the performance of\nLLMs without training (Brown et al., 2024; Snell\net al., 2024). These search algorithms, where\nmultiple branches of outcomes are explored dur-\ning search, have been widely applied in reinforce-\nment learning algorithms (Hart et al., 1968; Silver\net al., 2017) and many real-world applications such\nas AlphaGo (Silver et al., 2016) for their good\nexploration-exploitation trade-off. However, these\napproaches mainly rely on the internal knowledge\nof LLMs to search potential solutions, which might\nnot be optimal and leads to a amount of rollouts,\nsignificantly slowing down the decoding process.\nIn this paper, we leverage the external retrieval\nsources to enhance the deliberative search process\nwith LLMs, effectively differentiate the internal\nreasoning and external retrieval.\n3 Preliminary\nIn this section, we will first formally define our task\nand then introduce Monte Carlo Tree Search which\nis used in our proposed RAG-Star approach.\n\nTask Formulation. In this work, we mainly fo-\ncus on open-domain multi-hop question answer-\ning (Chen et al., 2019; Yang et al., 2018), which\nrequires multiple steps of reasoning across differ-\nent documents to answer questions. Previous work\ntypically adopts an iterative reason-then-generate\npipeline (Wei et al., 2022; Huang and Chang, 2023).\nAt each step, the LLM first infers an intermediate\nsub-query based on the current situation and then\ngenerates possible answers to the query. Formally,\ngiven a natural language input question, at the t-th\nstep, the LLM M\u03b8 (parameterized by \u03b8) first de-\nliberately reasons about a sub-query qt, followed\nby generating an answer at based on its inherent\nknowledge. In some literature (Yao et al., 2022;\nAsai et al., 2024), retrieval-augmented generation\n(RAG) has been employed to improve the factual-\nity of intermediate reasoning steps. For each sub-\nquery qt, the retriever retrieves top-K documents\nDt = {dt,k}K\nk=1 from an external large-scale cor-\npus, e.g., Wikipedia, supplying them to the LLM\nto generate more accurate answers.\nMonte Carlo Tree Search (MCTS).In existing lit-\nerature (Zelikman et al., 2024; Zhang et al., 2024),\nMCTS builds a search tree T based on a policy\nmodel \u03c0\u03b8, which is usually the target LLM M\u03b8.\nEach node st = [qt, at, N(st), V(st)] represents a\nstate comprising the sub-query qt, its answer at,\nthe number of visits N(st), and the value function\n(expected reward) V (st) for accurately answering\nquestions, except that the root node s0 = [q0] only\ncontains the original input question q0, and each\nedge is an action aiming to generate the next sub-\nquery. During the search process, MCTS runs for\nmultiple simulations. For the t-th simulation, it\nconducts four operations to expand the tree:\n\u2022 Selection aims to select a node with the highest\nUCT (Upper Confidence bounds applied to Trees)\nscore (Kocsis and Szepesv\u00e1ri, 2006) starting from\nthe root node s0. The UCT score of a child node\nwith state st is calculated as follows:\nUCT (st) =V (st) +w\ns\nln N(p)\nN(st) , (1)\nwhere w controls the exploration and exploitation,\nand p is the parent node of the current node st.\n\u2022 Expansion explores multiple child nodes\n{st+1} from the selected node st through repeated\nsampling based on the policy model \u03c0\u03b8.\n\u2022 Simulation aims to perform rollout for each\nexpanded child node st+1 until the task is solved\nand obtain a reward r based on the rollout results.\n\u2022 Backpropagation operation leverages the re-\nward r of the child node to update the expected\nreward V (st) of nodes along the path from the root\nnode to the current node:\nNnew(st) =Nold(st) + 1, (2)\nVnew(st) =Vold(st)Nold(st) +r\nNnew(st) , (3)\nwhere Nold(st) and Vold(st) are the number of vis-\nits and value function at last iteration, respectively.\n4 Approach\n4.1 Overview\nRAG has been an indispensable technique to ad-\ndress the inherent knowledge limitations of LLMs,\neffectively integrating requisite information and\ngrounding to reliable sources (Lewis et al., 2020a;\nGuu et al., 2020). However, existing work mainly\nutilizes RAG to provide supplementary knowledge,\nwhile overlooking a thorough investigation of RAG\non enhancing the inherent reasoning capabilities of\nLLMs. To address this, we propose RAG-Star, a\nframework to fully harness the potential of inter-\nnal knowledge in LLMs for multi-step reasoning\nguided by the external retrieval.\nOur RAG-Star framework contains two major\ntechnical steps. First, we propose tree-based sub-\nquery generation to perform deliberative reason-\ning with MCTS, totally relying on the inherent\nknowledge of LLMs. Second, we design retrieval-\naugmented verification capitalizing on RAG to as-\nsist in guiding the reasoning based on the external\nknowledge. Under this framework, RAG-Star first\nselects a node from the tree to explore (Section 4.2),\nthen generates the next sub-query and answers for\nobtaining new child nodes (Section 4.3), and com-\nputes a reward to the expanded nodes (Section 4.4).\nFinally, it backpropagates the reward to update their\nparent nodes on the tree (Section 4.4). This process\nwill iterate until the task is solved. Next, we will\ndescribe each step in detail.\n4.2 Node Selection\nTo answer multi-hop questions, our framework will\niterate the tree-based search process multiple times\nto gradually generate inference solutions in a step-\nby-step way. In our work, the solution is composed\nof a sequence of intermediate sub-queries and the\n\nWhich film has the \ndirector born later, Life \nHits or It'S In The Air?\nWho is the director \nof the film \"Life \nHits\"?\nWhen was the \ndirector of \"Life \nHits\" born?\nWhat year was \nthe film \"Life \nHits\" released?\nSubquery External\nCorpus\nLLM\nTopK Docs\nAnswer(1980)\nPrompt\nSubquery\nPrompt\nWho directed \nthe film \u201cLife \nHits\u201d?\n\u2026\nWhat\u2026 When\u2026\nMCTS\nProcedure\nWhat is the birth year of the \ndirector of the film \"Life Hits\"\nExpansion\nSelection\nBackpropagation\nReward\nModeling\nBirthday\nFilm\ndirector\nRefined Answer\nReward Model\nOriginal\nQuery\nReward Score\n\ud835\udc5f = \ud835\udc5f\ud835\udc5e \u2217\ud835\udc5f\ud835\udc4e\nSub\nquery\nAnswer\nOrigin Answer (1980)\nRefined Answer (1982)\nReward\n    2.0\nHistory\nReasoning Path\nSubquery\nTopK\nRefined Answer\nQuery Score\nAnswer Score\nQuery Score (\ud835\udc5f\ud835\udc5e)\nAnswer Score (\ud835\udc5f\ud835\udc4e)\nFigure 1: Overall framework of our proposed RAG-Star approach.\nassociated answers. At each iteration, it first selects\nan appropriate node from the current tree for the\nnext exploration or expansion. The selection opera-\ntion is based on the node values computed through\nthe reward modeling and backpropagation steps.\nSpecifically, starting from the root node s0 (i.e.,\nthe input question q0), our RAG-Star model se-\nlects one node with the highest score from its child\nnodes, and then sequentially selects the next best\nchild node layer-by-layer along the tree until reach-\ning a leaf node,i.e., the terminal state indicating the\nfinal answer. To better balance exploration and ex-\nploitation, we use the UCT algorithm (Kocsis and\nSzepesv\u00e1ri, 2006) to calculate the score of each\nnode according to its number of visits N(s) and\nexpected reward V (s) in Eq. 1.\n4.3 Plan Expansion\nAfter selecting the current node, it expands the\nsearch tree by repeatively sampling multiple child\nnodes as plan based on the policy model \u03c0\u03b8. Spe-\ncially, the expansion process involves two steps,\ni.e., sub-query generation and answer deduction.\nSub-query Planning. To generate the next sub-\nquery as plan, our approach first builds the con-\ntext information by concatenating states from the\nroot node to the current selected node, and then\ninstructs the policy model to sample the next sub-\nquery based on the context information. Formally,\ngiven the node st, we can extract a path from the\nroot node s0 to the current node st, denoted by\nH = {q0; \u27e8q1, a1\u27e9; ...; \u27e8qt, at\u27e9}, where q0 is the\noriginal input question and each \u27e8qi, ai\u27e9 pair de-\nnotes the planned sub-query and its answer veri-\nfied by our retrieval-augmented varification (Sec-\ntion 4.4). We convert this path into the context\ninformation, and feed it to the policy model \u03c0\u03b8 to\ngenerate the next sub-queryqt+1 = \u03c0\u03b8(H). During\ninference, we employ repeated sampling to sample\nsub-queries by mq times to fully exploit the policy\nmodel\u2019s inherent capabilities and obtain mq new\nexpanded sub-queries.\nAnswer Deduction. After planning the sub-query,\nwe further instruct the policy model to generate\nan answer to explore the internal knowledge of\nLLMs. Specially, for each planned sub-query qt+1,\nwe directly feed the historical context H and sub-\nquery into the policy model to generate a candidate\nanswer by leveraging the inherent knowledge en-\ncoded in its parameters as follows:\nat+1 = \u03c0\u03b8(H, qt+1). (4)\nIn this process, we do not consider the external\nknowledge from RAG to avoid knowledge con-\nflicts. We aim to fully exploit the potential of the\ninternal knowledge of LLMs without interference\nfrom external information, differing from previous\nretrieval-augmented work (Lewis et al., 2020b; Yao\net al., 2022) that might suffer from knowledge con-\nflicts and interference. After obtaining the answer,\nwe can store each \u27e8qt+1, at+1\u27e9 pair in the corre-\nsponding node state, which will be subsequently\nused for reward modeling.\nWhen completing the plan expansion process,\nwe can obtainmq child nodes for every parent node,\neach of which contains a sub-query qt+1 and its\nanswer at+1.\n4.4 Reward Modeling and Backpropagation\nTraditional MCTS methods require to perform ex-\npensive rollout from the current node until the task\nends to evaluate the expanded nodes. In our work,\nfollowing previous work on process-supervised re-\nward modeling (Setlur et al., 2024; Lightman et al.,\n\n2024), we propose retrieval-augmented verifica-\ntion and refinement by using external knowledge\nto verify the consistency between the model output\nand retrieved information. Specifially, we employ\nreward models to assign an estimated reward r to\nthe expanded node, which effectively quantifies the\neffectiveness of the policy model in successfully\nanswering the input question if continually reason-\ning from the current node. Next, we introduce\nthe involved two kinds of reward scores, namely\nanswer-aware reward and query-aware reward.\nAnswer-aware Reward. We first introduce the\nanswer-aware reward in the verification process.\nFirst, given a sub-query qt+1, we follow existing\nmethods (Lewis et al., 2020a) to retrieve top- K\ndocuments Dt+1 = {dt+1,k}K\nk=1 from the exter-\nnal corpus. Based on the retrieved documents, we\nthen employ the reward model to assign an answer-\naware reward ra to the currently generated answer\nat+1 from the internal knowledge of LLMs. Specif-\nically, there are overall three cases for the knowl-\nedge consistency between at+1 and Dt+1 with dif-\nferent rewards:\nra =\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n1, if at+1 cannot be verified by Dt+1\n2, if at+1 is in conflict with Dt+1\n3, if at+1 is aligned with Dt+1\nNote that in the second case (i.e., at+1 is in conflict\nwith Dt+1), we assign a moderate score 2 to the an-\nswer because we will refine at+1 with a new poten-\ntial answer \u02dcat+1 from the external knowledge Dt+1\nto support the policy model to continually reason\nfrom the current node. However, if the answerat+1\ncannot be verified by the external knowledge, we\nwill assign the lowest score 1 to the answer, avoid-\ning the policy model from exploring the potentially\nrisky solution space.\nQuery-aware Reward. In addition to evaluating\nthe consistency of the generated answer with ex-\nternal knowledge, we employ the reward model\nto provide a query-aware reward rq for mea-\nsuring the plausibility of the planned sub-query\nqt+1 based on the historical context information\nfrom the root node to current node, i.e., H =\n{q0; \u27e8q1, a1\u27e9; ...; \u27e8qt, at\u27e9}. If the sub-query evalu-\nated by the reward model is logically inconsistent\nwith the history plan, the score rq is set to 0; other-\nwise, it is set to 1. Therefore, the final reward r for\nthe expanded node st+1 is computed as r = ra \u00b7 rq.\nThis step aims to prevent the policy model from\ncontinuing to reason along illogical sub-queries.\nAfter obtaining the final reward for the newly\nexpanded node, we backpropagate the reward to up-\ndate the value of nodes from the root nodes0 to the\ncurrent node st+1. For each node s0, s1, ..., st+1 in\nthe path, its number of visits N(s) and the value\nV (s) will be updated according to Eq. 2. These up-\ndated values are used in the UCT algorithm in Eq. 1\nto guide the node selection at the next iteration.\n4.5 Reward Model Training\nIn the reward modeling process, the capacity of the\nreward model critically influences the search pro-\ncess and ultimate answer accuracy. However, uti-\nlizing close-source model API or very large LLMs\nincurs substantial computational costs for deploy-\nment. Hence, we adopt a knowledge distillation\ntechnique to transfer capabilities from an advanced\nLLM, which usually has more parameters, to a rel-\natively smaller model. This involves two phases:\ndata synthesis and instruction fine-tuning.\nDuring data synthesis, we mix up training sets\nfrom our evaluation datasets to maintain diversity.\nFirst, we adopt in-context learning to instruct the\npolicy model to generate a CoT format solution\nand then break down into multiple sub-steps, each\nincorporating the input question, accumulated rea-\nsoning paths, and a sub-query specific to the current\nstep. To further ensure diversity, only one random\nstep from each sample is selected for subsequent\ninstruction data creation. We then employ a more\nadvanced LLM (i.e., GPT-4o-mini) combined with\na retrieval system to evaluate the sub-query and its\nanswer for each step (Section 4.4), and filter the\noutput that fails to meet the format criteria. Finally,\nwe compile a dataset of intermediate steps and their\nquery and answer rewards from an advanced LLM.\nIn the instruction fine-tuning phase, we utilize the\nsynthetic samples to fine-tune a smaller LLM (i.e.,\nLlama-3.1-8B-Instruct), thereby enhancing its ca-\npabilities in reward modeling.\n5 Experiments\n5.1 Experimental Setup\nDatasets and Evaluation Metrics. We select\nfour typical complex multi-hop question-answering\ndatasets, i.e., HotpotQA (Yang et al., 2018), 2Wiki-\nMultihopQA (Ho et al., 2020), MusiQue (Trivedi\net al., 2022), and StrategyQA (Geva et al., 2021).\nFor evaluation metrics, we use Exact Match (EM),\nF1 score, and Cover Exact Match (Cover EM),\n\nwhere Cover EM measures whether the ground\ntruth answer is covered in the generated answer.\nWe randomly select 100 samples from the whole\nvalidation sets of each dataset as our final test set\nfor all baselines and our method.\nBaselines. We compare RAG-Star to the follow-\ning two types of baselines based on GPT-4o and\nLlama-3.1-8B-Instruct:\n\u2022 Vanilla prompting methods including direct\nprompting, Chain-of-Thought (CoT), and standard\nRAG. Direct prompting instructs the model to di-\nrectly generate answers and CoT incorporates in-\ntermediate reasoning steps, which are all based on\nthe inherent knowledge of LLMs. Standard RAG\nfirst retrieves documents from Wikipedia based on\nDPR (Karpukhin et al., 2020) as prompts and then\ngenerates the final answers.\n\u2022 Improved RAG methods including Iterative\nRAG (Xu et al., 2024), Judge-then-retrieve (Asai\net al., 2024), and Generate-then-retrieve (Wang\net al., 2023). We reimplement all of these baselines\nin our experiments. Iterative RAG iteratively de-\ncomposes the input question into sub-queries for\nretrieval and generation, ultimately ensembling all\nintermediate answers; Judge-then-retrieve first de-\ncides whether the retrieval is needed, autonomously\ndeciding to utilize either internal or external knowl-\nedge to aid in generation; Generate-then-retrieve\nfirst generates an initial answer used for retrieving\nmore documents relevant to the question and then\ngenerates the final answer based on documents.\nImplementation Details. We use a closed-source\nmodel (GPT-4o) and an open-source model (Llama-\n3.1-8B-Instruct) as our policy models to measure\nthe performance of the RAG-Star framework. For\nthe reward models, we use GPT-4o-mini and a\nfine-tuned Llama-3.1-8B-Instruct. For HotpotQA,\nwe only use the abstract of articles in Wikipedia\n2017 dump as the retrieval corpus following Yang\net al. (2018), while for other datasets, we use the\nwhole articles in Wikipedia 2018 dump (Karpukhin\net al., 2020). Moreover, for the retrieval model, we\nuse FAISS for index building and BGE-large-en-\nv1.5 (Xiao et al., 2023) for dense passage retrieval.\nFor all retrieval-based baselines, we retrieve top-5\ndocuments and employ greedy search for decoding\nwith a temperature of 0. For RAG-Star, we set the\nmaximum number of simulations to 50 and a maxi-\nmum of 6 layers. In UCT algorithm, the weight w\nto control the exploration and exploitation is set to\n0.2. We also retrieve top-5 documents and sample\nthree sub-queries at a time (mq = 3) with temper-\nature 1.0 and top-p sampling where p = 1.0. For\nanswer generation, we sample an answer using a\ntemperature of 0.9 and top-p sampling set to 1.0.\n5.2 Main Results\nTable 1 shows the results of RAG-Star and other\nbaselines across four representative multi-hop ques-\ntion answering datasets.\nFirstly, it can be observed that relatively smaller\nmodels (e.g., Llama-3.1-8B-Instruct) show limited\nperformance on these knowledge-intensive reason-\ning tasks, achieving below 10% across three met-\nrics in MusiQue. Although the Chain-of-Thought\ntechnique can slightly improve the answer recall\n(e.g., Cover EM scores of Llama-3.1-8B-Instruct\nand GPT-4o in MusiQue increase from 3.0% and\n19.0% to 16.0% and 27.0%, respectively), the\nmodel is prone to generating substantial irrele-\nvant information in the output, decreasing the over-\nall performance (e.g., F1 score of Llama-3.1-8B-\nInstruct drops from 21.9% to 7.1% on 2WikiMulti-\nhopQA).\nSecondly, based on the standard RAG, GPT-4o\nachieves substantial improvement in HotpotQA\n(e.g., Cover EM increases from 47.0% to 57.0%)\nbut exhibits a large decline in StrategyQA ( e.g.,\nCover EM from 73.0% to 62.0%), suggesting a po-\ntential conflict between external sources and inter-\nnal knowledge of LLMs. We speculate the reason\nmight be that using the retrieved information di-\nrectly as input incorporates some noises and makes\nthe LLM lost in the useful information. There-\nfore, by controlling the utilization of internal and\nexternal knowledge, Judge-then-Retrieve can sig-\nnificantly alleviate this issue (e.g., Cover EM from\n62.0% to 74.0% in StrategyQA). However, these\napproaches still present limited or even negative im-\nprovements in complex tasks (e.g., Cover EM from\n19.0% to 16.0% in MusiQue), necessitating effec-\ntive methods to consolidate external and internal\nknowledge.\nFinally, our approach outperforms all baselines\nacross most metrics in four datasets. RAG-Star\nintroduces a \u201cSystem 2\u201d-like slow and delibera-\ntive thinking process and employs RAG to verify\nand guide the multi-step reasoning process. By\nemploying retrieval-augmented verification, the re-\nward model can effectively encourage the model\ntowards plausible sub-query nodes or avert from\n\nMethod\nHotpotQA 2WikiMultihopQA MusiQue StrategyQA\nEM CEM F1 EM CEM F1 EM CEM F1 EM CEM F1\nLlama-3.1-8B-Instruct 14.0 25.0 26.0 9.0 29.0 21.9 2.0 3.0 3.9 63.0 65.0 63.0\n+ Chain-of-Tought 20.0 38.0 26.3 4.0 32.0 7.1 4.0 16.0 6.6 55.0 69.0 55.0\n+ Standard RAG 40.0 48.0 52.8 17.0 23.0 26.1 11.0 11.0 15.5 63.0 64.0 63.0\n+ Iterative RAG 26.0 31.0 36.9 22.0 23.0 26.0 7.0 11.0 15.9 61.0 63.0 61.0\n+ Generate-then-Retrieve 34.0 44.0 49.4 21.0 30.0 26.6 13.0 17.0 19.4 63.0 67.0 63.0\n+ Judge-then-Retrieve 39.0 48.0 53.9 18.0 26.0 26.8 10.0 10.0 16.0 58.0 63.0 58.0\n+ RAG-Star w Llama RM 42.0 44.0 54.4 34.0 38.0 42.0 13.0 18.0 22.2 71.0 72.0 71.0\n+ RAG-Star w GPT RM 46.0 49.0 60.0 38.0 43.0 46.8 22.2 27.0 30.7 67.6 69.0 67.6\nGPT-4o 43.0 47.0 56.7 36.0 42.0 45.7 13.0 19.0 24.3 70.0 73.0 70.0\n+ Chain-of-Tought 36.0 49.0 56.8 38.0 55.0 53.9 20.0 27.0 29.6 37.0 79.0 37.0\n+ Standard RAG 47.0 57.0 63.7 25.0 26.0 31.2 14.0 18.0 20.6 45.0 62.0 45.0\n+ Iterative RAG 47.0 59.0 63.3 19.0 24.0 26.3 15.0 26.0 25.5 32.0 74.0 32.0\n+ Generate-then-Retrieve 44.0 57.0 62.0 29.0 36.0 37.5 23.0 28.0 31.0 50.0 68.0 50.0\n+ Judge-then-Retrieve 44.0 50.0 58.6 28.0 29.0 32.2 14.0 16.0 22.8 72.0 74.0 72.0\n+ RAG-Star w Llama RM 48.0 54.0 66.3 47.0 68.0 62.8 25.0 36.0 39.0 61.0 86.0 61.0\n+ RAG-Star w GPT RM 48.0 57.0 68.6 48.0 63.0 61.7 29.0 40.0 43.5 60.0 81.0 60.0\nTable 1: Evaluation results on four representative multi-hop question answering tasks. \u201cRM\u201d is short for reward\nmodel. The bold and underline fonts denote the best and second best results in each dataset, respectively.\nMethod\nGPT-4o Llama3.1-8B\nCEM F1 CEM F1\nRAG-Star (Ours) 84.0 68.3 75.0 73.3\nw/o Query Score 82.0 68.0 71.0 69.0\nw/o Answer Score 80.0 66.3 66.0 65.3\nw/o Retrieval 78.0 67.3 67.0 66.0\nw/o Refine 77.0 68.2 70.0 68.1\nTable 2: Ablation study in StrategyQA.\npotential risky nodes. For example, equipped with\nour RAG-Star framework, Llama-3.1-8B-Instruct\nachieves higher scores in two challenging reason-\ning datasets, i.e., 2WikiMultihopQA and MusiQue,\nsignificantly beyond all baseline methods.\n5.3 Further Analysis\nWe report further analysis in StrategyQA with ran-\ndomly selected 100 samples \u2013 we have similar find-\nings in other datasets.\nAblation Study. To validate the effectiveness of\nour proposed framework, we conduct an ablation\nanalysis of its key design elements. We design\nfour variants: (1) w/o Retrieval removes the re-\ntrieved documents in reward modeling; (2) w/o\nRefine does not refine the conflict answer with re-\ntrieved documents in reward modeling; (3) w/o\nQuery Reward removes the query-aware reward rq\nfor scoring; and (4) w/o Answer Reward removes\nthe answer-aware reward ra for scoring. We show\nthe results in Table 2. It is clear that all the vari-\nFigure 2: Cover EM performance on the StrategyQA\nw.r.t. the number of simulations (Left) or the number of\ntraining data (Right).\nants perform worse than the original method, in-\ndicating the effectiveness of each component in\nour framework. Specifically, the performance of\nw/o Retrieval drops significantly for Llama-3.1-8B,\nindicating that using external knowledge for ver-\nification can be highly beneficial for the inherent\nreasoning of LLMs. Similarly, w/o Refine leads to a\ndecline in model performance, which highlights the\nimportance of repurposing external sources for cor-\nrecting the errors in the model\u2019s reasoning process.\nMoreover, both w/o Query Reward and w/o Answer\nReward variants lead to a substantial performance\ndecline, which suggests that the consistency and\nlogical plausibility of intermediate sub-queries and\nanswers are both critical for the model to plan the\ncorrect path towards the final answer.\nEffect of Simulation Scaling. Typically, scal-\ning the simulation iterations will lead to a higher\n\nWas Christian born earlier?Reward: 0\nChristian,born14December1972,isaDanish\u2026 It\u2019sintheAir is \u2026 comedydirectedbyAnthony\u2026 Anthony (10November1901to 19May1964) \u2026\nWhich film has the director born later, Life Hits or It\u2019s In The Air?(Answer: Life Hits)\nSub-query:When was the director of\u201cLife Hits\u201d born?Answer:1889    Reward: 0 Sub-query: Who is the director of It\u2019s In The Air?Answer:Reward: 2AnthonyClaudeSub-query: When wasAnthony born?Answer:1901Reward:3\u2026LifeHitsisa2006dramafilmdirectedbyChristian\u2026\nSub-query: Who is the director of \u201cLife Hits\u201d ?Answer: Christian Reward: 3\nSub-query:When wasChristian born?Answer:Reward:219981972\u2026Input Question\n\u2705\n \u274erefine\nWhat's Christian's date of birth? Reward: 1\nVerification byRetrieved Documents\n\u274erefine\n\u2705\nTherefore, the final answer is Life HitsOutputFailed NodeSuccess NodeAnswer Node\nFigure 3: A qualitative example showing the deliberative reasoning process of RAG-Star in 2WikiMultihopQA.\nlevel of task-solving capability. To explore the re-\nlationship between simulation scaling and the final\nperformance of RAG-Star, we test our model under\ndifferent maximum simulation iterations. Specifi-\ncally, we vary the maximum simulation rounds in\na set {10, 20, 30, 40, 50, 60}, and evaluate Llama\nand GPT-4o in StrategyQA with GPT-4o-mini as\nthe reward model. The results are presented in the\nFigure 2. We can see that as the maximum number\nof simulation increases, the model\u2019s performance\ngradually improves, although the average time con-\nsumed also rises to some extent. This highlights\nthat scaling the test-time computation can further\npromote more thorough exploration and exploita-\ntion by the policy model within the search space.\nHowever, as the number of simulations further in-\ncreases, the performance of the policy model tends\nto be saturated. Due to the limitation of inherent\nknowledge, the policy model cannot benefit a lot\nfrom conducting more simulations.\nEffect of Reward Model. In our framework, the\nreward model is used to assess the logical plausibil-\nity of the sub-query and the consistency between\nthe output answer and external sources. In this part,\nwe aim to explore how to train open-source reward\nmodels (i.e., Llama-3.1-8B-Instruct) to achieve per-\nformance comparable to closed-source LLMs (i.e.,\nGPT-4o-mini) by varying amounts of training data\nfrom 20K to 80K. Specifically, we employ different\namounts of training data to fine-tune Llama-3.1-8B-\nInstruct and use the fine-tuned model to evaluate\nthe sub-query and its answer. As shown in Fig-\nure 2, we can see that as the amount of training\ndata increases, the reward model can achieve more\naccurate verification quality, significantly benefit-\ning the planning and reasoning of the policy model.\nHowever, the performance gains tend to saturate at\nlater stages, necessitating instruction tuning data\nwith higher diversity and quality.\n5.4 Case Study\nTo facilitate understanding of the entire workflow\nof our proposed RAG-Star, we present a qualita-\ntive analysis in 2WikiMultihopQA. Throughout\nthe search process, the LLM initializes the input\nquestion as root node and conducts multiple simu-\nlations, eventually reaching the terminal leaf node,\nwhich can be vividly represented as a tree. As\nshown in Figure 3, after selecting the first query\n(i.e., Who is the director of \u201cLife Hits\u201d?), the model\nexpands multiple children nodes by repeated sam-\npling. At the next iteration, the model refines the\ngenerated answer ( i.e., 1998 ) for the sub-query\n(\u201cWhen was Christian born?\u201d) based on retrieved\ndocuments and the reward model returns an overall\nscore of 2. By iterating the multi-step reasoning\nand retrieval-augmented verification processes for\nseveral rounds, the model outputs the final answer\n(i.e., Life Hits). In the task-solving process, the pol-\nicy model generates an answer to the current sub-\nquery based on its internal knowledge, which might\nbe erroneous due to the limited pre-training corpus\nin time or the memorization mistakes. Therefore,\nthe external knowledge can be beneficial to validate\nthe correctness of inherent knowledge of LLMs, ef-\nfectively guiding the model to plan a reasonable\npath.\n6 Conclusion\nIn this work, we proposed RAG-Star, a novel RAG\napproach for leveraging external retrieval technique\nto enhance the multi-step reasoning capabilities of\nLLMs. RAG-Star employed Monte Carlo Tree\nSearch to search intermediate sub-queries and cor-\nresponding answers. Moreover, RAG-Star intro-\nduced retrieval-augmented verification to evaluate\n\nthe plausibility and consistency of the planned sub-\nqueries and answers based on a query-aware and\nan answer-aware reward. At each iteration, RAG-\nStar conducted node selection, plan expansion, re-\nward modeling, and reward backpropagation se-\nquentially to consolidate the internal knowledge of\nLLMs and external knowledge from RAG. Exten-\nsive experiments on several datasets showed that\nour proposed RAG-Star outperforms the traditional\nRAG and reasoning methods.\nLimitations\nDespite the great efforts that we have made, the\nexperimental analysis is still limited due to the\nmassive computational cost of tree-based search\napproaches. We will investigate into more types\nof complex reasoning tasks and datasets. In our\nmodel, we only leverage Monte Carlo Tree Search\nto conduct our deleberative reasoning process. we\nmay consider investigate more kinds of search algo-\nrithms to verify the generalization and robustness\nof our proposed framework. Moreover, the perfor-\nmance of our model is affected by the feedback\nquality provided by the reward model. Therefore, a\nwell-trained and performant reward model is impor-\ntant for guiding the reasoning process. We will con-\nsider other fine-tuning strategies and more LLMs\nin reward modeling.\nReferences\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206\u20132240. PMLR.\nBradley C. A. Brown, Jordan Juravsky, Ryan Saul\nEhrlich, Ronald Clark, Quoc V . Le, Christopher R\u00e9,\nand Azalia Mirhoseini. 2024. Large language mon-\nkeys: Scaling inference compute with repeated sam-\npling. CoRR, abs/2407.21787.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nJifan Chen, Shih-Ting Lin, and Greg Durrett. 2019.\nMulti-hop question answering via reasoning chains.\nCoRR, abs/1910.02610.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? A question answering benchmark with\nimplicit reasoning strategies. Trans. Assoc. Comput.\nLinguistics, 9:346\u2013361.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning , pages 3929\u20133938.\nPMLR.\nPeter E. Hart, Nils J. Nilsson, and Bertram Raphael.\n1968. A formal basis for the heuristic determina-\ntion of minimum cost paths. IEEE Trans. Syst. Sci.\nCybern., 4(2):100\u2013107.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing A multi-hop\nQA dataset for comprehensive evaluation of reason-\ning steps. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 6609\u20136625. International Committee on\nComputational Linguistics.\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 1049\u20131065. Association for Computa-\ntional Linguistics.\nZhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li,\nHaoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan\nYe, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing\nYang, Ting Wu, Binjie Wang, Shichao Sun, Yang\nXiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei\nQin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng,\nShaoting Zhang, Dahua Lin, Yu Qiao, and Pengfei\nLiu. 2024. Olympicarena: Benchmarking multi-\ndiscipline cognitive reasoning for superintelligent AI.\nCoRR, abs/2406.12753.\n\nDaniel Kahneman. 2011. Thinking, fast and slow. Far-\nrar, Straus and Giroux.\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nLevente Kocsis and Csaba Szepesv\u00e1ri. 2006. Bandit\nbased monte-carlo planning. In Machine Learning:\nECML 2006, 17th European Conference on Machine\nLearning, Berlin, Germany, September 18-22, 2006,\nProceedings, volume 4212 of Lecture Notes in Com-\nputer Science, pages 282\u2013293. Springer.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020a. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459\u20139474.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harri-\nson Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2024. Let\u2019s verify step by step. In The Twelfth In-\nternational Conference on Learning Representations,\nICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\nReview.net.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023. Faithful chain-of-\nthought reasoning. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023 , pages 305\u2013\n329. Association for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report. OpenAI Blog.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang\nGeng, Jacob Eisenstein, Rishabh Agarwal, Alekh\nAgarwal, Jonathan Berant, and Aviral Kumar.\n2024. Rewarding progress: Scaling automated pro-\ncess verifiers for llm reasoning. arXiv preprint\narXiv:2410.08146.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur\nGuez, Laurent Sifre, George van den Driessche, Ju-\nlian Schrittwieser, Ioannis Antonoglou, Vedavyas\nPanneershelvam, Marc Lanctot, Sander Dieleman,\nDominik Grewe, John Nham, Nal Kalchbrenner, Ilya\nSutskever, Timothy P. Lillicrap, Madeleine Leach,\nKoray Kavukcuoglu, Thore Graepel, and Demis Has-\nsabis. 2016. Mastering the game of go with deep neu-\nral networks and tree search. Nat., 529(7587):484\u2013\n489.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioan-\nnis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore\nGraepel, Timothy P. Lillicrap, Karen Simonyan, and\nDemis Hassabis. 2017. Mastering chess and shogi\nby self-play with a general reinforcement learning\nalgorithm. CoRR, abs/1712.01815.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku-\nmar. 2024. Scaling LLM test-time compute optimally\ncan be more effective than scaling model parameters.\nCoRR, abs/2408.03314.\nRichard Sutton. 2019. The bitter lesson. Incomplete\nIdeas (blog), 13(1):38.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V . Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. 2023. Challenging\nbig-bench tasks and whether chain-of-thought can\nsolve them. In Findings of the Association for Com-\nputational Linguistics: ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pages 13003\u201313051. Association for\nComputational Linguistics.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTrans. Assoc. Comput. Linguistics, 10:539\u2013554.\nChaojie Wang, Yanchen Deng, Zhiyi Lv, Zeng Liang,\nJujie He, Shuicheng Yan, and Bo An. 2024a. Q*:\nImproving multi-step reasoning for llms with deliber-\native planning. CoRR, abs/2406.14283.\nFei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen,\nand Sercan \u00d6 Ar\u0131k. 2024b. Astute rag: Overcom-\ning imperfect retrieval augmentation and knowledge\nconflicts for large language models. arXiv preprint\narXiv:2410.07176.\nLiang Wang, Nan Yang, and Furu Wei. 2023.\nQuery2doc: Query expansion with large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2023, Singapore, December 6-10, 2023,\npages 9414\u20139423. Association for Computational\nLinguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompting\nelicits reasoning in large language models. In Ad-\nvances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Process-\ning Systems 2022, NeurIPS 2022, New Orleans, LA,\nUSA, November 28 - December 9, 2022.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding.\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng,\nand Tat-Seng Chua. 2024. Search-in-the-chain: Inter-\nactively enhancing large language models with search\nfor knowledge-intensive tasks. In Proceedings of the\nACM on Web Conference 2024, WWW 2024, Singa-\npore, May 13-17, 2024, pages 1362\u20131373. ACM.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2369\u20132380. Association for Computational\nLinguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint.\nEric Zelikman, Georges Harik, Yijia Shao, Varuna\nJayasiri, Nick Haber, and Noah D. Goodman. 2024.\nQuiet-star: Language models can teach themselves\nto think before speaking. CoRR, abs/2403.09629.\nDi Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong\nLi, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco\nPavone, Yuqiang Li, et al. 2024. Llama-berry: Pair-\nwise optimization for o1-like olympiad-level mathe-\nmatical reasoning. arXiv preprint arXiv:2410.02884.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023a. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2023, Toronto, Canada, July\n9-14, 2023, pages 5823\u20135840. Association for Com-\nputational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023b. A survey of large language models. CoRR,\nabs/2303.18223.\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang,\nYongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yu-\njiu Yang. 2023. Solving math word problems via\ncooperative reasoning induced language models. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 4471\u20134485. Association for Computa-\ntional Linguistics.",
  "authors": [
    "Jinhao Jiang",
    "Jiayi Chen",
    "Junyi Li",
    "Ruiyang Ren",
    "Shijie Wang",
    "Wayne Xin Zhao",
    "Yang Song",
    "Tao Zhang"
  ],
  "summary": "Existing large language models (LLMs) show exceptional problem-solving capabilities but might struggle with complex reasoning tasks. Despite the successes of chain-of-thought and tree-based search methods, they mainly depend on the internal knowledge of LLMs to search over intermediate reasoning steps, limited to dealing with simple tasks involving fewer reasoning steps. In this paper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the retrieved information to guide the tree-based deliberative reasoning process that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and answers for reasoning based on the LLM itself. To consolidate internal and external knowledge, we propose an retrieval-augmented verification that utilizes query- and answer-aware reward modeling to provide feedback for the inherent reasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms previous RAG and reasoning methods.",
  "pdf_url": "https://arxiv.org/pdf/2412.12881v1",
  "entry_id": "http://arxiv.org/abs/2412.12881v1",
  "published": "2024-12-17",
  "updated": "2024-12-17",
  "comment": "LLM;RAG;MCTS",
  "journal_ref": null,
  "doi": null,
  "primary_category": "cs.CL",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "links": [
    {
      "href": "https://arxiv.org/abs/2412.12881v1",
      "rel": "alternate",
      "title": null
    },
    {
      "href": "https://arxiv.org/pdf/2412.12881v1",
      "rel": "related",
      "title": "pdf"
    }
  ],
  "full_text": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented\nVerification and Refinement\nJinhao Jiang1\u2217, Jiayi Chen2\u2217, Junyi Li4\u2217, Ruiyang Ren1, Shijie Wang3\nWayne Xin Zhao1\u2020, Yang Song5\u2020, Tao Zhang5\n1Gaoling School of Artificial Intelligence, Renmin University of China.\n2Wuhan University of Science and Technology.3Northeastern University at Qinhuangdao.\n4Department of Computer Science, National University of Singapore. 5BOSS Zhipin, Beijing, China.\njiangjinhao@ruc.edu.cn, batmanfly@gmail.com\nAbstract\nExisting large language models (LLMs) show\nexceptional problem-solving capabilities but\nmight struggle with complex reasoning tasks.\nDespite the successes of chain-of-thought and\ntree-based search methods, they mainly depend\non the internal knowledge of LLMs to search\nover intermediate reasoning steps, limited to\ndealing with simple tasks involving fewer rea-\nsoning steps. In this paper, we propose RAG-\nStar, a novel RAG approach that integrates the\nretrieved information to guide the tree-based\ndeliberative reasoning process that relies on\nthe inherent knowledge of LLMs. By lever-\naging Monte Carlo Tree Search, RAG-Star it-\neratively plans intermediate sub-queries and\nanswers for reasoning based on the LLM itself.\nTo consolidate internal and external knowledge,\nwe propose an retrieval-augmented verification\nthat utilizes query- and answer-aware reward\nmodeling to provide feedback for the inherent\nreasoning of LLMs. Our experiments involv-\ning Llama-3.1-8B-Instruct and GPT-4o demon-\nstrate that RAG-Star significantly outperforms\nprevious RAG and reasoning methods.\n1 Introduction\nDespite the excellent capabilities of large language\nmodels (LLMs) (Zhao et al., 2023b), they still face\nsignificant challenges in complex reasoning tasks\n(e.g., multi-hop question answering), which often\ngo beyond simple, single-step problem-solving,\ndemanding a deeper level of cognitive reasoning\nacross multiple facts, sources, or contexts (Huang\net al., 2024; Suzgun et al., 2023). Great efforts have\nbeen made to improve the reasoning effectiveness\nof LLMs by conducing step-by-step reasoning, ex-\nemplified by chain-of-thought (CoT) (Wei et al.,\n2022). However, as the number of reasoning steps\ngrows, LLMs are often prone to introduce logical\n\u2217 Equal contributions.\n\u2020 Corresponding author.\nerrors, factual hallucinations, or inconsistent state-\nments (Wei et al., 2022; Lyu et al., 2023).\nIn fact, step-by-step reasoning in the auto-\nregressive generation paradigm can be described\nas akin to \u201cSystem 1\u201d, a mode of thinking which is\nfast, instinctive but less accurate (Kahneman, 2011).\nConversely, solving complex reasoning problems\nrequires more in-depth, deliberative, and logical\nthinking, known as the \u201cSystem 2\u201d mode, which re-\nquires conscious effort to conduct massive strategic\ndecision-making (Kahneman, 2011). To enhance\nthe \u201cSystem 2\u201d reasoning capabilities of LLMs,\nprior studies have proposed to conduct deliberative\ngeneration by leveraging basic tree search algo-\nrithms (e.g., Monte Carlo Tree Search (Silver et al.,\n2017)). However, LLMs in these studies mainly\ndepend on their internal knowledge to search over\nintermediate reasoning steps, limited to handling\nproblems with relatively simple reasoning process.\nTo leverage external knowledge in model reasoning,\nextensive research has sought to augment LLMs\nwith external information sources (a.k.a. retrieval-\naugmented generation, RAG) (Lewis et al., 2020b;\nYao et al., 2022), while existing efforts mainly con-\nsider sequential reasoning structure, which cannot\nnaturally support more complex reasoning structure\nlike MCTS. Thus, we raise the following research\nquestion: Can RAG enhance the deliberative rea-\nsoning capabilities of LLMs?\nIn light of this, in this paper, we propose RAG-\nStar, a novel RAG-enhanced framework designed\nto improve multi-step reasoning capabilities of\nLLMs with deliberative planning. As the major\ntechnical contribution, RAG-Star can fully exploit\nthe internal knowledge of LLMs to plan the multi-\nstep reasoning, and meanwhile integrating the ex-\nternal retrieval to guide the internal reasoning pro-\ncess. To achieve this goal, we first introduce a\ntree-based search algorithm (i.e., Monte Carlo Tree\nSearch, MCTS) with LLMs to search over possible\nplans for solving the problem at hand where a com-\narXiv:2412.12881v1  [cs.CL]  17 Dec 2024\n\nplete plan is composed of a sequence of sub-queries\nand corresponding answers. Starting from the input\nquestion (root node), RAG-Star iteratively gener-\nates and selects an appropriate sub-query and its an-\nswer (intermediate node), which aims to maximally\nexplore the optimal sub-query path towards the fi-\nnal answer solely based on the inherent knowledge\nof LLMs. Second, different from existing delibera-\ntion methods (Wang et al., 2024a; Yao et al., 2023),\nRAG-Star proposes retrieval-augmented verifica-\ntion that involves both query- and answer-aware\nreward modeling, fully exploiting external sources\nto guide the internal deliberative reasoning. In our\napproach, instead of directly interfering in the rea-\nsoning process of LLMs, we consider employing\nRAG to refine the derived reasoning steps in MCTS,\nwhich can effectively reduce the conflicts between\ninherent and external knowledge, which has been\na common issue when using RAG methods (Wang\net al., 2024b; Gao et al., 2023).\nWe conduct extensive experiments to verify the\neffectiveness of RAG-Star based on Llama-3.1-8B-\nInstruct and GPT-4o. Our method outperforms the\nbaselines by up to 18.98% and 16.19% on average\nfor Llama-3.1-8B and GPT-4o, respectively.\nOur main contributions can be summarized as:\n\u2022 We propose RAG-Star that leverages external\nretrieval to enahnce the deliberative reasoning of\nLLMs based on their internal knowledge.\n\u2022 We design an effective retrieval-augmented\nverification and refinement to evaluate and correct\nthe inherent reasoning process.\n\u2022 We conduct extensive experiments on sev-\neral datasets, where RAG-Star significantly out-\nperforms existing RAG and reasoning methods.\n2 Related Work\nRetrieval-Augmented LLMs. Augmenting large\nlanguage models (LLMs) with retrieval has been\nextensively studied in existing literature (Lewis\net al., 2020a; Borgeaud et al., 2022; Guu et al.,\n2020), which incorporates a differentiable retriever\nto provide external sources for LLMs. Further-\nmore, LLMs have made significant advancements\nin many reasoning tasks, such as code genera-\ntion (OpenAI, 2023), math word problems (Zhu\net al., 2023) and question answering (Brown et al.,\n2020). Chain-of-thought (CoT) has been reported\nas an emergent ability of LLMs when they are large\nenough (Wei et al., 2022), which encourages LLMs\nto generate explicit intermediate reasoning steps\nin reasoning rather than simply providing answers\ndirectly. To elicit or improve the multi-step reason-\ning capability of LLMs, several approaches seek\nto harness the strengths of both CoT and retrieval\non knowledge-intensive complex reasoning tasks,\nsuch as multi-hop question answering (Yao et al.,\n2022; Zhao et al., 2023a). The rationales gained\nfrom reasoning enhance the retrieval of more rel-\nevant information, while the retrieved knowledge\nimproves the factuality of intermediate reasoning\nsteps. However, these approaches primarily take\nretrieved documents as direct input to the model,\neasily suffering from knowledge conflicts between\nthe parametric knowledge of LLMs and the exter-\nnal sources. In contrast, our RAG-Star framework\nintegrates tree-based search to fully explore the so-\nlution space and repurpose the retrieval information\nas external guidance to the reasoning process.\nEnhancing LLMs with Search. Applying search\non top of LLMs has been a topic of much inter-\nest. Several recent works have explored search\nalgorithms to improve the performance of LLMs\nduring the inference stage (Wang et al., 2024a;\nZhang et al., 2024). The bitter lesson (Sutton,\n2019) famously suggests that two forms of scal-\ning, i.e., learning and search, supersede all other\napproaches. Many studies have proven that scal-\ning the inference-time computation can lead to\nsubstantial improvements in the performance of\nLLMs without training (Brown et al., 2024; Snell\net al., 2024). These search algorithms, where\nmultiple branches of outcomes are explored dur-\ning search, have been widely applied in reinforce-\nment learning algorithms (Hart et al., 1968; Silver\net al., 2017) and many real-world applications such\nas AlphaGo (Silver et al., 2016) for their good\nexploration-exploitation trade-off. However, these\napproaches mainly rely on the internal knowledge\nof LLMs to search potential solutions, which might\nnot be optimal and leads to a amount of rollouts,\nsignificantly slowing down the decoding process.\nIn this paper, we leverage the external retrieval\nsources to enhance the deliberative search process\nwith LLMs, effectively differentiate the internal\nreasoning and external retrieval.\n3 Preliminary\nIn this section, we will first formally define our task\nand then introduce Monte Carlo Tree Search which\nis used in our proposed RAG-Star approach.\n\nTask Formulation. In this work, we mainly fo-\ncus on open-domain multi-hop question answer-\ning (Chen et al., 2019; Yang et al., 2018), which\nrequires multiple steps of reasoning across differ-\nent documents to answer questions. Previous work\ntypically adopts an iterative reason-then-generate\npipeline (Wei et al., 2022; Huang and Chang, 2023).\nAt each step, the LLM first infers an intermediate\nsub-query based on the current situation and then\ngenerates possible answers to the query. Formally,\ngiven a natural language input question, at the t-th\nstep, the LLM M\u03b8 (parameterized by \u03b8) first de-\nliberately reasons about a sub-query qt, followed\nby generating an answer at based on its inherent\nknowledge. In some literature (Yao et al., 2022;\nAsai et al., 2024), retrieval-augmented generation\n(RAG) has been employed to improve the factual-\nity of intermediate reasoning steps. For each sub-\nquery qt, the retriever retrieves top-K documents\nDt = {dt,k}K\nk=1 from an external large-scale cor-\npus, e.g., Wikipedia, supplying them to the LLM\nto generate more accurate answers.\nMonte Carlo Tree Search (MCTS).In existing lit-\nerature (Zelikman et al., 2024; Zhang et al., 2024),\nMCTS builds a search tree T based on a policy\nmodel \u03c0\u03b8, which is usually the target LLM M\u03b8.\nEach node st = [qt, at, N(st), V(st)] represents a\nstate comprising the sub-query qt, its answer at,\nthe number of visits N(st), and the value function\n(expected reward) V (st) for accurately answering\nquestions, except that the root node s0 = [q0] only\ncontains the original input question q0, and each\nedge is an action aiming to generate the next sub-\nquery. During the search process, MCTS runs for\nmultiple simulations. For the t-th simulation, it\nconducts four operations to expand the tree:\n\u2022 Selection aims to select a node with the highest\nUCT (Upper Confidence bounds applied to Trees)\nscore (Kocsis and Szepesv\u00e1ri, 2006) starting from\nthe root node s0. The UCT score of a child node\nwith state st is calculated as follows:\nUCT (st) =V (st) +w\ns\nln N(p)\nN(st) , (1)\nwhere w controls the exploration and exploitation,\nand p is the parent node of the current node st.\n\u2022 Expansion explores multiple child nodes\n{st+1} from the selected node st through repeated\nsampling based on the policy model \u03c0\u03b8.\n\u2022 Simulation aims to perform rollout for each\nexpanded child node st+1 until the task is solved\nand obtain a reward r based on the rollout results.\n\u2022 Backpropagation operation leverages the re-\nward r of the child node to update the expected\nreward V (st) of nodes along the path from the root\nnode to the current node:\nNnew(st) =Nold(st) + 1, (2)\nVnew(st) =Vold(st)Nold(st) +r\nNnew(st) , (3)\nwhere Nold(st) and Vold(st) are the number of vis-\nits and value function at last iteration, respectively.\n4 Approach\n4.1 Overview\nRAG has been an indispensable technique to ad-\ndress the inherent knowledge limitations of LLMs,\neffectively integrating requisite information and\ngrounding to reliable sources (Lewis et al., 2020a;\nGuu et al., 2020). However, existing work mainly\nutilizes RAG to provide supplementary knowledge,\nwhile overlooking a thorough investigation of RAG\non enhancing the inherent reasoning capabilities of\nLLMs. To address this, we propose RAG-Star, a\nframework to fully harness the potential of inter-\nnal knowledge in LLMs for multi-step reasoning\nguided by the external retrieval.\nOur RAG-Star framework contains two major\ntechnical steps. First, we propose tree-based sub-\nquery generation to perform deliberative reason-\ning with MCTS, totally relying on the inherent\nknowledge of LLMs. Second, we design retrieval-\naugmented verification capitalizing on RAG to as-\nsist in guiding the reasoning based on the external\nknowledge. Under this framework, RAG-Star first\nselects a node from the tree to explore (Section 4.2),\nthen generates the next sub-query and answers for\nobtaining new child nodes (Section 4.3), and com-\nputes a reward to the expanded nodes (Section 4.4).\nFinally, it backpropagates the reward to update their\nparent nodes on the tree (Section 4.4). This process\nwill iterate until the task is solved. Next, we will\ndescribe each step in detail.\n4.2 Node Selection\nTo answer multi-hop questions, our framework will\niterate the tree-based search process multiple times\nto gradually generate inference solutions in a step-\nby-step way. In our work, the solution is composed\nof a sequence of intermediate sub-queries and the\n\nWhich film has the \ndirector born later, Life \nHits or It'S In The Air?\nWho is the director \nof the film \"Life \nHits\"?\nWhen was the \ndirector of \"Life \nHits\" born?\nWhat year was \nthe film \"Life \nHits\" released?\nSubquery External\nCorpus\nLLM\nTopK Docs\nAnswer(1980)\nPrompt\nSubquery\nPrompt\nWho directed \nthe film \u201cLife \nHits\u201d?\n\u2026\nWhat\u2026 When\u2026\nMCTS\nProcedure\nWhat is the birth year of the \ndirector of the film \"Life Hits\"\nExpansion\nSelection\nBackpropagation\nReward\nModeling\nBirthday\nFilm\ndirector\nRefined Answer\nReward Model\nOriginal\nQuery\nReward Score\n\ud835\udc5f = \ud835\udc5f\ud835\udc5e \u2217\ud835\udc5f\ud835\udc4e\nSub\nquery\nAnswer\nOrigin Answer (1980)\nRefined Answer (1982)\nReward\n    2.0\nHistory\nReasoning Path\nSubquery\nTopK\nRefined Answer\nQuery Score\nAnswer Score\nQuery Score (\ud835\udc5f\ud835\udc5e)\nAnswer Score (\ud835\udc5f\ud835\udc4e)\nFigure 1: Overall framework of our proposed RAG-Star approach.\nassociated answers. At each iteration, it first selects\nan appropriate node from the current tree for the\nnext exploration or expansion. The selection opera-\ntion is based on the node values computed through\nthe reward modeling and backpropagation steps.\nSpecifically, starting from the root node s0 (i.e.,\nthe input question q0), our RAG-Star model se-\nlects one node with the highest score from its child\nnodes, and then sequentially selects the next best\nchild node layer-by-layer along the tree until reach-\ning a leaf node,i.e., the terminal state indicating the\nfinal answer. To better balance exploration and ex-\nploitation, we use the UCT algorithm (Kocsis and\nSzepesv\u00e1ri, 2006) to calculate the score of each\nnode according to its number of visits N(s) and\nexpected reward V (s) in Eq. 1.\n4.3 Plan Expansion\nAfter selecting the current node, it expands the\nsearch tree by repeatively sampling multiple child\nnodes as plan based on the policy model \u03c0\u03b8. Spe-\ncially, the expansion process involves two steps,\ni.e., sub-query generation and answer deduction.\nSub-query Planning. To generate the next sub-\nquery as plan, our approach first builds the con-\ntext information by concatenating states from the\nroot node to the current selected node, and then\ninstructs the policy model to sample the next sub-\nquery based on the context information. Formally,\ngiven the node st, we can extract a path from the\nroot node s0 to the current node st, denoted by\nH = {q0; \u27e8q1, a1\u27e9; ...; \u27e8qt, at\u27e9}, where q0 is the\noriginal input question and each \u27e8qi, ai\u27e9 pair de-\nnotes the planned sub-query and its answer veri-\nfied by our retrieval-augmented varification (Sec-\ntion 4.4). We convert this path into the context\ninformation, and feed it to the policy model \u03c0\u03b8 to\ngenerate the next sub-queryqt+1 = \u03c0\u03b8(H). During\ninference, we employ repeated sampling to sample\nsub-queries by mq times to fully exploit the policy\nmodel\u2019s inherent capabilities and obtain mq new\nexpanded sub-queries.\nAnswer Deduction. After planning the sub-query,\nwe further instruct the policy model to generate\nan answer to explore the internal knowledge of\nLLMs. Specially, for each planned sub-query qt+1,\nwe directly feed the historical context H and sub-\nquery into the policy model to generate a candidate\nanswer by leveraging the inherent knowledge en-\ncoded in its parameters as follows:\nat+1 = \u03c0\u03b8(H, qt+1). (4)\nIn this process, we do not consider the external\nknowledge from RAG to avoid knowledge con-\nflicts. We aim to fully exploit the potential of the\ninternal knowledge of LLMs without interference\nfrom external information, differing from previous\nretrieval-augmented work (Lewis et al., 2020b; Yao\net al., 2022) that might suffer from knowledge con-\nflicts and interference. After obtaining the answer,\nwe can store each \u27e8qt+1, at+1\u27e9 pair in the corre-\nsponding node state, which will be subsequently\nused for reward modeling.\nWhen completing the plan expansion process,\nwe can obtainmq child nodes for every parent node,\neach of which contains a sub-query qt+1 and its\nanswer at+1.\n4.4 Reward Modeling and Backpropagation\nTraditional MCTS methods require to perform ex-\npensive rollout from the current node until the task\nends to evaluate the expanded nodes. In our work,\nfollowing previous work on process-supervised re-\nward modeling (Setlur et al., 2024; Lightman et al.,\n\n2024), we propose retrieval-augmented verifica-\ntion and refinement by using external knowledge\nto verify the consistency between the model output\nand retrieved information. Specifially, we employ\nreward models to assign an estimated reward r to\nthe expanded node, which effectively quantifies the\neffectiveness of the policy model in successfully\nanswering the input question if continually reason-\ning from the current node. Next, we introduce\nthe involved two kinds of reward scores, namely\nanswer-aware reward and query-aware reward.\nAnswer-aware Reward. We first introduce the\nanswer-aware reward in the verification process.\nFirst, given a sub-query qt+1, we follow existing\nmethods (Lewis et al., 2020a) to retrieve top- K\ndocuments Dt+1 = {dt+1,k}K\nk=1 from the exter-\nnal corpus. Based on the retrieved documents, we\nthen employ the reward model to assign an answer-\naware reward ra to the currently generated answer\nat+1 from the internal knowledge of LLMs. Specif-\nically, there are overall three cases for the knowl-\nedge consistency between at+1 and Dt+1 with dif-\nferent rewards:\nra =\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n1, if at+1 cannot be verified by Dt+1\n2, if at+1 is in conflict with Dt+1\n3, if at+1 is aligned with Dt+1\nNote that in the second case (i.e., at+1 is in conflict\nwith Dt+1), we assign a moderate score 2 to the an-\nswer because we will refine at+1 with a new poten-\ntial answer \u02dcat+1 from the external knowledge Dt+1\nto support the policy model to continually reason\nfrom the current node. However, if the answerat+1\ncannot be verified by the external knowledge, we\nwill assign the lowest score 1 to the answer, avoid-\ning the policy model from exploring the potentially\nrisky solution space.\nQuery-aware Reward. In addition to evaluating\nthe consistency of the generated answer with ex-\nternal knowledge, we employ the reward model\nto provide a query-aware reward rq for mea-\nsuring the plausibility of the planned sub-query\nqt+1 based on the historical context information\nfrom the root node to current node, i.e., H =\n{q0; \u27e8q1, a1\u27e9; ...; \u27e8qt, at\u27e9}. If the sub-query evalu-\nated by the reward model is logically inconsistent\nwith the history plan, the score rq is set to 0; other-\nwise, it is set to 1. Therefore, the final reward r for\nthe expanded node st+1 is computed as r = ra \u00b7 rq.\nThis step aims to prevent the policy model from\ncontinuing to reason along illogical sub-queries.\nAfter obtaining the final reward for the newly\nexpanded node, we backpropagate the reward to up-\ndate the value of nodes from the root nodes0 to the\ncurrent node st+1. For each node s0, s1, ..., st+1 in\nthe path, its number of visits N(s) and the value\nV (s) will be updated according to Eq. 2. These up-\ndated values are used in the UCT algorithm in Eq. 1\nto guide the node selection at the next iteration.\n4.5 Reward Model Training\nIn the reward modeling process, the capacity of the\nreward model critically influences the search pro-\ncess and ultimate answer accuracy. However, uti-\nlizing close-source model API or very large LLMs\nincurs substantial computational costs for deploy-\nment. Hence, we adopt a knowledge distillation\ntechnique to transfer capabilities from an advanced\nLLM, which usually has more parameters, to a rel-\natively smaller model. This involves two phases:\ndata synthesis and instruction fine-tuning.\nDuring data synthesis, we mix up training sets\nfrom our evaluation datasets to maintain diversity.\nFirst, we adopt in-context learning to instruct the\npolicy model to generate a CoT format solution\nand then break down into multiple sub-steps, each\nincorporating the input question, accumulated rea-\nsoning paths, and a sub-query specific to the current\nstep. To further ensure diversity, only one random\nstep from each sample is selected for subsequent\ninstruction data creation. We then employ a more\nadvanced LLM (i.e., GPT-4o-mini) combined with\na retrieval system to evaluate the sub-query and its\nanswer for each step (Section 4.4), and filter the\noutput that fails to meet the format criteria. Finally,\nwe compile a dataset of intermediate steps and their\nquery and answer rewards from an advanced LLM.\nIn the instruction fine-tuning phase, we utilize the\nsynthetic samples to fine-tune a smaller LLM (i.e.,\nLlama-3.1-8B-Instruct), thereby enhancing its ca-\npabilities in reward modeling.\n5 Experiments\n5.1 Experimental Setup\nDatasets and Evaluation Metrics. We select\nfour typical complex multi-hop question-answering\ndatasets, i.e., HotpotQA (Yang et al., 2018), 2Wiki-\nMultihopQA (Ho et al., 2020), MusiQue (Trivedi\net al., 2022), and StrategyQA (Geva et al., 2021).\nFor evaluation metrics, we use Exact Match (EM),\nF1 score, and Cover Exact Match (Cover EM),\n\nwhere Cover EM measures whether the ground\ntruth answer is covered in the generated answer.\nWe randomly select 100 samples from the whole\nvalidation sets of each dataset as our final test set\nfor all baselines and our method.\nBaselines. We compare RAG-Star to the follow-\ning two types of baselines based on GPT-4o and\nLlama-3.1-8B-Instruct:\n\u2022 Vanilla prompting methods including direct\nprompting, Chain-of-Thought (CoT), and standard\nRAG. Direct prompting instructs the model to di-\nrectly generate answers and CoT incorporates in-\ntermediate reasoning steps, which are all based on\nthe inherent knowledge of LLMs. Standard RAG\nfirst retrieves documents from Wikipedia based on\nDPR (Karpukhin et al., 2020) as prompts and then\ngenerates the final answers.\n\u2022 Improved RAG methods including Iterative\nRAG (Xu et al., 2024), Judge-then-retrieve (Asai\net al., 2024), and Generate-then-retrieve (Wang\net al., 2023). We reimplement all of these baselines\nin our experiments. Iterative RAG iteratively de-\ncomposes the input question into sub-queries for\nretrieval and generation, ultimately ensembling all\nintermediate answers; Judge-then-retrieve first de-\ncides whether the retrieval is needed, autonomously\ndeciding to utilize either internal or external knowl-\nedge to aid in generation; Generate-then-retrieve\nfirst generates an initial answer used for retrieving\nmore documents relevant to the question and then\ngenerates the final answer based on documents.\nImplementation Details. We use a closed-source\nmodel (GPT-4o) and an open-source model (Llama-\n3.1-8B-Instruct) as our policy models to measure\nthe performance of the RAG-Star framework. For\nthe reward models, we use GPT-4o-mini and a\nfine-tuned Llama-3.1-8B-Instruct. For HotpotQA,\nwe only use the abstract of articles in Wikipedia\n2017 dump as the retrieval corpus following Yang\net al. (2018), while for other datasets, we use the\nwhole articles in Wikipedia 2018 dump (Karpukhin\net al., 2020). Moreover, for the retrieval model, we\nuse FAISS for index building and BGE-large-en-\nv1.5 (Xiao et al., 2023) for dense passage retrieval.\nFor all retrieval-based baselines, we retrieve top-5\ndocuments and employ greedy search for decoding\nwith a temperature of 0. For RAG-Star, we set the\nmaximum number of simulations to 50 and a maxi-\nmum of 6 layers. In UCT algorithm, the weight w\nto control the exploration and exploitation is set to\n0.2. We also retrieve top-5 documents and sample\nthree sub-queries at a time (mq = 3) with temper-\nature 1.0 and top-p sampling where p = 1.0. For\nanswer generation, we sample an answer using a\ntemperature of 0.9 and top-p sampling set to 1.0.\n5.2 Main Results\nTable 1 shows the results of RAG-Star and other\nbaselines across four representative multi-hop ques-\ntion answering datasets.\nFirstly, it can be observed that relatively smaller\nmodels (e.g., Llama-3.1-8B-Instruct) show limited\nperformance on these knowledge-intensive reason-\ning tasks, achieving below 10% across three met-\nrics in MusiQue. Although the Chain-of-Thought\ntechnique can slightly improve the answer recall\n(e.g., Cover EM scores of Llama-3.1-8B-Instruct\nand GPT-4o in MusiQue increase from 3.0% and\n19.0% to 16.0% and 27.0%, respectively), the\nmodel is prone to generating substantial irrele-\nvant information in the output, decreasing the over-\nall performance (e.g., F1 score of Llama-3.1-8B-\nInstruct drops from 21.9% to 7.1% on 2WikiMulti-\nhopQA).\nSecondly, based on the standard RAG, GPT-4o\nachieves substantial improvement in HotpotQA\n(e.g., Cover EM increases from 47.0% to 57.0%)\nbut exhibits a large decline in StrategyQA ( e.g.,\nCover EM from 73.0% to 62.0%), suggesting a po-\ntential conflict between external sources and inter-\nnal knowledge of LLMs. We speculate the reason\nmight be that using the retrieved information di-\nrectly as input incorporates some noises and makes\nthe LLM lost in the useful information. There-\nfore, by controlling the utilization of internal and\nexternal knowledge, Judge-then-Retrieve can sig-\nnificantly alleviate this issue (e.g., Cover EM from\n62.0% to 74.0% in StrategyQA). However, these\napproaches still present limited or even negative im-\nprovements in complex tasks (e.g., Cover EM from\n19.0% to 16.0% in MusiQue), necessitating effec-\ntive methods to consolidate external and internal\nknowledge.\nFinally, our approach outperforms all baselines\nacross most metrics in four datasets. RAG-Star\nintroduces a \u201cSystem 2\u201d-like slow and delibera-\ntive thinking process and employs RAG to verify\nand guide the multi-step reasoning process. By\nemploying retrieval-augmented verification, the re-\nward model can effectively encourage the model\ntowards plausible sub-query nodes or avert from\n\nMethod\nHotpotQA 2WikiMultihopQA MusiQue StrategyQA\nEM CEM F1 EM CEM F1 EM CEM F1 EM CEM F1\nLlama-3.1-8B-Instruct 14.0 25.0 26.0 9.0 29.0 21.9 2.0 3.0 3.9 63.0 65.0 63.0\n+ Chain-of-Tought 20.0 38.0 26.3 4.0 32.0 7.1 4.0 16.0 6.6 55.0 69.0 55.0\n+ Standard RAG 40.0 48.0 52.8 17.0 23.0 26.1 11.0 11.0 15.5 63.0 64.0 63.0\n+ Iterative RAG 26.0 31.0 36.9 22.0 23.0 26.0 7.0 11.0 15.9 61.0 63.0 61.0\n+ Generate-then-Retrieve 34.0 44.0 49.4 21.0 30.0 26.6 13.0 17.0 19.4 63.0 67.0 63.0\n+ Judge-then-Retrieve 39.0 48.0 53.9 18.0 26.0 26.8 10.0 10.0 16.0 58.0 63.0 58.0\n+ RAG-Star w Llama RM 42.0 44.0 54.4 34.0 38.0 42.0 13.0 18.0 22.2 71.0 72.0 71.0\n+ RAG-Star w GPT RM 46.0 49.0 60.0 38.0 43.0 46.8 22.2 27.0 30.7 67.6 69.0 67.6\nGPT-4o 43.0 47.0 56.7 36.0 42.0 45.7 13.0 19.0 24.3 70.0 73.0 70.0\n+ Chain-of-Tought 36.0 49.0 56.8 38.0 55.0 53.9 20.0 27.0 29.6 37.0 79.0 37.0\n+ Standard RAG 47.0 57.0 63.7 25.0 26.0 31.2 14.0 18.0 20.6 45.0 62.0 45.0\n+ Iterative RAG 47.0 59.0 63.3 19.0 24.0 26.3 15.0 26.0 25.5 32.0 74.0 32.0\n+ Generate-then-Retrieve 44.0 57.0 62.0 29.0 36.0 37.5 23.0 28.0 31.0 50.0 68.0 50.0\n+ Judge-then-Retrieve 44.0 50.0 58.6 28.0 29.0 32.2 14.0 16.0 22.8 72.0 74.0 72.0\n+ RAG-Star w Llama RM 48.0 54.0 66.3 47.0 68.0 62.8 25.0 36.0 39.0 61.0 86.0 61.0\n+ RAG-Star w GPT RM 48.0 57.0 68.6 48.0 63.0 61.7 29.0 40.0 43.5 60.0 81.0 60.0\nTable 1: Evaluation results on four representative multi-hop question answering tasks. \u201cRM\u201d is short for reward\nmodel. The bold and underline fonts denote the best and second best results in each dataset, respectively.\nMethod\nGPT-4o Llama3.1-8B\nCEM F1 CEM F1\nRAG-Star (Ours) 84.0 68.3 75.0 73.3\nw/o Query Score 82.0 68.0 71.0 69.0\nw/o Answer Score 80.0 66.3 66.0 65.3\nw/o Retrieval 78.0 67.3 67.0 66.0\nw/o Refine 77.0 68.2 70.0 68.1\nTable 2: Ablation study in StrategyQA.\npotential risky nodes. For example, equipped with\nour RAG-Star framework, Llama-3.1-8B-Instruct\nachieves higher scores in two challenging reason-\ning datasets, i.e., 2WikiMultihopQA and MusiQue,\nsignificantly beyond all baseline methods.\n5.3 Further Analysis\nWe report further analysis in StrategyQA with ran-\ndomly selected 100 samples \u2013 we have similar find-\nings in other datasets.\nAblation Study. To validate the effectiveness of\nour proposed framework, we conduct an ablation\nanalysis of its key design elements. We design\nfour variants: (1) w/o Retrieval removes the re-\ntrieved documents in reward modeling; (2) w/o\nRefine does not refine the conflict answer with re-\ntrieved documents in reward modeling; (3) w/o\nQuery Reward removes the query-aware reward rq\nfor scoring; and (4) w/o Answer Reward removes\nthe answer-aware reward ra for scoring. We show\nthe results in Table 2. It is clear that all the vari-\nFigure 2: Cover EM performance on the StrategyQA\nw.r.t. the number of simulations (Left) or the number of\ntraining data (Right).\nants perform worse than the original method, in-\ndicating the effectiveness of each component in\nour framework. Specifically, the performance of\nw/o Retrieval drops significantly for Llama-3.1-8B,\nindicating that using external knowledge for ver-\nification can be highly beneficial for the inherent\nreasoning of LLMs. Similarly, w/o Refine leads to a\ndecline in model performance, which highlights the\nimportance of repurposing external sources for cor-\nrecting the errors in the model\u2019s reasoning process.\nMoreover, both w/o Query Reward and w/o Answer\nReward variants lead to a substantial performance\ndecline, which suggests that the consistency and\nlogical plausibility of intermediate sub-queries and\nanswers are both critical for the model to plan the\ncorrect path towards the final answer.\nEffect of Simulation Scaling. Typically, scal-\ning the simulation iterations will lead to a higher\n\nWas Christian born earlier?Reward: 0\nChristian,born14December1972,isaDanish\u2026 It\u2019sintheAir is \u2026 comedydirectedbyAnthony\u2026 Anthony (10November1901to 19May1964) \u2026\nWhich film has the director born later, Life Hits or It\u2019s In The Air?(Answer: Life Hits)\nSub-query:When was the director of\u201cLife Hits\u201d born?Answer:1889    Reward: 0 Sub-query: Who is the director of It\u2019s In The Air?Answer:Reward: 2AnthonyClaudeSub-query: When wasAnthony born?Answer:1901Reward:3\u2026LifeHitsisa2006dramafilmdirectedbyChristian\u2026\nSub-query: Who is the director of \u201cLife Hits\u201d ?Answer: Christian Reward: 3\nSub-query:When wasChristian born?Answer:Reward:219981972\u2026Input Question\n\u2705\n \u274erefine\nWhat's Christian's date of birth? Reward: 1\nVerification byRetrieved Documents\n\u274erefine\n\u2705\nTherefore, the final answer is Life HitsOutputFailed NodeSuccess NodeAnswer Node\nFigure 3: A qualitative example showing the deliberative reasoning process of RAG-Star in 2WikiMultihopQA.\nlevel of task-solving capability. To explore the re-\nlationship between simulation scaling and the final\nperformance of RAG-Star, we test our model under\ndifferent maximum simulation iterations. Specifi-\ncally, we vary the maximum simulation rounds in\na set {10, 20, 30, 40, 50, 60}, and evaluate Llama\nand GPT-4o in StrategyQA with GPT-4o-mini as\nthe reward model. The results are presented in the\nFigure 2. We can see that as the maximum number\nof simulation increases, the model\u2019s performance\ngradually improves, although the average time con-\nsumed also rises to some extent. This highlights\nthat scaling the test-time computation can further\npromote more thorough exploration and exploita-\ntion by the policy model within the search space.\nHowever, as the number of simulations further in-\ncreases, the performance of the policy model tends\nto be saturated. Due to the limitation of inherent\nknowledge, the policy model cannot benefit a lot\nfrom conducting more simulations.\nEffect of Reward Model. In our framework, the\nreward model is used to assess the logical plausibil-\nity of the sub-query and the consistency between\nthe output answer and external sources. In this part,\nwe aim to explore how to train open-source reward\nmodels (i.e., Llama-3.1-8B-Instruct) to achieve per-\nformance comparable to closed-source LLMs (i.e.,\nGPT-4o-mini) by varying amounts of training data\nfrom 20K to 80K. Specifically, we employ different\namounts of training data to fine-tune Llama-3.1-8B-\nInstruct and use the fine-tuned model to evaluate\nthe sub-query and its answer. As shown in Fig-\nure 2, we can see that as the amount of training\ndata increases, the reward model can achieve more\naccurate verification quality, significantly benefit-\ning the planning and reasoning of the policy model.\nHowever, the performance gains tend to saturate at\nlater stages, necessitating instruction tuning data\nwith higher diversity and quality.\n5.4 Case Study\nTo facilitate understanding of the entire workflow\nof our proposed RAG-Star, we present a qualita-\ntive analysis in 2WikiMultihopQA. Throughout\nthe search process, the LLM initializes the input\nquestion as root node and conducts multiple simu-\nlations, eventually reaching the terminal leaf node,\nwhich can be vividly represented as a tree. As\nshown in Figure 3, after selecting the first query\n(i.e., Who is the director of \u201cLife Hits\u201d?), the model\nexpands multiple children nodes by repeated sam-\npling. At the next iteration, the model refines the\ngenerated answer ( i.e., 1998 ) for the sub-query\n(\u201cWhen was Christian born?\u201d) based on retrieved\ndocuments and the reward model returns an overall\nscore of 2. By iterating the multi-step reasoning\nand retrieval-augmented verification processes for\nseveral rounds, the model outputs the final answer\n(i.e., Life Hits). In the task-solving process, the pol-\nicy model generates an answer to the current sub-\nquery based on its internal knowledge, which might\nbe erroneous due to the limited pre-training corpus\nin time or the memorization mistakes. Therefore,\nthe external knowledge can be beneficial to validate\nthe correctness of inherent knowledge of LLMs, ef-\nfectively guiding the model to plan a reasonable\npath.\n6 Conclusion\nIn this work, we proposed RAG-Star, a novel RAG\napproach for leveraging external retrieval technique\nto enhance the multi-step reasoning capabilities of\nLLMs. RAG-Star employed Monte Carlo Tree\nSearch to search intermediate sub-queries and cor-\nresponding answers. Moreover, RAG-Star intro-\nduced retrieval-augmented verification to evaluate\n\nthe plausibility and consistency of the planned sub-\nqueries and answers based on a query-aware and\nan answer-aware reward. At each iteration, RAG-\nStar conducted node selection, plan expansion, re-\nward modeling, and reward backpropagation se-\nquentially to consolidate the internal knowledge of\nLLMs and external knowledge from RAG. Exten-\nsive experiments on several datasets showed that\nour proposed RAG-Star outperforms the traditional\nRAG and reasoning methods.\nLimitations\nDespite the great efforts that we have made, the\nexperimental analysis is still limited due to the\nmassive computational cost of tree-based search\napproaches. We will investigate into more types\nof complex reasoning tasks and datasets. In our\nmodel, we only leverage Monte Carlo Tree Search\nto conduct our deleberative reasoning process. we\nmay consider investigate more kinds of search algo-\nrithms to verify the generalization and robustness\nof our proposed framework. Moreover, the perfor-\nmance of our model is affected by the feedback\nquality provided by the reward model. Therefore, a\nwell-trained and performant reward model is impor-\ntant for guiding the reasoning process. We will con-\nsider other fine-tuning strategies and more LLMs\nin reward modeling.\nReferences\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206\u20132240. PMLR.\nBradley C. A. Brown, Jordan Juravsky, Ryan Saul\nEhrlich, Ronald Clark, Quoc V . Le, Christopher R\u00e9,\nand Azalia Mirhoseini. 2024. Large language mon-\nkeys: Scaling inference compute with repeated sam-\npling. CoRR, abs/2407.21787.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nJifan Chen, Shih-Ting Lin, and Greg Durrett. 2019.\nMulti-hop question answering via reasoning chains.\nCoRR, abs/1910.02610.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. arXiv preprint\narXiv:2312.10997.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? A question answering benchmark with\nimplicit reasoning strategies. Trans. Assoc. Comput.\nLinguistics, 9:346\u2013361.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning , pages 3929\u20133938.\nPMLR.\nPeter E. Hart, Nils J. Nilsson, and Bertram Raphael.\n1968. A formal basis for the heuristic determina-\ntion of minimum cost paths. IEEE Trans. Syst. Sci.\nCybern., 4(2):100\u2013107.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing A multi-hop\nQA dataset for comprehensive evaluation of reason-\ning steps. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 6609\u20136625. International Committee on\nComputational Linguistics.\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 1049\u20131065. Association for Computa-\ntional Linguistics.\nZhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li,\nHaoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan\nYe, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing\nYang, Ting Wu, Binjie Wang, Shichao Sun, Yang\nXiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei\nQin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng,\nShaoting Zhang, Dahua Lin, Yu Qiao, and Pengfei\nLiu. 2024. Olympicarena: Benchmarking multi-\ndiscipline cognitive reasoning for superintelligent AI.\nCoRR, abs/2406.12753.\n\nDaniel Kahneman. 2011. Thinking, fast and slow. Far-\nrar, Straus and Giroux.\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nLevente Kocsis and Csaba Szepesv\u00e1ri. 2006. Bandit\nbased monte-carlo planning. In Machine Learning:\nECML 2006, 17th European Conference on Machine\nLearning, Berlin, Germany, September 18-22, 2006,\nProceedings, volume 4212 of Lecture Notes in Com-\nputer Science, pages 282\u2013293. Springer.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020a. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459\u20139474.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harri-\nson Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2024. Let\u2019s verify step by step. In The Twelfth In-\nternational Conference on Learning Representations,\nICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\nReview.net.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023. Faithful chain-of-\nthought reasoning. In Proceedings of the 13th In-\nternational Joint Conference on Natural Language\nProcessing and the 3rd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics, IJCNLP 2023 -Volume 1: Long Papers,\nNusa Dua, Bali, November 1 - 4, 2023 , pages 305\u2013\n329. Association for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report. OpenAI Blog.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang\nGeng, Jacob Eisenstein, Rishabh Agarwal, Alekh\nAgarwal, Jonathan Berant, and Aviral Kumar.\n2024. Rewarding progress: Scaling automated pro-\ncess verifiers for llm reasoning. arXiv preprint\narXiv:2410.08146.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur\nGuez, Laurent Sifre, George van den Driessche, Ju-\nlian Schrittwieser, Ioannis Antonoglou, Vedavyas\nPanneershelvam, Marc Lanctot, Sander Dieleman,\nDominik Grewe, John Nham, Nal Kalchbrenner, Ilya\nSutskever, Timothy P. Lillicrap, Madeleine Leach,\nKoray Kavukcuoglu, Thore Graepel, and Demis Has-\nsabis. 2016. Mastering the game of go with deep neu-\nral networks and tree search. Nat., 529(7587):484\u2013\n489.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioan-\nnis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore\nGraepel, Timothy P. Lillicrap, Karen Simonyan, and\nDemis Hassabis. 2017. Mastering chess and shogi\nby self-play with a general reinforcement learning\nalgorithm. CoRR, abs/1712.01815.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Ku-\nmar. 2024. Scaling LLM test-time compute optimally\ncan be more effective than scaling model parameters.\nCoRR, abs/2408.03314.\nRichard Sutton. 2019. The bitter lesson. Incomplete\nIdeas (blog), 13(1):38.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V . Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. 2023. Challenging\nbig-bench tasks and whether chain-of-thought can\nsolve them. In Findings of the Association for Com-\nputational Linguistics: ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pages 13003\u201313051. Association for\nComputational Linguistics.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTrans. Assoc. Comput. Linguistics, 10:539\u2013554.\nChaojie Wang, Yanchen Deng, Zhiyi Lv, Zeng Liang,\nJujie He, Shuicheng Yan, and Bo An. 2024a. Q*:\nImproving multi-step reasoning for llms with deliber-\native planning. CoRR, abs/2406.14283.\nFei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen,\nand Sercan \u00d6 Ar\u0131k. 2024b. Astute rag: Overcom-\ning imperfect retrieval augmentation and knowledge\nconflicts for large language models. arXiv preprint\narXiv:2410.07176.\nLiang Wang, Nan Yang, and Furu Wei. 2023.\nQuery2doc: Query expansion with large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2023, Singapore, December 6-10, 2023,\npages 9414\u20139423. Association for Computational\nLinguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,\nand Denny Zhou. 2022. Chain-of-thought prompting\nelicits reasoning in large language models. In Ad-\nvances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Process-\ning Systems 2022, NeurIPS 2022, New Orleans, LA,\nUSA, November 28 - December 9, 2022.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding.\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng,\nand Tat-Seng Chua. 2024. Search-in-the-chain: Inter-\nactively enhancing large language models with search\nfor knowledge-intensive tasks. In Proceedings of the\nACM on Web Conference 2024, WWW 2024, Singa-\npore, May 13-17, 2024, pages 1362\u20131373. ACM.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2369\u20132380. Association for Computational\nLinguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint.\nEric Zelikman, Georges Harik, Yijia Shao, Varuna\nJayasiri, Nick Haber, and Noah D. Goodman. 2024.\nQuiet-star: Language models can teach themselves\nto think before speaking. CoRR, abs/2403.09629.\nDi Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong\nLi, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco\nPavone, Yuqiang Li, et al. 2024. Llama-berry: Pair-\nwise optimization for o1-like olympiad-level mathe-\nmatical reasoning. arXiv preprint arXiv:2410.02884.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023a. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2023, Toronto, Canada, July\n9-14, 2023, pages 5823\u20135840. Association for Com-\nputational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023b. A survey of large language models. CoRR,\nabs/2303.18223.\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang,\nYongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yu-\njiu Yang. 2023. Solving math word problems via\ncooperative reasoning induced language models. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 4471\u20134485. Association for Computa-\ntional Linguistics.",
  "full_text_length": 48209,
  "link_pdf": "https://arxiv.org/pdf/2412.12881v1",
  "paper_id": "2412.12881v1"
}