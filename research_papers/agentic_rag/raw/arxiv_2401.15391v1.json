{
  "source": "arxiv",
  "query": "Agentic RAG",
  "fetched_at": "2025-11-21T17:17:55.034549",
  "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
  "url": "http://arxiv.org/abs/2401.15391v1",
  "content": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for\nMulti-Hop Queries\nYixuan Tangand Yi Yang\nHong Kong University of Science and Technology\n{yixuantang,imyiyang}@ust.hk\nAbstract\nRetrieval-augmented generation (RAG) aug-\nments large language models (LLM) by re-\ntrieving relevant knowledge, showing promis-\ning potential in mitigating LLM hallucinations\nand enhancing response quality, thereby facil-\nitating the great adoption of LLMs in prac-\ntice. However, we find that existing RAG sys-\ntems are inadequate in answering multi-hop\nqueries, which require retrieving and reasoning\nover multiple pieces of supporting evidence.\nFurthermore, to our knowledge, no existing\nRAG benchmarking dataset focuses on multi-\nhop queries. In this paper, we develop a novel\ndataset, MultiHop-RAG, which consists of a\nknowledge base, a large collection of multi-\nhop queries, their ground-truth answers, and\nthe associated supporting evidence. We detail\nthe procedure of building the dataset, utiliz-\ning an English news article dataset as the un-\nderlying RAG knowledge base. We demon-\nstrate the benchmarking utility of MultiHop-\nRAG in two experiments. The first experiment\ncompares different embedding models for re-\ntrieving evidence for multi-hop queries. In the\nsecond experiment, we examine the capabili-\nties of various state-of-the-art LLMs, includ-\ning GPT-4, PaLM, and Llama2-70B, in rea-\nsoning and answering multi-hop queries given\nthe evidence. Both experiments reveal that ex-\nisting RAG methods perform unsatisfactorily\nin retrieving and answering multi-hop queries.\nWe hope MultiHop-RAG will be a valuable re-\nsource for the community in developing effec-\ntive RAG systems, thereby facilitating greater\nadoption of LLMs in practice. The MultiHop-\nRAG and implemented RAG system is publicly\navailable athttps://github.com/yixuantt/\nMultiHop-RAG/.\n1 Introduction\nThe emergence of large language models (LLMs),\nsuch as ChatGPT, has fostered a wide range of inno-\nvations, powering intelligent chatbots and other nat-\nural language processing (NLP) applications (Ope-\nFigure 1: RAG with multi-hop query.\nnAI, 2023). One promising use case is Retrieval-\nAugmented Generation (RAG) (Asai et al., 2023),\nwhich optimizes the output of a large language\nmodel by referencing an external knowledge base\noutside of the LLM training data sources before\ngenerating a response. RAG improves LLM\u2019s re-\nsponse (Borgeaud et al., 2022) and also mitigates\nthe occurrence of hallucinations, thereby enhancing\nthe models\u2019 credibility (Gao et al., 2023). LLM-\nbased frameworks, such as LlamaIndex (Liu, 2022)\nand LangChain (Chase, 2022), specialize in sup-\nporting RAG pipelines.\nIn real-world Retrieval-Augmented Generation\n(RAG) applications, a user\u2019s query often necessi-\ntates retrieving and reasoning over evidence from\nmultiple documents, a process known asmulti-hop\nquery. For instance, consider financial analysis us-\ning a database of financial reports. A financial ana-\nlyst might query, Which company among Google,\nApple, and Nvidia reported the largest profit mar-\ngins in their third-quarter reports for 2023? or\ninquire about a specific company\u2019s performance\nover time, such as How does Apple\u2019s sales trend\nlook over the past three years? These queries re-\nquire evidence from multiple documents to formu-\nlate an answer. Due to the multifaceted nature of\nsuch queries, involving information from various\nsources, traditional similarity matching methods\nlike cosine similarity between query and financial\narXiv:2401.15391v1  [cs.CL]  27 Jan 2024\n\nNews source Fortune Magazine The Sydney Morning Herald\nEvidence Back then, just like today, home prices had boomed\nfor years before Fed officials were ultimately forced\nto hike interest rates aggressively in an attempt to\nfight inflation.\nPostponements of such reports could complicate\nthings for the Fed, which has insisted it will make\nupcoming decisions on interest rates based on what\nincoming data say about the economy.\nClaim Federal Reserve officials were forced to aggressively\nhike interest rates to combat inflation after years of\nbooming home prices.\nThe Federal Reserve has insisted that it will base its\nupcoming decisions on interest rates on the incoming\neconomic data.\nBridge-Topic Interest rate hikes to combat inflation Interest rate decisions based on economic data\nBridge-Entity Federal Reserve Federal Reserve\nQuery Does the article from Fortune suggest that the Federal Reserve\u2019s interest rate hikes are a response to past\nconditions, such as booming home prices, while The Sydney Morning Herald article indicates that the\nFederal Reserve\u2019s future interest rate decisions will be based on incoming economic data?\nAnswer Yes\nTable 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased\nclaim, the bridge-topic and bridge-entity, and the corresponding answer.\nreport chunk embeddings might not yield optimal\nresults. We demonstrate this multi-hop retrieval\nprocess in Figure 1.\nHowever, existing RAG benchmarks, such as\nRGB (Chen et al., 2023) and RECALL (Liu et al.,\n2023), mainly evaluate a simple case where the an-\nswer of a query can be retrieved and solved using\none single piece of evidence. None of these bench-\nmarks assess the retrieval and reasoning capability\nof LLMs for complex multi-hop queries. To ad-\ndress this gap and make RAG benchmarking more\nclosely resemble real-world scenarios, in this paper,\nwe introduce MultiHop-RAG. To our knowledge,\nMultiHop-RAG is one of the first RAG datasets\nfocusing specifically on multi-hop queries.\nBased on the RAG queries commonly encoun-\ntered in real-world scenarios, we first categorize\nmulti-hop queries into four types: Inference query,\nComparison query , Temporal query , and Null\nquery. The first three types \u2014 Inference, Com-\nparison, and Temporal \u2014 require the retrieval and\nanalysis of evidence from multiple sources, encom-\npassing tasks like inferring relationships, compar-\ning data points, and sequencing events over time.\nThe Null query represents a scenario where the\nquery cannot be derived from the knowledge base.\nThis category is crucial for assessing whether an\nLLM might hallucinate an answer to a multi-hop\nquery when the retrieved text lacks relevance.\nWe construct our RAG knowledge base using a\ncollection of news articles. Using GPT-4 as a data\ngenerator, we then take an extensive procedure to\nconstruct a diverse set of multi-hop queries, each\nrequiring the retrieval and reasoning over multiple\ndocuments. An example of query construction is\nshown in Table 1. First, we begin by extracting\nfactual sentences from each news article as evi-\ndence. For example, an extracted piece of evidence\nfrom an article may state: \u201cBack then, just like\ntoday, home prices had boomed for years before\nFed officials were ultimately forced to hike interest\nrates aggressively in an attempt to fight inflation.\u201d\nSecond, we input each evidence piece into GPT-4,\nprompting it to rephrase the evidence into a claim.\nThis claim is clarified with a disambiguated topic\nand entity. For instance, GPT-4 might rephrase the\naforementioned evidence into: \u201cFederal Reserve\nofficials were forced to aggressively hike interest\nrates to combat inflation after years of booming\nhome prices\u201d, identifying \u201cInterest rate hikes to\ncombat inflation\u201d as the topic and \u201cFederal Re-\nserve\u201d as the entity. These topics and entities act as\nbridges for constructing multi-hop queries, known\nas bridge-topic or bridge-entity. Next, we use GPT-\n4 to generate specific multi-hop queries related to\nthe same bridge-topic or bridge-entity, accompa-\nnied by the correct answers. Lastly, we undertake\na validation step to ensure the data quality.\nWe demonstrate the benchmarking capabilities\nof MultiHop-RAG using two experiments, utilizing\na RAG system implemented with LlamaIndex (Liu,\n2022). The first experiment involves a comparison\nof different embedding models for retrieving rele-\nvant evidence for multi-hop queries. In the second\nexperiment, we assess the reasoning and answering\nabilities of various state-of-the-art LLMs, including\nGPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B,\nand Mixtral-8x7B, for multi-hop queries when re-\ntrieved text is provided. The results from both ex-\nperiments indicate that the current RAG implemen-\ntations are inadequate for effectively retrieving and\nanswering multi-hop queries. We publicly release\n\nthis challenging MultiHop-RAG dataset and hope it\nwill be a valuable resource for the community in de-\nveloping and benchmarking RAG systems, thereby\nunleashing the great potential of generative AI in\npractice.\n2 RAG with multi-Hop queries\n2.1 Retrieval-augmented Generation (RAG)\nIn an RAG application, we utilize an external cor-\npus, denoted as D, which comprises multiple docu-\nments and serves as the knowledge base. Each doc-\nument within this corpus, represented as di \u2208 D, is\nsegmented into a set of chunks.These chunks are\nthen transformed into vector representations using\nan embedding model and stored in an embedding\ndatabase. Given a user query q, the system typi-\ncally retrieves the top-K chunks that best match the\nquery. These chunks constitute the retrieval set\nfor query q, represented as Rq = {r1, r2, ..., rK}.\nThe retrieved chunks, combined with the query\nand an optional prompt, are then fed into an LLM\nto generate a final answer, following the format:\nLLM(q, Rq, prompt) \u2192 answer.\n2.2 Multi-Hop Query\nWe define a multi-hop query as one that requires\nretrieving and reasoning over multiple pieces of\nsupporting evidence to provide an answer. In other\nwords, for a multi-hop query q, the chunks in the\nretrieval set Rq collectively provide an answer\nto q. For example, the query \"Which company\namong Google, Apple, and Nvidia reported the\nlargest profit margins in their third-quarter reports\nfor 2023?\" requires 1) retrieving relevant pieces of\nevidence related to profit margins from the reports\nof the three companies; 2) generating an answer by\ncomparing and reasoning from the multiple pieces\nof retrieved evidence. This differs from a single-\nhop query such as \"What is Google\u2019s profit margin\nin the third-quarter reports for 2023,\" where the\nanswer can be directly derived from a single piece\nof evidence.\nBased on the queries commonly used in real-\nworld RAG systems, we identify four types of\nmulti-hop queries. For each type, we present a\nhypothetical query within the context of a financial\nRAG system, where the knowledge base consists\nof a collection of annual reports.\nInference query:For such a query q, the answer\nis deduced through reasoning from the retrieval\nset Rq. An example of an inference query might\nbe: Which report discusses the supply chain risk of\nApple, the 2019 annual report or the 2020 annual\nreport?\nComparison query: For such a query q, the an-\nswer requires a comparison of evidence within the\nretrieval set Rq. For instance, a comparison query\nmight ask: Did Netflix or Google report higher\nrevenue for the year 2023?\"\nTemporal query:For such a query q, the answer\nrequires an analysis of the temporal information\nof the retrieved chunks. For example, a temporal\nquery may ask: Did Apple introduce the AirTag\ntracking device before or after the launch of the 5th\ngeneration iPad Pro?\nNull query:For such as queryq, the answer cannot\nbe derived from the retrieved set Rq. We include\nthe null query to assess the generation quality, es-\npecially regarding the issue of hallucination. For a\nnull query, even though a retrieved set is provided,\nan LLM should produce a null response instead\nof hallucinating an answer. For example, assum-\ning ABCD is a non-existent company, a null query\nmight ask: What are the sales of company ABCD\nas reported in its 2022 and 2023 annual reports?\n2.3 Evaluation Metrics\nAn RAG system handling multi-hop queries can be\nassessed from two key aspects: retrieval evaluation\nand generation evaluation.\nRetrieval Evaluation:Evidently, the quality of\nthe retrieval set Rq determines the final genera-\ntion quality. We compare the retrieved set with\nthe ground truth evidence associated with each\nquery, except for the null queries, as they have\nno evidence to derive from. Assuming the top-\nK chunks are retrieved, i.e., |Rq| = K, we use\nretrieval evaluation metrics including Mean Aver-\nage Precision at K (MAP@K), Mean Reciprocal\nRank at K (MRR@K), and Hit Rate at K (Hit@K).\nMAP@K measures the average top-K retrieval pre-\ncision across all queries. MRR@K calculates the\naverage of the reciprocal ranks of the first relevant\nchunk for each query, considering the top-K re-\ntrieved set. Hit@K metric measures the fraction of\nevidence that appears in the top-K retrieved set.\nResponse Evaluation: Since the multi-hop\nquery requires reasoning over multiple pieces of\nretrieved chunks, we can also evaluate the reason-\ning capability of the LLM by comparing the LLM\nresponse with the ground truth answer of the query.\n\nFigure 2: MultiHop-RAG Construction Pipeline.\n3 A Benchmarking Dataset:\nMultiHop-RAG\nIn this section, we provide detailed information\non the construction of the MultiHop-RAG dataset.\nSpecifically, we describe the process of creating a\nset of multi-hop queries, along with the correspond-\ning ground truth evidence sets and answers derived\nfrom a collection of news articles.\n3.1 MultiHop-RAG Construction\nStep 1: Dataset Collection.We download a news\ndataset using the mediastack API 1, a REST API in-\nterface delivering worldwide news data. The news\ndata source comprises various English-language\nwebsites covering a range of news categories: en-\ntertainment, business, sports, technology, health,\nand science. To mimic real-world RAG scenarios,\nwhere the knowledge base data, such as an enter-\nprise\u2019s internal data, may differ from the LLMs\u2019\ntraining data, we select news articles published\nfrom September 26, 2023, to December 26, 2023.\nThis timeframe extends beyond the knowledge cut-\noff of some widely-used LLMs, including Chat-\nGPT and LLaMA, as of the time of writing. This\nselection also helps in teasing out the possibility\nof the underlying LLM having been exposed to\nthese news articles. We only keep articles with a\ntoken length greater than or equal to 1,024. Every\n1https://mediastack.com/\nnews article is paired with metadata, including the\ntitle, publish date, author, category, URL, and news\nsource.\nStep 2: Evidence Extraction.For each article, we\nextract factual or opinion sentences using a trained\nlanguage model 2. These factual sentences are later\nused as evidence for answering multi-hop queries.\nWe retain only those news articles containing ev-\nidence that may have overlapping keywords with\nother news articles. This allows us to later create\nmulti-hop queries where the answer\u2019s evidences\nare drawn from multiple sources.\nStep 3: Claim, Bridge-Entity, Bridge-Topic Gen-\neration. Our goal is to use GPT-4 to automatically\ngenerate high-quality multi-hop queries using the\nevidence set. However, the raw evidence obtained\nfrom Step 2 is not ideal for query generation due\nto inconsistency in linguistic structure. For exam-\nple, some pieces of evidence use pronouns to refer\nto subjects and lack the actual entity in the text.\nTo address this, we employ GPT-4 to paraphrase\nthe evidence, which we refer to as claims, given\nthe original evidence and its context. To ensure\nconsistency between the generated claim and the\nevidence, we further perform fact-checking using\nthe UniEval (Zhong et al., 2022) framework to ver-\nify the alignment between the evidence and claim.\nAppendix A presents the prompt used for GPT-4\nfor claim generation.\nBridge-Entity and Bridge-Topic:The shared en-\ntity or topic across pieces of evidence is referred to\nas the bridge-entity or bridge-topic. These bridge-\nentities or bridge-topics can be used to link dif-\nferent pieces of evidence from which a multi-hop\nquery\u2019s answer is derived. For example, in a claim\nsuch as \u201cGoogle reports its third-quarter results for\n2023, showcasing a detailed overview of its finan-\ncial performance, including revenue growth, profit\nmargins\u201d, the term profit margin can be viewed as\na bridge-topic and the term Google can be viewed\nas a bridge-entity that links the different pieces of\nevidence. We prompt GPT-4 to identify the bridge-\nentity and bridge-topic for each claim. Appendix A\nalso presents the prompt used for GPT-4 for bridge\ngeneration.\nStep 4: Query and Answer Generation.In this\nstep, we leverage the bridge-entity or bridge-topic\nto generate multi-hop queries. Specifically, we first\ngroup the claims having the same bridge-entity or\n2https://huggingface.co/lighteternal/fact-or-opinion-xlmr-\nel\n\nbridge-topic into a claim set. We restrict the claim\nset to have at least two claims but no more than four\nclaims. For each type of query, we feed the claim\nset to GPT-4 and prompt it with an instruction to\ngenerate a query with information from each claim.\nBelow, we explain the specifications for different\nmulti-hop query types. In the construction of each\nquery, we also include the source of the news article\nwhere the supporting evidence is associated with\nto mimic real-world RAG scenarios. Appendix\nA presents the prompts used for GPT-4 for query\ngeneration.\nInference Query:These queries are formulated\nby synthesizing the various characterizations of the\nbridge-entity across multiple claims, with the final\nanswer being the identification of the entity itself.\nComparison Query: These queries are struc-\ntured to compare the similarities and differences\nrelated to the bridge entity or topic. The resultant\nanswer to such queries is typically a definitive \u201cyes\u201d\nor \u201cno\u201d, based on the comparison.\nTemporal Query: These queries explore the\ntemporal ordering of events across different points\nin time. The answer to such queries is typically a\n\u201cyes\u201d or \u201cno\u201d or a single temporal indicator word\nlike \u201cbefore\u201d or \u201cafter\u201d.\nNull Query: Null query is a query whose an-\nswer cannot be derived from the retrieved set. To\ncreate null queries, we generate multi-hop queries\nusing entities that do not exist in the existing bridge-\nentities. To add complexity, we also include fic-\ntional news source metadata when formulating\nthese questions, ensuring that the questions do not\nreference any contextually relevant content from\nthe knowledge base. The answer to the null query\nshould be \u201cinsufficient information\u201d or similar.\nStep 5: Quality Assurance.Finally, we use two\napproaches to reassure the dataset quality. First, we\nmanually review a subset sample of the generated\nmulti-hop queries, their corresponding evidence\nsets, and the final answers. The results of the man-\nual examination indicate a high degree of accuracy\nand data quality. Second, we utilize GPT-4 to as-\nsess each example in the dataset against the follow-\ning criteria: 1) The generated query must utilize\nall provided evidence in formulating the response;\n2) The query should be answerable solely based\non the provided evidence; 3) The response to the\ngenerated query should be either a single word or\na specific entity; 4) The query must conform to its\ndesignated query type.\nCategory Avg. Tokens Entry Count\ntechnology 2262.3 172\nentertainment 2084.3 114\nsports 2030.6 211\nscience 1745.5 21\nbusiness 1723.8 81\nhealth 1481.1 10\ntotal 2046.5 609\nTable 2: Descriptive statistics of the news article knowl-\nedge base in MultiHop-RAG.\nQuery Category Entry Count Percentage\nInference Query 816 31.92%\nComparison Query 856 33.49%\nTemporal Query 583 22.81%\nNull Query 301 11.78%\nTotal 2,556 100.00 %\nTable 3: The distribution of query types in MultiHop-\nRAG.\n3.2 Descriptive Statistics\nThe MultiHop-RAG dataset contains six different\ntypes of news articles, covering 609 distinct news,\nwith an average of 2,046 tokens. The distribution of\nthe news categories is shown in Table 2. MultiHop-\nRAG contains four types of multi-hop queries and\nthe distribution of these queries is shown in Table\n3. In total, about 88% of queries in the dataset are\nnon-null queries where answers can be retrieved\nand reasoned from the knowledge base. In addition,\nthe form of queries exhibits considerable diversity.\nApproximately 27% of interrogative queries start\nwith \"does,\" around 15% initiate with \"what,\" a\nsimilar proportion start \"which,\" and 14% begin\nwith \"who,\" with the remainder incorporating a\nsmall percentage of other interrogative words such\nas \"when.\" Moreover, the number of evidence re-\nquired to answer a multi-hop query varies. Table\n4 shows the distribution of evidence numbers for\neach query in the dataset. Around 42% of queries\ncan be answered using two pieces of evidence,\nwhile approximately 30% and 15% of queries can\nbe answered using three or four pieces of evidence,\nrespectively.\n4 Benchmarking RAG system using\nMultiHop-RAG\nMultiHop-RAG can be used as a benchmark for var-\nious RAG-related tasks. Broadly speaking, RAG-\n\nNum. of Evidence Needed Count Percentage\n0 (Null Query) 301 11.78%\n2 1078 42.18%\n3 779 30.48%\n4 398 15.56%\nTotal 2,556 100.00 %\nTable 4: The distribution of the number of evidence\nrequired to answer multi-hop queries in MultiHop-RAG.\nrelated tasks can be categorized as retrieval-related\ntasks and generation-related tasks. A retrieval-\nrelated task focuses on retrieving relevant text from\nthe knowledge base, while a generation-related task\nfocuses on generating high-quality responses given\nthe retrieved text. In this section, we showcase two\nuse cases for each task where MultiHop-RAG can\nbe employed.\n4.1 Retrieval-related Task\nAn important design choice in an RAG system is\nthe selection of the embedding model. An embed-\nding model converts data into numerical vectors\nand subsequently stores these vectors in embedding\ndatabases. In this experiment, we evaluate differ-\nent embedding models by examining their retrieval\nquality.\nExperiment Setup:We implement an RAG sys-\ntem using the LlamaIndex framework (Liu, 2022).\nWe partition the documents in the MultiHop-RAG\nknowledge base into chunks, each consisting of 256\ntokens. We then convert the chunks using an em-\nbedding model and save the embeddings into a vec-\ntor database. Similarly, in the retrieval step, we con-\nvert a query using the same embedding model and\nretrieve the top-K most relevant chunks that have\nthe highest cosine similarity with the query embed-\nding. In this experiment, we test a variety set of em-\nbedding models, including the ada-embeddings by\nOpenAI (text-embedding-ada-002, text-search-ada-\nquery-001), voyage-02 3, llm-embedder (Zhang\net al., 2023), bge-large-en-v1.5 (Xiao et al., 2023),\njina-embeddings-v2-base-en (G\u00fcnther et al., 2023),\ne5-base-v2 (Wang et al., 2022), and instructor-large\n(Su et al., 2023). NULL queries are excluded in\nthis experiment because there is no matching evi-\ndence to the query. Additionally, we also include\na Reranker module to examine the retrieval perfor-\nmance, using bge-reranker-large (Xiao et al., 2023).\nAfter retrieving 20 related chunks using the em-\n3https://www.voyageai.com/\nbedding model, we further select the top-K chunks\nusing the Reranker.\nExperiment Result:Table 5 shows the retrieval\nresult of using different embedding models. It\nshows that there is still a significant gap in retriev-\ning relevant evidence for the multi-hop queries.\nWhile Rerank can effectively improve retrieval rel-\nevance, the highest Hits@10 is only 0.7467 when\nthe Reranker technique is used. Moreover, the drop\nin the highest Hits@4 to 0.6625 is worrisome. In\npractical RAG systems, the underlying LLM of-\nten has a context window limit. As a result, the\nnumber of retrieved chunks is usually restricted to\na small number. The low values of the retrieval\nmetrics highlight the challenges in retrieving rele-\nvant pieces of evidence for multi-hop queries when\nusing direct similarity matching between the multi-\nhop query and text chunks.\n4.2 Generation-related Task\nThe underlying LLMs play a crucial role in gen-\nerating responses in an RAG system. In this ex-\nperiment, we evaluate the quality of generated re-\nsponses under two different settings. In the first\nsetting, we employ the best-performing retrieval\nmodel, namely voyage-02 with bge-reranker-large,\nas indicated in Table 5, to retrieve the top-K texts\nand then feed them into the LLM. In the second\nsetting, we use the ground-truth evidence associ-\nated with each query as the retrieved text for the\nLLM. This setting represents a ceiling performance\nfor testing the LLM\u2019s response capabilities, as it\nutilizes the actual evidences.\nExperiment Setup: In the first experiment, we\nretrieve top-6 chunks so that the total length of the\nretrieved text does not exceed 2,048. All queries\nin MultiHop-RAG are tested in the experiment.\nIn the second experiment, since the null queries\ndo not have associated evidence, we exclude this\ntype of query in the experiment. For the LLMs\nused in the experiment, we consider state-of-the-\nart commercial models, including GPT-4 (OpenAI,\n2023), GPT-3.5, Claude-2 (Anthropic, 2023), and\nGoogle-PaLM (Google, 2023). We obtain answers\nusing the provided API of the respective models.\nWe also assess some open-source models, includ-\ning Mixtral-8x7b-instruct (Jiang et al., 2024) and\nLlama-2-70b-chat-hf (Touvron et al., 2023).\nExperiment Results:Table 6 shows the response\naccuracy of different LLMs. First, we can see\nthat the response accuracy rate using the retrieved\n\nEmbedding Without Reranker With bge-reranker-large\nMRR@10 MAP@10 Hits@10 Hits@4 MRR@10 MAP@10 Hits@10 Hits@4\ntext-embedding-ada-002 0.4203 0.3431 0.6381 0.504 0.5477 0.4625 0.7059 0.6169\ntext-search-ada-query-001 0.4203 0.3431 0.6399 0.5031 0.5483 0.4625 0.7064 0.6174\nllm-embedder 0.2558 0.1725 0.4499 0.3189 0.425 0.3059 0.5478 0.4756\nbge-large-en-v1.5 0.4298 0.3423 0.6718 0.5221 0.563 0.4759 0.7183 0.6364\njina-embeddings-v2-base-en 0.0621 0.031 0.1479 0.0802 0.1412 0.0772 0.1909 0.1639\nintfloat/e5-base-v2 0.1843 0.1161 0.3556 0.2334 0.3237 0.2165 0.4176 0.3716\nvoyage-02 0.3934 0.3143 0.6506 0.4619 0.586 0.4795 0.7467 0.6625\nhkunlp/instructor-large 0.3458 0.265 0.5717 0.4229 0.5115 0.4118 0.659 0.5775\nTable 5: Retrieval performance of different embedding models.\nModels Accuracy\nRetrieved Chunk Ground-truth Chunk\nGPT-4 0.56 0.89\nChatGPT 0.44 0.57\nLlama-2-70b-chat-hf 0.28 0.32\nMixtral-8x7B-Instruct 0.32 0.36\nClaude-2.1 0.52 0.56\nGoogle-PaLM 0.47 0.74\nTable 6: Generation accuracy of LLMs.\nchunks is not satisfactory, with the state-of-the-\nart GPT-4 model achieving only 0.56 accuracy.\nThis is expected, because the retrieval component\nfalls short in retrieving relevant evidences from the\nknowledge base. Second, even when we provide\nthe LLM with the ground-truth evidences, we can\nsee that the response accuracy is far from being per-\nfect. Open source LLM such as Llama02-70B and\nMixtral-8x7B only achieve an accuracy of 0.32 and\n0.36 respectively. GPT-4 achieves strong reason-\ning capability with an accuracy of 0.89, followed\nby the second-based LLM Google-PaLM with an\naccuracy of 0.74.\nFigure 3 shows the detailed results of different\nquery types for GPT-4 and Mixtral-8x7B-instruct.\nBoth models show relatively high robustness on\nnull queries, meaning they are generally good at\ndetermining when a query cannot be answered\nbased on the retrieved text. This is encouraging be-\ncause one benefit of RAG is to mitigating the LLM\nhallucination issue by augmenting LLM with re-\ntrieval knowledge. However, Mixtral-8x7B model\nperforms significantly worse than the GPT-4 in\ncomparison and temporal queries. Upon reviewing\nthe incorrect responses, we find that Mixtral-8x7B\nfails to accurately handle logical negation, leading\nto misinterpretation of statements and thus a low\nperformance in the comparison queries. In addi-\ntion, Mixtral-8x7B often fails to correctly identify\nFigure 3: Generation accuracy for different query types.\nthe chronological order of events, which is crucial\nfor answering temporal queries where timing is a\nkey factor. Taken together, this experiment demon-\nstrates that there is still room for improvement in\nthe reasoning capabilities of LLMs, particularly\nthose that are open-source, for multi-hop queries.\n4.3 Other Use Cases\nBeyond embedding models and LLM generation,\nthere are other areas worth exploring. For exam-\nple, query decomposition is a widely utilized tech-\nnique in RAG frameworks, such as LLamaIndex.\nThis process involves breaking down the query\ninto smaller segments; it targets a single document\nfor retrieval and integrates the information subse-\nquently, thereby potentially enhancing retrieval ac-\ncuracy. Another advanced and promising approach\ninvolves building LLM-based agents that can au-\ntomatically plan and execute multi-hop queries,\nsuch as AutoGPT (Gravitas, 2023). Another area\nof interest is the hybrid retrieval approach, which\ncombines keyword and embedding matching tech-\n\nniques. We believe that there are many potential\nareas for enhancing RAG\u2019s performance on multi-\nhop queries, and the curated dataset MultiHop-\nRAG can be a valuable resource to the community.\n5 Related Work\nRAG Evaluation:As RAG systems gain increas-\ning popularity, a variety of RAG benchmarking\ndatasets and evaluation tools have been developed.\nFor instance, RGB (Chen et al., 2023) and RE-\nCALL (Liu et al., 2023) evaluate the performance\nof LLMs in generating responses for RAG systems\nunder conditions involving noisy, integrative, and\ncounterfactual queries. However, both datasets pri-\nmarily focus on evaluating the generation aspect\nof RAG systems without specifically addressing\ntheir retrieval accuracy. In addition, recent ad-\nvancements have been made in automated RAG\nevaluation tools, such as ARES (Saad-Falcon et al.,\n2023) and RAGAS (Es et al., 2023). These tools\nutilize LLMs to automatically assess the quality of\nRAG generation, yet they do not introduce bench-\nmarking datasets. Our work introduces one of the\nfirst RAG benchmarking datasets, consisting of a\nknowledge base, a large collection of multi-hop\nqueries, their ground-truth answers, and the associ-\nated supporting evidence, thereby complementing\nexisting RAG evaluations.\nRetrieval datasets: Apart from the context of\nRAG, several benchmarking datasets exist for in-\nformation retrieval evaluation. The FEVER (Fact\nExtraction and VERification) dataset, for instance,\ncontains claims classified as Supported, Refuted,\nor NotEnoughInfo by the given Wikipedia article\n(Thorne et al., 2018). Similarly, the SciFact dataset\ncomprises scientific claims paired with evidence-\ncontaining abstracts (Wadden et al., 2020). How-\never, the claims in both datasets are single-hop\nstatements, and the supporting evidence is from one\nsingle article, in contrast to the multi-hop queries\ndiscussed in this paper. Another dataset, HoVer,\ninvolves claims that require extracting and reason-\ning from multiple Wikipedia articles (Jiang et al.,\n2020). However, unlike our dataset, HoVer focuses\nsolely on classifying claims as either supported or\nnot supported by the articles without evaluating\nan LLM generation step. Moreover, in HoVer, the\nWikipedia articles from which evidence is drawn\nare given for claim verification, which is signifi-\ncantly different from our setting, where relevant\npieces of evidence need to be extracted from a\nlarge knowledge base. Separately, (Kamalloo et al.,\n2023) evaluates a range of commercial embedding\nAPIs for information retrieval, but this evaluation\nis not contextualized within the framework of RAG\nsystems either.\nMulti-document QA datasets: Question-\nanswering (QA) is a fundamental task in NLP, and\nseveral popular benchmarks, such as HotpotQA\n(Yang et al., 2018), MultiRC (Khashabi et al.,\n2018), and 2WikiMultiHopQA (Ho et al., 2020),\naim to achieve QA from multiple sources of\ndocuments. This task is similar to our multi-hop\nquery RAG task, as both involve reasoning from\nmultiple sources of information. However, these\ndatasets primarily focus on assessing a model\u2019s\nreasoning skills, and they do not emphasize the\nretrieval of evidence from a knowledge base.\nAdditionally, their primary data sources Wikipedia,\nsignificantly overlap with the training data of\nmost existing LLMs. If we use these sources for\nbenchmarking RAG systems, there is a potential\nconcern that LLM responses might rely on training\nknowledge rather than reasoning from the retrieved\nknowledge base.\n6 Conclusion\nIn this work, we introduce MultiHop-RAG, a novel\nand unique dataset designed for queries that re-\nquire retrieval and reasoning from multiple pieces\nof supporting evidence. These types of multi-hop\nqueries represent user queries commonly encoun-\ntered in real-world scenarios. MultiHop-RAG con-\nsists of a knowledge base, a large collection of\nmulti-hop queries, their ground-truth answers, and\nthe associated supporting evidence. This paper\ndetails the creation process of MultiHop-RAG, em-\nploying a hybrid approach that integrates human\neffort with GPT-4. Additionally, we explore two\nuse cases of MultiHop-RAG in the benchmarking\nof RAG systems, thereby highlighting the potential\napplications of this dataset. By publicly releas-\ning MultiHop-RAG, we aim to provide a valuable\nresource to the community, contributing to the ad-\nvancement and benchmarking of RAG systems.\nLimitations\nThis work has several limitations that can be im-\nproved in future research. First, our ground truth\nanswers are restricted to simple responses such as\n\u201cyes\", \u201cno\", entity names, or temporal indicators\nlike \u201cbefore\" or \u201cafter\" to facilitate the use of a\n\nstraightforward accuracy metric for evaluating gen-\neration performance. Future work could consider\nallowing free text as answers and employing more\nsophisticated metrics to assess generation quality.\nSecond, the current dataset limits supporting ev-\nidence for a query to a maximum of four pieces.\nFuture work can extend the dataset by including\nqueries that require retrieving and reasoning from\neven more evidence. Lastly, while our experiments\nutilize a basic RAG framework using LlamaIndex,\nfuture work could involve evaluating the answering\nof multi-hop queries using more advanced RAG\nframeworks or LLM-agent frameworks.\nReferences\nAnthropic. 2023. Claude 2.1 (May version). https:\n//api.anthropic.com/v1/messages. Claude 2.1.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\nChen. 2023. Retrieval-based language models and\napplications. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 6: Tutorial Abstracts), pages 41\u201346.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nProceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research , pages 2206\u20132240.\nPMLR.\nHarrison Chase. 2022. LangChain.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation.\nShahul Es, Jithin James, Luis Espinosa-Anke, and\nSteven Schockaert. 2023. Ragas: Automated evalua-\ntion of retrieval augmented generation.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nGoogle. 2023. PaLM 2 (May version).\nhttps://generativelanguage.googleapis.\ncom/v1beta2/models/. Chat-bison-002.\nSignificant Gravitas. 2023. Autogpt. https://github.\ncom/Significant-Gravitas/AutoGPT.\nMichael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaed-\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\nAkram, Susana Guzman, Georgios Mastrapas, Saba\nSturua, Bo Wang, Maximilian Werk, Nan Wang,\nand Han Xiao. 2023. Jina embeddings 2: 8192-\ntoken general-purpose text embeddings for long doc-\numents.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609\u20136625, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand, Gi-\nanna Lengyel, Guillaume Bour, Guillaume Lam-\nple, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao,\nTh\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth\u00e9e Lacroix, and William El Sayed. 2024. Mix-\ntral of experts.\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHoVer: A dataset for many-hop fact extraction and\nclaim verification. In Findings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nEhsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo,\nNandan Thakur, David Alfonso-Hermelo, Mehdi\nRezagholizadeh, and Jimmy Lin. 2023. Evaluat-\ning embedding apis for information retrieval. arXiv\npreprint arXiv:2305.06300.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nBeyond the Surface: A Challenge Set for Reading\nComprehension over Multiple Sentences. In Proc. of\nthe Annual Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nJerry Liu. 2022. LlamaIndex.\nYi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao\nZhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023.\nRecall: A benchmark for llms robustness against\nexternal counterfactual knowledge.\nOpenAI. 2023. GPT4 (Nov 7 version). https://chat.\nopenai.com/chat. gpt-4-1106-preview.\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and\nMatei Zaharia. 2023. Ares: An automated evalua-\ntion framework for retrieval-augmented generation\nsystems.\n\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\nYushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2023. One\nembedder, any task: Instruction-finetuned text em-\nbeddings.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534\u20137550, Online. As-\nsociation for Computational Linguistics.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,\nand Jian-Yun Nie. 2023. Retrieve anything to aug-\nment large language models.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022. Towards a unified multi-\ndimensional evaluator for text generation.\nA Appendix A: GPT-4 Prompts Used for\nData Generation\nWe present the prompts used for guiding GPT-4 for\ndata generation. Table 7 shows the prompt used for\nclaim generation, along with the corresponding top-\nics and entities within these claims. Table 8, Table\n9, and Table 10 respectively show the prompts used\nfor generating multi-hop queries of the inference,\ncomparison, and temporal types.\nB Appendix B: Dataset Examples\nIn this appendix, we present an example of each\ntype of multi-hop query included in the MultiHop-\nRAG dataset. These examples are illustrated in the\nrespective tables: Table 12 for Inference Queries,\nTable 13 for Comparison Queries, Table 14 for\nTemporal Queries, and Table 15 for Null Queries.\nEach query is paired with a ground-truth answer\nfor the evaluation of generation accuracy, while\nmultiple pieces of supporting evidence are included\nfor assessing retrieval performance. Additionally,\nmetadata such as the title, source, and publication\ntime of the news articles are provided as references.\n\nA \"claim\" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given\nevidence from the original context, please extract one claim and its associated topics.\nNote: The claim should not contain ambiguous references, such as \u2019he\u2019,\u2019 she,\u2019 and\u2019 it\u2019, and should use\ncomplete names. If there are multiple topics, give the most dominant one. The target of the claim (one\nentity)is the specific individual, group, or organization that the statement or assertion within a text is\ndirected towards or about which it is making a case. The topic of the claim should be a simple phrase\nrepresenting the claim\u2019s central argument concept. If there is no claim, please leave it blank. Please\ngenerate a claim based on the given evidence. Don\u2019t generate the evidence yourself.\nPlease give the response following this format:\nEvidence: [original context]\nClaims: [extract claim]\nClaim Target: [target]\nClaim Topic: [topic]\nHere are examples:\n<examples>\nNow, it\u2019s your turn.\n<News>\n<evidence>\nTable 7: Claim Generation Prompting\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of\ninformation from different locations or sources to arrive at an answer. The following are news articles\u2019\nmetadata and claims come from the articles. All the claims from the article are related to a similar\ntarget. Your task is to generate one multi-hop inference question based on the claims. Here are some\ninstructions:\n1. Find the Connection: The connection between claims is <target>, which is how these key pieces of\ninformation are related or how they can be combined to form a more complex idea.\n2. Formulate the Question: Create a question that cannot be answered by relying on just one of the\nsentences but instead requires understanding and linking the information from all of the sources. The\nanswer is <target>.\n3. Ensure Coherence: Make sure the question flows logically from the combined information and is\nclear and unambiguous.\n4. Use the keywords: <key set>\n<examples>\nContext:\n<Context>\nTable 8: Inference Query Generation Prompting\n\n<Context>\nThe above are news articles\u2019 metadata and claims come from the articles. All the claims from the\narticles are related to a similar target. Your task is to generate one comparison question based on all the\nclaims from different sources. This question needs to compare some factual elements of the claims that\nare explicitly stated to find where they agree or differ. The correct answer to this question is expressed\nas a comparative adjective, a statement of alignment, a simple yes or no. To generate a comparative\nquestion from claims, you need to use the following keywords: <key set>\nThe Good Comparison Questions:\n<examples>\nYour Comparison Question:\nTable 9: Comparison Query Generation Prompting\n<Context>\nPlease create a time-sensitive comparison question using metadata and excerpts from multiple news\narticles. That is to compare the consistency or sequence of reports on similar topics at multiple different\ntime points. If it is to compare the consistency, please clearly mention the news source and time in the\nquestion using <time frame>. If it is to compare sequences of reports, just clearly mention the news\nsource and do not mention the timeline. Utilize the following keywords provided in the <key set> to\nconstruct the question. The correct answer should based on the factual excerpts and is only one word.\n<examples>\nYour time-sensitive comparison question:\nTable 10: Temporal Query Generation Prompting\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of\ninformation from different locations or sources to arrive at an answer. Considering you have read\nat least two news articles on <entity>, construct a multi-hop question that incorporates all the news\nsources. The source of the news should be stated in the question. Also, ensure that the answer to the\nquestion is a single word/entity. Do not answer this question directly. Just give me the question:\nTable 11: Null Query Generation Prompting\n\nQuery: Which platform is at the center of discussions in articles from Music Business Worldwide,\nPolygon, and FOX News - Health, concerning the policing of AI-driven voice replication, the debate\nover \"reaction\" content, and being the most used app overnight by young people?\nAnswer: YouTube\nEvidence List:\nTitle: Sony Music\u2019s artists aren\u2019t involved in YouTube\u2019s new voice-cloning AI experiment.\nSource: Music Business Worldwide\nPublished Time: 2023-11-23T18:48:48+00:00\nFact: During this period of discussion, YouTube has made a number of positive announcements\nregarding the biggest issue for any rightsholder regarding AI-driven voice replication of artists: their\nability to police it.\nTitle: YouTube demonetizes popular content creator SSSniperwolf after doxxing accusations\nSource: Polygon\nPublished Time: 2023-10-25T18:18:06+00:00\nFact: The debate over \"reaction\" content on YouTube has been brewing for years, but a recent incident\nbetween two creators has refueled the urgency of the conversation.\nTitle: Cell phone shocker as 97% of kids use their device during school hours and beyond, says study\nSource: FOX News - Health\nPublished Time: 2023-10-01T09:05:26+00:00\nFact: Overnight phone use was primarily spent engaging with the same media, although YouTube\nappeared to be the longest-running app because videos were often left playing during the night.\nTable 12: The example of inference questions\nQuery: Did the Cnbc | World Business News Leader report on Nike\u2019s net income and the article from\nThe Age on the 10-year Treasury yield both report a decrease in their respective financial metrics?\nAnswer: Yes\nEvidence List:\nTitle: Nike misses revenue expectations for the first time in two years, beats on earnings and gross\nmargin\nSource: Cnbc | World Business News Leader\nPublished Time: 2023-09-28T20:31:00+00:00\nFact: The company\u2019s reported net income for the three-month period that ended August 31 was $1.45\nbillion, or 94 cents per share, compared with $1.47 billion, or 93 cents per share, a year earlier.\nTitle: ASX set to open higher as Wall Street rebounds; $A rises\nSource: The Age\nPublished Time: 2023-10-04T21:01:01+00:00\nFact: The yield on the 10-year Treasury, which is the centrepiece of the bond market, pulled back from\nits highest level since 2007, down to 4.73 per cent from 4.80 per cent late on Tuesday.\nTable 13: The example of comparison questions\n\nQuery: Was the performance of the Chicago Bears\u2019 defense reported as improved by Yardbarker after\nSporting News highlighted a sack by the Bears\u2019 defense on Joshua Dobbs during the NFL \u2019Monday\nNight Football\u2019 game?\nAnswer: Yes\nEvidence List:\nTitle: Bears vs. Vikings live score, updates, highlights from NFL \u2019Monday Night Football\u2019 game\nSource: Sporting News\nPublished Time: 2023-11-27T23:32:04+00:00\nFact: The Bears answer right back and sack Dobbs, with Sweat and Brisker in there to take him down.\nTitle: Hottest seat on each NFC team: Buns burning for these four head coaches\nSource: Yardbarker\nPublished Time: 2023-11-30T22:29:33+00:00\nFact: In his second season as HC, the defense has improved, but positive results are hard to come by\nbehind a lackluster offense ranked 19th in yards (323.2) and 21st in points per game (20.2).\nTable 14: The example of time-sensitive questions\nQuery: What is the first letter of the CEO\u2019s last name in the news article from Bloomberg on TomTom,\nand what is the first letter of the city where the company\u2019s headquarters is located in the news article\nfrom Reuters?\nAnswer: Insufficient information.\nTable 15: The example of negative rejection questions",
  "authors": [
    "Yixuan Tang",
    "Yi Yang"
  ],
  "summary": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.",
  "pdf_url": "https://arxiv.org/pdf/2401.15391v1",
  "entry_id": "http://arxiv.org/abs/2401.15391v1",
  "published": "2024-01-27",
  "updated": "2024-01-27",
  "comment": "Link: https://github.com/yixuantt/MultiHop-RAG/",
  "journal_ref": null,
  "doi": null,
  "primary_category": "cs.CL",
  "categories": [
    "cs.CL"
  ],
  "links": [
    {
      "href": "https://arxiv.org/abs/2401.15391v1",
      "rel": "alternate",
      "title": null
    },
    {
      "href": "https://arxiv.org/pdf/2401.15391v1",
      "rel": "related",
      "title": "pdf"
    }
  ],
  "full_text": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for\nMulti-Hop Queries\nYixuan Tangand Yi Yang\nHong Kong University of Science and Technology\n{yixuantang,imyiyang}@ust.hk\nAbstract\nRetrieval-augmented generation (RAG) aug-\nments large language models (LLM) by re-\ntrieving relevant knowledge, showing promis-\ning potential in mitigating LLM hallucinations\nand enhancing response quality, thereby facil-\nitating the great adoption of LLMs in prac-\ntice. However, we find that existing RAG sys-\ntems are inadequate in answering multi-hop\nqueries, which require retrieving and reasoning\nover multiple pieces of supporting evidence.\nFurthermore, to our knowledge, no existing\nRAG benchmarking dataset focuses on multi-\nhop queries. In this paper, we develop a novel\ndataset, MultiHop-RAG, which consists of a\nknowledge base, a large collection of multi-\nhop queries, their ground-truth answers, and\nthe associated supporting evidence. We detail\nthe procedure of building the dataset, utiliz-\ning an English news article dataset as the un-\nderlying RAG knowledge base. We demon-\nstrate the benchmarking utility of MultiHop-\nRAG in two experiments. The first experiment\ncompares different embedding models for re-\ntrieving evidence for multi-hop queries. In the\nsecond experiment, we examine the capabili-\nties of various state-of-the-art LLMs, includ-\ning GPT-4, PaLM, and Llama2-70B, in rea-\nsoning and answering multi-hop queries given\nthe evidence. Both experiments reveal that ex-\nisting RAG methods perform unsatisfactorily\nin retrieving and answering multi-hop queries.\nWe hope MultiHop-RAG will be a valuable re-\nsource for the community in developing effec-\ntive RAG systems, thereby facilitating greater\nadoption of LLMs in practice. The MultiHop-\nRAG and implemented RAG system is publicly\navailable athttps://github.com/yixuantt/\nMultiHop-RAG/.\n1 Introduction\nThe emergence of large language models (LLMs),\nsuch as ChatGPT, has fostered a wide range of inno-\nvations, powering intelligent chatbots and other nat-\nural language processing (NLP) applications (Ope-\nFigure 1: RAG with multi-hop query.\nnAI, 2023). One promising use case is Retrieval-\nAugmented Generation (RAG) (Asai et al., 2023),\nwhich optimizes the output of a large language\nmodel by referencing an external knowledge base\noutside of the LLM training data sources before\ngenerating a response. RAG improves LLM\u2019s re-\nsponse (Borgeaud et al., 2022) and also mitigates\nthe occurrence of hallucinations, thereby enhancing\nthe models\u2019 credibility (Gao et al., 2023). LLM-\nbased frameworks, such as LlamaIndex (Liu, 2022)\nand LangChain (Chase, 2022), specialize in sup-\nporting RAG pipelines.\nIn real-world Retrieval-Augmented Generation\n(RAG) applications, a user\u2019s query often necessi-\ntates retrieving and reasoning over evidence from\nmultiple documents, a process known asmulti-hop\nquery. For instance, consider financial analysis us-\ning a database of financial reports. A financial ana-\nlyst might query, Which company among Google,\nApple, and Nvidia reported the largest profit mar-\ngins in their third-quarter reports for 2023? or\ninquire about a specific company\u2019s performance\nover time, such as How does Apple\u2019s sales trend\nlook over the past three years? These queries re-\nquire evidence from multiple documents to formu-\nlate an answer. Due to the multifaceted nature of\nsuch queries, involving information from various\nsources, traditional similarity matching methods\nlike cosine similarity between query and financial\narXiv:2401.15391v1  [cs.CL]  27 Jan 2024\n\nNews source Fortune Magazine The Sydney Morning Herald\nEvidence Back then, just like today, home prices had boomed\nfor years before Fed officials were ultimately forced\nto hike interest rates aggressively in an attempt to\nfight inflation.\nPostponements of such reports could complicate\nthings for the Fed, which has insisted it will make\nupcoming decisions on interest rates based on what\nincoming data say about the economy.\nClaim Federal Reserve officials were forced to aggressively\nhike interest rates to combat inflation after years of\nbooming home prices.\nThe Federal Reserve has insisted that it will base its\nupcoming decisions on interest rates on the incoming\neconomic data.\nBridge-Topic Interest rate hikes to combat inflation Interest rate decisions based on economic data\nBridge-Entity Federal Reserve Federal Reserve\nQuery Does the article from Fortune suggest that the Federal Reserve\u2019s interest rate hikes are a response to past\nconditions, such as booming home prices, while The Sydney Morning Herald article indicates that the\nFederal Reserve\u2019s future interest rate decisions will be based on incoming economic data?\nAnswer Yes\nTable 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased\nclaim, the bridge-topic and bridge-entity, and the corresponding answer.\nreport chunk embeddings might not yield optimal\nresults. We demonstrate this multi-hop retrieval\nprocess in Figure 1.\nHowever, existing RAG benchmarks, such as\nRGB (Chen et al., 2023) and RECALL (Liu et al.,\n2023), mainly evaluate a simple case where the an-\nswer of a query can be retrieved and solved using\none single piece of evidence. None of these bench-\nmarks assess the retrieval and reasoning capability\nof LLMs for complex multi-hop queries. To ad-\ndress this gap and make RAG benchmarking more\nclosely resemble real-world scenarios, in this paper,\nwe introduce MultiHop-RAG. To our knowledge,\nMultiHop-RAG is one of the first RAG datasets\nfocusing specifically on multi-hop queries.\nBased on the RAG queries commonly encoun-\ntered in real-world scenarios, we first categorize\nmulti-hop queries into four types: Inference query,\nComparison query , Temporal query , and Null\nquery. The first three types \u2014 Inference, Com-\nparison, and Temporal \u2014 require the retrieval and\nanalysis of evidence from multiple sources, encom-\npassing tasks like inferring relationships, compar-\ning data points, and sequencing events over time.\nThe Null query represents a scenario where the\nquery cannot be derived from the knowledge base.\nThis category is crucial for assessing whether an\nLLM might hallucinate an answer to a multi-hop\nquery when the retrieved text lacks relevance.\nWe construct our RAG knowledge base using a\ncollection of news articles. Using GPT-4 as a data\ngenerator, we then take an extensive procedure to\nconstruct a diverse set of multi-hop queries, each\nrequiring the retrieval and reasoning over multiple\ndocuments. An example of query construction is\nshown in Table 1. First, we begin by extracting\nfactual sentences from each news article as evi-\ndence. For example, an extracted piece of evidence\nfrom an article may state: \u201cBack then, just like\ntoday, home prices had boomed for years before\nFed officials were ultimately forced to hike interest\nrates aggressively in an attempt to fight inflation.\u201d\nSecond, we input each evidence piece into GPT-4,\nprompting it to rephrase the evidence into a claim.\nThis claim is clarified with a disambiguated topic\nand entity. For instance, GPT-4 might rephrase the\naforementioned evidence into: \u201cFederal Reserve\nofficials were forced to aggressively hike interest\nrates to combat inflation after years of booming\nhome prices\u201d, identifying \u201cInterest rate hikes to\ncombat inflation\u201d as the topic and \u201cFederal Re-\nserve\u201d as the entity. These topics and entities act as\nbridges for constructing multi-hop queries, known\nas bridge-topic or bridge-entity. Next, we use GPT-\n4 to generate specific multi-hop queries related to\nthe same bridge-topic or bridge-entity, accompa-\nnied by the correct answers. Lastly, we undertake\na validation step to ensure the data quality.\nWe demonstrate the benchmarking capabilities\nof MultiHop-RAG using two experiments, utilizing\na RAG system implemented with LlamaIndex (Liu,\n2022). The first experiment involves a comparison\nof different embedding models for retrieving rele-\nvant evidence for multi-hop queries. In the second\nexperiment, we assess the reasoning and answering\nabilities of various state-of-the-art LLMs, including\nGPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B,\nand Mixtral-8x7B, for multi-hop queries when re-\ntrieved text is provided. The results from both ex-\nperiments indicate that the current RAG implemen-\ntations are inadequate for effectively retrieving and\nanswering multi-hop queries. We publicly release\n\nthis challenging MultiHop-RAG dataset and hope it\nwill be a valuable resource for the community in de-\nveloping and benchmarking RAG systems, thereby\nunleashing the great potential of generative AI in\npractice.\n2 RAG with multi-Hop queries\n2.1 Retrieval-augmented Generation (RAG)\nIn an RAG application, we utilize an external cor-\npus, denoted as D, which comprises multiple docu-\nments and serves as the knowledge base. Each doc-\nument within this corpus, represented as di \u2208 D, is\nsegmented into a set of chunks.These chunks are\nthen transformed into vector representations using\nan embedding model and stored in an embedding\ndatabase. Given a user query q, the system typi-\ncally retrieves the top-K chunks that best match the\nquery. These chunks constitute the retrieval set\nfor query q, represented as Rq = {r1, r2, ..., rK}.\nThe retrieved chunks, combined with the query\nand an optional prompt, are then fed into an LLM\nto generate a final answer, following the format:\nLLM(q, Rq, prompt) \u2192 answer.\n2.2 Multi-Hop Query\nWe define a multi-hop query as one that requires\nretrieving and reasoning over multiple pieces of\nsupporting evidence to provide an answer. In other\nwords, for a multi-hop query q, the chunks in the\nretrieval set Rq collectively provide an answer\nto q. For example, the query \"Which company\namong Google, Apple, and Nvidia reported the\nlargest profit margins in their third-quarter reports\nfor 2023?\" requires 1) retrieving relevant pieces of\nevidence related to profit margins from the reports\nof the three companies; 2) generating an answer by\ncomparing and reasoning from the multiple pieces\nof retrieved evidence. This differs from a single-\nhop query such as \"What is Google\u2019s profit margin\nin the third-quarter reports for 2023,\" where the\nanswer can be directly derived from a single piece\nof evidence.\nBased on the queries commonly used in real-\nworld RAG systems, we identify four types of\nmulti-hop queries. For each type, we present a\nhypothetical query within the context of a financial\nRAG system, where the knowledge base consists\nof a collection of annual reports.\nInference query:For such a query q, the answer\nis deduced through reasoning from the retrieval\nset Rq. An example of an inference query might\nbe: Which report discusses the supply chain risk of\nApple, the 2019 annual report or the 2020 annual\nreport?\nComparison query: For such a query q, the an-\nswer requires a comparison of evidence within the\nretrieval set Rq. For instance, a comparison query\nmight ask: Did Netflix or Google report higher\nrevenue for the year 2023?\"\nTemporal query:For such a query q, the answer\nrequires an analysis of the temporal information\nof the retrieved chunks. For example, a temporal\nquery may ask: Did Apple introduce the AirTag\ntracking device before or after the launch of the 5th\ngeneration iPad Pro?\nNull query:For such as queryq, the answer cannot\nbe derived from the retrieved set Rq. We include\nthe null query to assess the generation quality, es-\npecially regarding the issue of hallucination. For a\nnull query, even though a retrieved set is provided,\nan LLM should produce a null response instead\nof hallucinating an answer. For example, assum-\ning ABCD is a non-existent company, a null query\nmight ask: What are the sales of company ABCD\nas reported in its 2022 and 2023 annual reports?\n2.3 Evaluation Metrics\nAn RAG system handling multi-hop queries can be\nassessed from two key aspects: retrieval evaluation\nand generation evaluation.\nRetrieval Evaluation:Evidently, the quality of\nthe retrieval set Rq determines the final genera-\ntion quality. We compare the retrieved set with\nthe ground truth evidence associated with each\nquery, except for the null queries, as they have\nno evidence to derive from. Assuming the top-\nK chunks are retrieved, i.e., |Rq| = K, we use\nretrieval evaluation metrics including Mean Aver-\nage Precision at K (MAP@K), Mean Reciprocal\nRank at K (MRR@K), and Hit Rate at K (Hit@K).\nMAP@K measures the average top-K retrieval pre-\ncision across all queries. MRR@K calculates the\naverage of the reciprocal ranks of the first relevant\nchunk for each query, considering the top-K re-\ntrieved set. Hit@K metric measures the fraction of\nevidence that appears in the top-K retrieved set.\nResponse Evaluation: Since the multi-hop\nquery requires reasoning over multiple pieces of\nretrieved chunks, we can also evaluate the reason-\ning capability of the LLM by comparing the LLM\nresponse with the ground truth answer of the query.\n\nFigure 2: MultiHop-RAG Construction Pipeline.\n3 A Benchmarking Dataset:\nMultiHop-RAG\nIn this section, we provide detailed information\non the construction of the MultiHop-RAG dataset.\nSpecifically, we describe the process of creating a\nset of multi-hop queries, along with the correspond-\ning ground truth evidence sets and answers derived\nfrom a collection of news articles.\n3.1 MultiHop-RAG Construction\nStep 1: Dataset Collection.We download a news\ndataset using the mediastack API 1, a REST API in-\nterface delivering worldwide news data. The news\ndata source comprises various English-language\nwebsites covering a range of news categories: en-\ntertainment, business, sports, technology, health,\nand science. To mimic real-world RAG scenarios,\nwhere the knowledge base data, such as an enter-\nprise\u2019s internal data, may differ from the LLMs\u2019\ntraining data, we select news articles published\nfrom September 26, 2023, to December 26, 2023.\nThis timeframe extends beyond the knowledge cut-\noff of some widely-used LLMs, including Chat-\nGPT and LLaMA, as of the time of writing. This\nselection also helps in teasing out the possibility\nof the underlying LLM having been exposed to\nthese news articles. We only keep articles with a\ntoken length greater than or equal to 1,024. Every\n1https://mediastack.com/\nnews article is paired with metadata, including the\ntitle, publish date, author, category, URL, and news\nsource.\nStep 2: Evidence Extraction.For each article, we\nextract factual or opinion sentences using a trained\nlanguage model 2. These factual sentences are later\nused as evidence for answering multi-hop queries.\nWe retain only those news articles containing ev-\nidence that may have overlapping keywords with\nother news articles. This allows us to later create\nmulti-hop queries where the answer\u2019s evidences\nare drawn from multiple sources.\nStep 3: Claim, Bridge-Entity, Bridge-Topic Gen-\neration. Our goal is to use GPT-4 to automatically\ngenerate high-quality multi-hop queries using the\nevidence set. However, the raw evidence obtained\nfrom Step 2 is not ideal for query generation due\nto inconsistency in linguistic structure. For exam-\nple, some pieces of evidence use pronouns to refer\nto subjects and lack the actual entity in the text.\nTo address this, we employ GPT-4 to paraphrase\nthe evidence, which we refer to as claims, given\nthe original evidence and its context. To ensure\nconsistency between the generated claim and the\nevidence, we further perform fact-checking using\nthe UniEval (Zhong et al., 2022) framework to ver-\nify the alignment between the evidence and claim.\nAppendix A presents the prompt used for GPT-4\nfor claim generation.\nBridge-Entity and Bridge-Topic:The shared en-\ntity or topic across pieces of evidence is referred to\nas the bridge-entity or bridge-topic. These bridge-\nentities or bridge-topics can be used to link dif-\nferent pieces of evidence from which a multi-hop\nquery\u2019s answer is derived. For example, in a claim\nsuch as \u201cGoogle reports its third-quarter results for\n2023, showcasing a detailed overview of its finan-\ncial performance, including revenue growth, profit\nmargins\u201d, the term profit margin can be viewed as\na bridge-topic and the term Google can be viewed\nas a bridge-entity that links the different pieces of\nevidence. We prompt GPT-4 to identify the bridge-\nentity and bridge-topic for each claim. Appendix A\nalso presents the prompt used for GPT-4 for bridge\ngeneration.\nStep 4: Query and Answer Generation.In this\nstep, we leverage the bridge-entity or bridge-topic\nto generate multi-hop queries. Specifically, we first\ngroup the claims having the same bridge-entity or\n2https://huggingface.co/lighteternal/fact-or-opinion-xlmr-\nel\n\nbridge-topic into a claim set. We restrict the claim\nset to have at least two claims but no more than four\nclaims. For each type of query, we feed the claim\nset to GPT-4 and prompt it with an instruction to\ngenerate a query with information from each claim.\nBelow, we explain the specifications for different\nmulti-hop query types. In the construction of each\nquery, we also include the source of the news article\nwhere the supporting evidence is associated with\nto mimic real-world RAG scenarios. Appendix\nA presents the prompts used for GPT-4 for query\ngeneration.\nInference Query:These queries are formulated\nby synthesizing the various characterizations of the\nbridge-entity across multiple claims, with the final\nanswer being the identification of the entity itself.\nComparison Query: These queries are struc-\ntured to compare the similarities and differences\nrelated to the bridge entity or topic. The resultant\nanswer to such queries is typically a definitive \u201cyes\u201d\nor \u201cno\u201d, based on the comparison.\nTemporal Query: These queries explore the\ntemporal ordering of events across different points\nin time. The answer to such queries is typically a\n\u201cyes\u201d or \u201cno\u201d or a single temporal indicator word\nlike \u201cbefore\u201d or \u201cafter\u201d.\nNull Query: Null query is a query whose an-\nswer cannot be derived from the retrieved set. To\ncreate null queries, we generate multi-hop queries\nusing entities that do not exist in the existing bridge-\nentities. To add complexity, we also include fic-\ntional news source metadata when formulating\nthese questions, ensuring that the questions do not\nreference any contextually relevant content from\nthe knowledge base. The answer to the null query\nshould be \u201cinsufficient information\u201d or similar.\nStep 5: Quality Assurance.Finally, we use two\napproaches to reassure the dataset quality. First, we\nmanually review a subset sample of the generated\nmulti-hop queries, their corresponding evidence\nsets, and the final answers. The results of the man-\nual examination indicate a high degree of accuracy\nand data quality. Second, we utilize GPT-4 to as-\nsess each example in the dataset against the follow-\ning criteria: 1) The generated query must utilize\nall provided evidence in formulating the response;\n2) The query should be answerable solely based\non the provided evidence; 3) The response to the\ngenerated query should be either a single word or\na specific entity; 4) The query must conform to its\ndesignated query type.\nCategory Avg. Tokens Entry Count\ntechnology 2262.3 172\nentertainment 2084.3 114\nsports 2030.6 211\nscience 1745.5 21\nbusiness 1723.8 81\nhealth 1481.1 10\ntotal 2046.5 609\nTable 2: Descriptive statistics of the news article knowl-\nedge base in MultiHop-RAG.\nQuery Category Entry Count Percentage\nInference Query 816 31.92%\nComparison Query 856 33.49%\nTemporal Query 583 22.81%\nNull Query 301 11.78%\nTotal 2,556 100.00 %\nTable 3: The distribution of query types in MultiHop-\nRAG.\n3.2 Descriptive Statistics\nThe MultiHop-RAG dataset contains six different\ntypes of news articles, covering 609 distinct news,\nwith an average of 2,046 tokens. The distribution of\nthe news categories is shown in Table 2. MultiHop-\nRAG contains four types of multi-hop queries and\nthe distribution of these queries is shown in Table\n3. In total, about 88% of queries in the dataset are\nnon-null queries where answers can be retrieved\nand reasoned from the knowledge base. In addition,\nthe form of queries exhibits considerable diversity.\nApproximately 27% of interrogative queries start\nwith \"does,\" around 15% initiate with \"what,\" a\nsimilar proportion start \"which,\" and 14% begin\nwith \"who,\" with the remainder incorporating a\nsmall percentage of other interrogative words such\nas \"when.\" Moreover, the number of evidence re-\nquired to answer a multi-hop query varies. Table\n4 shows the distribution of evidence numbers for\neach query in the dataset. Around 42% of queries\ncan be answered using two pieces of evidence,\nwhile approximately 30% and 15% of queries can\nbe answered using three or four pieces of evidence,\nrespectively.\n4 Benchmarking RAG system using\nMultiHop-RAG\nMultiHop-RAG can be used as a benchmark for var-\nious RAG-related tasks. Broadly speaking, RAG-\n\nNum. of Evidence Needed Count Percentage\n0 (Null Query) 301 11.78%\n2 1078 42.18%\n3 779 30.48%\n4 398 15.56%\nTotal 2,556 100.00 %\nTable 4: The distribution of the number of evidence\nrequired to answer multi-hop queries in MultiHop-RAG.\nrelated tasks can be categorized as retrieval-related\ntasks and generation-related tasks. A retrieval-\nrelated task focuses on retrieving relevant text from\nthe knowledge base, while a generation-related task\nfocuses on generating high-quality responses given\nthe retrieved text. In this section, we showcase two\nuse cases for each task where MultiHop-RAG can\nbe employed.\n4.1 Retrieval-related Task\nAn important design choice in an RAG system is\nthe selection of the embedding model. An embed-\nding model converts data into numerical vectors\nand subsequently stores these vectors in embedding\ndatabases. In this experiment, we evaluate differ-\nent embedding models by examining their retrieval\nquality.\nExperiment Setup:We implement an RAG sys-\ntem using the LlamaIndex framework (Liu, 2022).\nWe partition the documents in the MultiHop-RAG\nknowledge base into chunks, each consisting of 256\ntokens. We then convert the chunks using an em-\nbedding model and save the embeddings into a vec-\ntor database. Similarly, in the retrieval step, we con-\nvert a query using the same embedding model and\nretrieve the top-K most relevant chunks that have\nthe highest cosine similarity with the query embed-\nding. In this experiment, we test a variety set of em-\nbedding models, including the ada-embeddings by\nOpenAI (text-embedding-ada-002, text-search-ada-\nquery-001), voyage-02 3, llm-embedder (Zhang\net al., 2023), bge-large-en-v1.5 (Xiao et al., 2023),\njina-embeddings-v2-base-en (G\u00fcnther et al., 2023),\ne5-base-v2 (Wang et al., 2022), and instructor-large\n(Su et al., 2023). NULL queries are excluded in\nthis experiment because there is no matching evi-\ndence to the query. Additionally, we also include\na Reranker module to examine the retrieval perfor-\nmance, using bge-reranker-large (Xiao et al., 2023).\nAfter retrieving 20 related chunks using the em-\n3https://www.voyageai.com/\nbedding model, we further select the top-K chunks\nusing the Reranker.\nExperiment Result:Table 5 shows the retrieval\nresult of using different embedding models. It\nshows that there is still a significant gap in retriev-\ning relevant evidence for the multi-hop queries.\nWhile Rerank can effectively improve retrieval rel-\nevance, the highest Hits@10 is only 0.7467 when\nthe Reranker technique is used. Moreover, the drop\nin the highest Hits@4 to 0.6625 is worrisome. In\npractical RAG systems, the underlying LLM of-\nten has a context window limit. As a result, the\nnumber of retrieved chunks is usually restricted to\na small number. The low values of the retrieval\nmetrics highlight the challenges in retrieving rele-\nvant pieces of evidence for multi-hop queries when\nusing direct similarity matching between the multi-\nhop query and text chunks.\n4.2 Generation-related Task\nThe underlying LLMs play a crucial role in gen-\nerating responses in an RAG system. In this ex-\nperiment, we evaluate the quality of generated re-\nsponses under two different settings. In the first\nsetting, we employ the best-performing retrieval\nmodel, namely voyage-02 with bge-reranker-large,\nas indicated in Table 5, to retrieve the top-K texts\nand then feed them into the LLM. In the second\nsetting, we use the ground-truth evidence associ-\nated with each query as the retrieved text for the\nLLM. This setting represents a ceiling performance\nfor testing the LLM\u2019s response capabilities, as it\nutilizes the actual evidences.\nExperiment Setup: In the first experiment, we\nretrieve top-6 chunks so that the total length of the\nretrieved text does not exceed 2,048. All queries\nin MultiHop-RAG are tested in the experiment.\nIn the second experiment, since the null queries\ndo not have associated evidence, we exclude this\ntype of query in the experiment. For the LLMs\nused in the experiment, we consider state-of-the-\nart commercial models, including GPT-4 (OpenAI,\n2023), GPT-3.5, Claude-2 (Anthropic, 2023), and\nGoogle-PaLM (Google, 2023). We obtain answers\nusing the provided API of the respective models.\nWe also assess some open-source models, includ-\ning Mixtral-8x7b-instruct (Jiang et al., 2024) and\nLlama-2-70b-chat-hf (Touvron et al., 2023).\nExperiment Results:Table 6 shows the response\naccuracy of different LLMs. First, we can see\nthat the response accuracy rate using the retrieved\n\nEmbedding Without Reranker With bge-reranker-large\nMRR@10 MAP@10 Hits@10 Hits@4 MRR@10 MAP@10 Hits@10 Hits@4\ntext-embedding-ada-002 0.4203 0.3431 0.6381 0.504 0.5477 0.4625 0.7059 0.6169\ntext-search-ada-query-001 0.4203 0.3431 0.6399 0.5031 0.5483 0.4625 0.7064 0.6174\nllm-embedder 0.2558 0.1725 0.4499 0.3189 0.425 0.3059 0.5478 0.4756\nbge-large-en-v1.5 0.4298 0.3423 0.6718 0.5221 0.563 0.4759 0.7183 0.6364\njina-embeddings-v2-base-en 0.0621 0.031 0.1479 0.0802 0.1412 0.0772 0.1909 0.1639\nintfloat/e5-base-v2 0.1843 0.1161 0.3556 0.2334 0.3237 0.2165 0.4176 0.3716\nvoyage-02 0.3934 0.3143 0.6506 0.4619 0.586 0.4795 0.7467 0.6625\nhkunlp/instructor-large 0.3458 0.265 0.5717 0.4229 0.5115 0.4118 0.659 0.5775\nTable 5: Retrieval performance of different embedding models.\nModels Accuracy\nRetrieved Chunk Ground-truth Chunk\nGPT-4 0.56 0.89\nChatGPT 0.44 0.57\nLlama-2-70b-chat-hf 0.28 0.32\nMixtral-8x7B-Instruct 0.32 0.36\nClaude-2.1 0.52 0.56\nGoogle-PaLM 0.47 0.74\nTable 6: Generation accuracy of LLMs.\nchunks is not satisfactory, with the state-of-the-\nart GPT-4 model achieving only 0.56 accuracy.\nThis is expected, because the retrieval component\nfalls short in retrieving relevant evidences from the\nknowledge base. Second, even when we provide\nthe LLM with the ground-truth evidences, we can\nsee that the response accuracy is far from being per-\nfect. Open source LLM such as Llama02-70B and\nMixtral-8x7B only achieve an accuracy of 0.32 and\n0.36 respectively. GPT-4 achieves strong reason-\ning capability with an accuracy of 0.89, followed\nby the second-based LLM Google-PaLM with an\naccuracy of 0.74.\nFigure 3 shows the detailed results of different\nquery types for GPT-4 and Mixtral-8x7B-instruct.\nBoth models show relatively high robustness on\nnull queries, meaning they are generally good at\ndetermining when a query cannot be answered\nbased on the retrieved text. This is encouraging be-\ncause one benefit of RAG is to mitigating the LLM\nhallucination issue by augmenting LLM with re-\ntrieval knowledge. However, Mixtral-8x7B model\nperforms significantly worse than the GPT-4 in\ncomparison and temporal queries. Upon reviewing\nthe incorrect responses, we find that Mixtral-8x7B\nfails to accurately handle logical negation, leading\nto misinterpretation of statements and thus a low\nperformance in the comparison queries. In addi-\ntion, Mixtral-8x7B often fails to correctly identify\nFigure 3: Generation accuracy for different query types.\nthe chronological order of events, which is crucial\nfor answering temporal queries where timing is a\nkey factor. Taken together, this experiment demon-\nstrates that there is still room for improvement in\nthe reasoning capabilities of LLMs, particularly\nthose that are open-source, for multi-hop queries.\n4.3 Other Use Cases\nBeyond embedding models and LLM generation,\nthere are other areas worth exploring. For exam-\nple, query decomposition is a widely utilized tech-\nnique in RAG frameworks, such as LLamaIndex.\nThis process involves breaking down the query\ninto smaller segments; it targets a single document\nfor retrieval and integrates the information subse-\nquently, thereby potentially enhancing retrieval ac-\ncuracy. Another advanced and promising approach\ninvolves building LLM-based agents that can au-\ntomatically plan and execute multi-hop queries,\nsuch as AutoGPT (Gravitas, 2023). Another area\nof interest is the hybrid retrieval approach, which\ncombines keyword and embedding matching tech-\n\nniques. We believe that there are many potential\nareas for enhancing RAG\u2019s performance on multi-\nhop queries, and the curated dataset MultiHop-\nRAG can be a valuable resource to the community.\n5 Related Work\nRAG Evaluation:As RAG systems gain increas-\ning popularity, a variety of RAG benchmarking\ndatasets and evaluation tools have been developed.\nFor instance, RGB (Chen et al., 2023) and RE-\nCALL (Liu et al., 2023) evaluate the performance\nof LLMs in generating responses for RAG systems\nunder conditions involving noisy, integrative, and\ncounterfactual queries. However, both datasets pri-\nmarily focus on evaluating the generation aspect\nof RAG systems without specifically addressing\ntheir retrieval accuracy. In addition, recent ad-\nvancements have been made in automated RAG\nevaluation tools, such as ARES (Saad-Falcon et al.,\n2023) and RAGAS (Es et al., 2023). These tools\nutilize LLMs to automatically assess the quality of\nRAG generation, yet they do not introduce bench-\nmarking datasets. Our work introduces one of the\nfirst RAG benchmarking datasets, consisting of a\nknowledge base, a large collection of multi-hop\nqueries, their ground-truth answers, and the associ-\nated supporting evidence, thereby complementing\nexisting RAG evaluations.\nRetrieval datasets: Apart from the context of\nRAG, several benchmarking datasets exist for in-\nformation retrieval evaluation. The FEVER (Fact\nExtraction and VERification) dataset, for instance,\ncontains claims classified as Supported, Refuted,\nor NotEnoughInfo by the given Wikipedia article\n(Thorne et al., 2018). Similarly, the SciFact dataset\ncomprises scientific claims paired with evidence-\ncontaining abstracts (Wadden et al., 2020). How-\never, the claims in both datasets are single-hop\nstatements, and the supporting evidence is from one\nsingle article, in contrast to the multi-hop queries\ndiscussed in this paper. Another dataset, HoVer,\ninvolves claims that require extracting and reason-\ning from multiple Wikipedia articles (Jiang et al.,\n2020). However, unlike our dataset, HoVer focuses\nsolely on classifying claims as either supported or\nnot supported by the articles without evaluating\nan LLM generation step. Moreover, in HoVer, the\nWikipedia articles from which evidence is drawn\nare given for claim verification, which is signifi-\ncantly different from our setting, where relevant\npieces of evidence need to be extracted from a\nlarge knowledge base. Separately, (Kamalloo et al.,\n2023) evaluates a range of commercial embedding\nAPIs for information retrieval, but this evaluation\nis not contextualized within the framework of RAG\nsystems either.\nMulti-document QA datasets: Question-\nanswering (QA) is a fundamental task in NLP, and\nseveral popular benchmarks, such as HotpotQA\n(Yang et al., 2018), MultiRC (Khashabi et al.,\n2018), and 2WikiMultiHopQA (Ho et al., 2020),\naim to achieve QA from multiple sources of\ndocuments. This task is similar to our multi-hop\nquery RAG task, as both involve reasoning from\nmultiple sources of information. However, these\ndatasets primarily focus on assessing a model\u2019s\nreasoning skills, and they do not emphasize the\nretrieval of evidence from a knowledge base.\nAdditionally, their primary data sources Wikipedia,\nsignificantly overlap with the training data of\nmost existing LLMs. If we use these sources for\nbenchmarking RAG systems, there is a potential\nconcern that LLM responses might rely on training\nknowledge rather than reasoning from the retrieved\nknowledge base.\n6 Conclusion\nIn this work, we introduce MultiHop-RAG, a novel\nand unique dataset designed for queries that re-\nquire retrieval and reasoning from multiple pieces\nof supporting evidence. These types of multi-hop\nqueries represent user queries commonly encoun-\ntered in real-world scenarios. MultiHop-RAG con-\nsists of a knowledge base, a large collection of\nmulti-hop queries, their ground-truth answers, and\nthe associated supporting evidence. This paper\ndetails the creation process of MultiHop-RAG, em-\nploying a hybrid approach that integrates human\neffort with GPT-4. Additionally, we explore two\nuse cases of MultiHop-RAG in the benchmarking\nof RAG systems, thereby highlighting the potential\napplications of this dataset. By publicly releas-\ning MultiHop-RAG, we aim to provide a valuable\nresource to the community, contributing to the ad-\nvancement and benchmarking of RAG systems.\nLimitations\nThis work has several limitations that can be im-\nproved in future research. First, our ground truth\nanswers are restricted to simple responses such as\n\u201cyes\", \u201cno\", entity names, or temporal indicators\nlike \u201cbefore\" or \u201cafter\" to facilitate the use of a\n\nstraightforward accuracy metric for evaluating gen-\neration performance. Future work could consider\nallowing free text as answers and employing more\nsophisticated metrics to assess generation quality.\nSecond, the current dataset limits supporting ev-\nidence for a query to a maximum of four pieces.\nFuture work can extend the dataset by including\nqueries that require retrieving and reasoning from\neven more evidence. Lastly, while our experiments\nutilize a basic RAG framework using LlamaIndex,\nfuture work could involve evaluating the answering\nof multi-hop queries using more advanced RAG\nframeworks or LLM-agent frameworks.\nReferences\nAnthropic. 2023. Claude 2.1 (May version). https:\n//api.anthropic.com/v1/messages. Claude 2.1.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\nChen. 2023. Retrieval-based language models and\napplications. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 6: Tutorial Abstracts), pages 41\u201346.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nProceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research , pages 2206\u20132240.\nPMLR.\nHarrison Chase. 2022. LangChain.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation.\nShahul Es, Jithin James, Luis Espinosa-Anke, and\nSteven Schockaert. 2023. Ragas: Automated evalua-\ntion of retrieval augmented generation.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nGoogle. 2023. PaLM 2 (May version).\nhttps://generativelanguage.googleapis.\ncom/v1beta2/models/. Chat-bison-002.\nSignificant Gravitas. 2023. Autogpt. https://github.\ncom/Significant-Gravitas/AutoGPT.\nMichael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaed-\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\nAkram, Susana Guzman, Georgios Mastrapas, Saba\nSturua, Bo Wang, Maximilian Werk, Nan Wang,\nand Han Xiao. 2023. Jina embeddings 2: 8192-\ntoken general-purpose text embeddings for long doc-\numents.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609\u20136625, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand, Gi-\nanna Lengyel, Guillaume Bour, Guillaume Lam-\nple, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao,\nTh\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth\u00e9e Lacroix, and William El Sayed. 2024. Mix-\ntral of experts.\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHoVer: A dataset for many-hop fact extraction and\nclaim verification. In Findings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nEhsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo,\nNandan Thakur, David Alfonso-Hermelo, Mehdi\nRezagholizadeh, and Jimmy Lin. 2023. Evaluat-\ning embedding apis for information retrieval. arXiv\npreprint arXiv:2305.06300.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nBeyond the Surface: A Challenge Set for Reading\nComprehension over Multiple Sentences. In Proc. of\nthe Annual Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nJerry Liu. 2022. LlamaIndex.\nYi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao\nZhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023.\nRecall: A benchmark for llms robustness against\nexternal counterfactual knowledge.\nOpenAI. 2023. GPT4 (Nov 7 version). https://chat.\nopenai.com/chat. gpt-4-1106-preview.\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and\nMatei Zaharia. 2023. Ares: An automated evalua-\ntion framework for retrieval-augmented generation\nsystems.\n\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\nYushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2023. One\nembedder, any task: Instruction-finetuned text em-\nbeddings.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534\u20137550, Online. As-\nsociation for Computational Linguistics.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,\nand Jian-Yun Nie. 2023. Retrieve anything to aug-\nment large language models.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022. Towards a unified multi-\ndimensional evaluator for text generation.\nA Appendix A: GPT-4 Prompts Used for\nData Generation\nWe present the prompts used for guiding GPT-4 for\ndata generation. Table 7 shows the prompt used for\nclaim generation, along with the corresponding top-\nics and entities within these claims. Table 8, Table\n9, and Table 10 respectively show the prompts used\nfor generating multi-hop queries of the inference,\ncomparison, and temporal types.\nB Appendix B: Dataset Examples\nIn this appendix, we present an example of each\ntype of multi-hop query included in the MultiHop-\nRAG dataset. These examples are illustrated in the\nrespective tables: Table 12 for Inference Queries,\nTable 13 for Comparison Queries, Table 14 for\nTemporal Queries, and Table 15 for Null Queries.\nEach query is paired with a ground-truth answer\nfor the evaluation of generation accuracy, while\nmultiple pieces of supporting evidence are included\nfor assessing retrieval performance. Additionally,\nmetadata such as the title, source, and publication\ntime of the news articles are provided as references.\n\nA \"claim\" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given\nevidence from the original context, please extract one claim and its associated topics.\nNote: The claim should not contain ambiguous references, such as \u2019he\u2019,\u2019 she,\u2019 and\u2019 it\u2019, and should use\ncomplete names. If there are multiple topics, give the most dominant one. The target of the claim (one\nentity)is the specific individual, group, or organization that the statement or assertion within a text is\ndirected towards or about which it is making a case. The topic of the claim should be a simple phrase\nrepresenting the claim\u2019s central argument concept. If there is no claim, please leave it blank. Please\ngenerate a claim based on the given evidence. Don\u2019t generate the evidence yourself.\nPlease give the response following this format:\nEvidence: [original context]\nClaims: [extract claim]\nClaim Target: [target]\nClaim Topic: [topic]\nHere are examples:\n<examples>\nNow, it\u2019s your turn.\n<News>\n<evidence>\nTable 7: Claim Generation Prompting\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of\ninformation from different locations or sources to arrive at an answer. The following are news articles\u2019\nmetadata and claims come from the articles. All the claims from the article are related to a similar\ntarget. Your task is to generate one multi-hop inference question based on the claims. Here are some\ninstructions:\n1. Find the Connection: The connection between claims is <target>, which is how these key pieces of\ninformation are related or how they can be combined to form a more complex idea.\n2. Formulate the Question: Create a question that cannot be answered by relying on just one of the\nsentences but instead requires understanding and linking the information from all of the sources. The\nanswer is <target>.\n3. Ensure Coherence: Make sure the question flows logically from the combined information and is\nclear and unambiguous.\n4. Use the keywords: <key set>\n<examples>\nContext:\n<Context>\nTable 8: Inference Query Generation Prompting\n\n<Context>\nThe above are news articles\u2019 metadata and claims come from the articles. All the claims from the\narticles are related to a similar target. Your task is to generate one comparison question based on all the\nclaims from different sources. This question needs to compare some factual elements of the claims that\nare explicitly stated to find where they agree or differ. The correct answer to this question is expressed\nas a comparative adjective, a statement of alignment, a simple yes or no. To generate a comparative\nquestion from claims, you need to use the following keywords: <key set>\nThe Good Comparison Questions:\n<examples>\nYour Comparison Question:\nTable 9: Comparison Query Generation Prompting\n<Context>\nPlease create a time-sensitive comparison question using metadata and excerpts from multiple news\narticles. That is to compare the consistency or sequence of reports on similar topics at multiple different\ntime points. If it is to compare the consistency, please clearly mention the news source and time in the\nquestion using <time frame>. If it is to compare sequences of reports, just clearly mention the news\nsource and do not mention the timeline. Utilize the following keywords provided in the <key set> to\nconstruct the question. The correct answer should based on the factual excerpts and is only one word.\n<examples>\nYour time-sensitive comparison question:\nTable 10: Temporal Query Generation Prompting\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of\ninformation from different locations or sources to arrive at an answer. Considering you have read\nat least two news articles on <entity>, construct a multi-hop question that incorporates all the news\nsources. The source of the news should be stated in the question. Also, ensure that the answer to the\nquestion is a single word/entity. Do not answer this question directly. Just give me the question:\nTable 11: Null Query Generation Prompting\n\nQuery: Which platform is at the center of discussions in articles from Music Business Worldwide,\nPolygon, and FOX News - Health, concerning the policing of AI-driven voice replication, the debate\nover \"reaction\" content, and being the most used app overnight by young people?\nAnswer: YouTube\nEvidence List:\nTitle: Sony Music\u2019s artists aren\u2019t involved in YouTube\u2019s new voice-cloning AI experiment.\nSource: Music Business Worldwide\nPublished Time: 2023-11-23T18:48:48+00:00\nFact: During this period of discussion, YouTube has made a number of positive announcements\nregarding the biggest issue for any rightsholder regarding AI-driven voice replication of artists: their\nability to police it.\nTitle: YouTube demonetizes popular content creator SSSniperwolf after doxxing accusations\nSource: Polygon\nPublished Time: 2023-10-25T18:18:06+00:00\nFact: The debate over \"reaction\" content on YouTube has been brewing for years, but a recent incident\nbetween two creators has refueled the urgency of the conversation.\nTitle: Cell phone shocker as 97% of kids use their device during school hours and beyond, says study\nSource: FOX News - Health\nPublished Time: 2023-10-01T09:05:26+00:00\nFact: Overnight phone use was primarily spent engaging with the same media, although YouTube\nappeared to be the longest-running app because videos were often left playing during the night.\nTable 12: The example of inference questions\nQuery: Did the Cnbc | World Business News Leader report on Nike\u2019s net income and the article from\nThe Age on the 10-year Treasury yield both report a decrease in their respective financial metrics?\nAnswer: Yes\nEvidence List:\nTitle: Nike misses revenue expectations for the first time in two years, beats on earnings and gross\nmargin\nSource: Cnbc | World Business News Leader\nPublished Time: 2023-09-28T20:31:00+00:00\nFact: The company\u2019s reported net income for the three-month period that ended August 31 was $1.45\nbillion, or 94 cents per share, compared with $1.47 billion, or 93 cents per share, a year earlier.\nTitle: ASX set to open higher as Wall Street rebounds; $A rises\nSource: The Age\nPublished Time: 2023-10-04T21:01:01+00:00\nFact: The yield on the 10-year Treasury, which is the centrepiece of the bond market, pulled back from\nits highest level since 2007, down to 4.73 per cent from 4.80 per cent late on Tuesday.\nTable 13: The example of comparison questions\n\nQuery: Was the performance of the Chicago Bears\u2019 defense reported as improved by Yardbarker after\nSporting News highlighted a sack by the Bears\u2019 defense on Joshua Dobbs during the NFL \u2019Monday\nNight Football\u2019 game?\nAnswer: Yes\nEvidence List:\nTitle: Bears vs. Vikings live score, updates, highlights from NFL \u2019Monday Night Football\u2019 game\nSource: Sporting News\nPublished Time: 2023-11-27T23:32:04+00:00\nFact: The Bears answer right back and sack Dobbs, with Sweat and Brisker in there to take him down.\nTitle: Hottest seat on each NFC team: Buns burning for these four head coaches\nSource: Yardbarker\nPublished Time: 2023-11-30T22:29:33+00:00\nFact: In his second season as HC, the defense has improved, but positive results are hard to come by\nbehind a lackluster offense ranked 19th in yards (323.2) and 21st in points per game (20.2).\nTable 14: The example of time-sensitive questions\nQuery: What is the first letter of the CEO\u2019s last name in the news article from Bloomberg on TomTom,\nand what is the first letter of the city where the company\u2019s headquarters is located in the news article\nfrom Reuters?\nAnswer: Insufficient information.\nTable 15: The example of negative rejection questions",
  "full_text_length": 49453,
  "link_pdf": "https://arxiv.org/pdf/2401.15391v1",
  "paper_id": "2401.15391v1"
}