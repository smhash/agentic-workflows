{
  "source": "arxiv",
  "query": "Agentic RAG",
  "fetched_at": "2025-11-21T17:17:56.518733",
  "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation",
  "url": "http://arxiv.org/abs/2510.22344v1",
  "content": "FAIR-RAG: Faithful Adaptive Iterative Refinement for\nRetrieval-Augmented Generation\nMohammad Aghajani Asl\nSharif University of Technology\nm.aghajani@physics.sharif.edu\nMajid Asgari-Bidhendi\nIran University of Science and Technology\nNoor Avaran Jelvehaye Maanaei Najm Co., Ltd.\nmajid.asgari@gmail.com\nBehrooz Minaei-Bidgoli\nIran University of Science and Technology\nb_minaei@iust.ac.ir\nAbstract\nWhile Retrieval-Augmented Generation (RAG)\nmitigates hallucination and knowledge stale-\nness in Large Language Models (LLMs), exist-\ning frameworks often falter on complex, multi-\nhop queries that require synthesizing informa-\ntion from disparate sources. Current advanced\nRAG methods, employing iterative or adaptive\nstrategies, still lack a robust mechanism to sys-\ntematically identify and fill evidence gaps, of-\nten propagating noise or failing to gather a com-\nprehensive context. In this paper, we introduce\nFAIR-RAG, a novel agentic framework that\ntransforms the standard RAG pipeline into a\ndynamic, evidence-driven reasoning process.\nAt the core of FAIR-RAG is an Iterative Refine-\nment Cycle governed by a novel module we\nterm Structured Evidence Assessment (SEA).\nThe SEA acts as an analytical gating mecha-\nnism: it deconstructs the initial query into a\nchecklist of required findings and systemati-\ncally audits the aggregated evidence to iden-\ntify confirmed facts and, critically, explicit in-\nformational gaps. These identified gaps pro-\nvide a precise, actionable signal to an Adap-\ntive Query Refinement agent, which then gen-\nerates new, targeted sub-queries to retrieve the\nmissing information. This cycle repeats until\nthe evidence is verified as sufficient, ensuring\na comprehensive context for a final, strictly\nfaithful generation step. We conduct exten-\nsive experiments on challenging multi-hop QA\nbenchmarks, including HotpotQA, 2WikiMul-\ntiHopQA, and MusiQue.Under a unified and\ncontrolled experimental setup,FAIR-RAG\nsignificantly outperforms strong representative\nbaselines. On HotpotQA, it achieves an F1-\nscore of 0.453\u2014an absolute improvement of\n8.3 points over the strongest iterative baseline\u2014\nestablishing a new state-of-the-art for this\nclass of methods on these benchmarks.Our\nwork demonstrates that a structured, evidence-\ndriven refinement process with explicit gap\nanalysis is crucial for unlocking reliable and\naccurate reasoning in advanced RAG systems\nfor complex, knowledge-intensive tasks.\n1 Introduction\nLarge Language Models (LLMs) have demon-\nstrated remarkable capabilities across a wide range\nof natural language processing tasks, including\nquestion-answering (QA) (Brown et al., 2020;\nChowdhery et al., 2022). However, their knowl-\nedge is inherently static, confined to the data they\nwere trained on, which leads to factual inaccura-\ncies and an inability to reason about events beyond\ntheir training cut-off date. Furthermore, LLMs\nare prone to \u201challucination,\u201d generating plausible\nyet factually incorrect information, which severely\nlimits their reliability in knowledge-intensive ap-\nplications (Ji et al., 2023). To mitigate these is-\nsues, Retrieval-Augmented Generation (RAG) has\nemerged as a prominent paradigm. By ground-\ning the generation process on information retrieved\nfrom external knowledge bases, RAG systems aim\nto produce more accurate, timely, and verifiable\nresponses (Lewis et al., 2020).\nDespite its advantages, the standard \u201cretrieve-\nthen-read\u201d RAG pipeline often falls short when\nfaced with real-world user queries, which are fre-\nquently complex and cannot be answered through\na single-shot retrieval step. For instance, a query\nsuch as\u201cWhich movie, directed by the same per-\nson who directed Inception, won an Oscar for\narXiv:2510.22344v1  [cs.CL]  25 Oct 2025\n\nBest Cinematography?\u201drequires multi-hop rea-\nsoning (Yang et al., 2018): first identifying the\ndirector ofInception(Christopher Nolan) and then\nsearching for his movies that have won the spec-\nified award. Standard RAG frameworks struggle\nwith such multi-hop queries, as well as comparative\nor analytical questions that require synthesizing in-\nformation from multiple sources. Moreover, they\nare not robust to suboptimal user query formula-\ntions and often fail to enforce that the generated\nanswer remains strictly faithful to the retrieved evi-\ndence, thus perpetuating the risk of hallucination.\nTo address these limitations, several advanced\nRAG methodologies have been proposed.Itera-\ntive approaches, such as ITER-RETGEN (Shao\net al., 2023), refine the retrieved information over\nmultiple cycles by using the previously generated\noutput as context for the next retrieval step.Adap-\ntive approaches, like Adaptive-RAG (Jeong et al.,\n2024), aim for efficiency by dynamically selecting\nthe retrieval strategy (e.g., no retrieval, single-step,\nor multi-step) based on an initial assessment of the\nquery\u2019s complexity. Other frameworks like SELF-\nRAG (Asai et al., 2023) introduce self-reflection\nmechanisms, training the LLM to generate special\ntokens that control the retrieval and critique pro-\ncess on the fly. Nonetheless, a gap remains for a\nframework that synergistically combines iterative\nevidence refinement with adaptive query genera-\ntion and an explicit, modular faithfulness check.\nExisting iterative methods can propagate noise by\nusing entire generations as queries, while adaptive\nmethods may not sufficiently refine the evidence\nneeded for complex queries after the initial routing.\nIn this paper, we introduceFAIR-RAG, a\nnovel framework forFaithful,Adaptive,Iterative\nRefinement inRetrieval-AugmentedGeneration.\nFAIR-RAG is designed to robustly handle complex\nqueries by orchestrating a dynamic, multi-stage\nworkflow. The framework begins with anAdaptive\nRoutingmodule that analyzes query complexity to\ndetermine an optimal execution path, either by di-\nrectly answering simple queries or by allocating the\nappropriate computational resources for complex\nones. For non-trivial queries, FAIR-RAG initiates\na cyclical process designed to progressively build\nand validate a context. At the core of this cycle is\nanIterative Refinementloop where LLM agents\nintelligently decompose information needs, retrieve\nevidence, and filter out noise. Crucially, each cy-\ncle culminates in aStructured Evidence Assess-\nment (SEA)module, which acts as an analytical\ngating mechanism. This module emulates a cog-\nnitive workflow by first deconstructing the user\u2019s\nquery into a checklist of required findings. It then\nsystematically synthesizes the retrieved evidence\nagainst this checklist, verifying what is confirmed\nand, most importantly, explicitly identifying any\n\u201cRemaining Gaps.\u201dThese identified gaps provide\na precise, actionable signal for theAdaptive Query\nRefinementmodule, which then generates new, tar-\ngeted queries specifically designed to retrieve the\nmissing information. This evidence-centric loop\ncontinues until sufficiency is achieved, ensuring\nthat the finalFaithful Answer Generationstep\nis strictly grounded in a comprehensive and ver-\nified knowledge context, substantially enhancing\ntrustworthiness and reducing hallucination.\nOur main contributions are as follows:\n\u2022 We introduce a novel agentic RAG architec-\nture centered on anIterative Refinement\nloop. This evidence-centric cycle is governed\nby an analytical gating mechanism we term\nStructured Evidence Assessment (SEA). By\nsystematically deconstructing the query and\nidentifying specific information gaps, the SEA\nmodule intelligently guides subsequent itera-\ntions. This process progressively builds and\nvalidates a comprehensive context, enabling\nthe system to robustly handle complex, multi-\nfaceted, and multi-hop queries where single-\npass retrieval would fail.\n\u2022 We design a sophisticated, two-stage query\nstrategy. It begins withsemantic decomposi-\ntionto ensure all facets of the initial query are\naddressed, and more importantly, incorporates\nanAdaptive Query Refinementmechanism\nthat analyzes evidence gaps to intelligently\ngenerate new queries, effectively reasoning\nabout what information is still missing.\n\u2022 We implement an integrated approach to re-\nsource optimization throughdynamic re-\nsource allocation. Our framework employs\nan initialAdaptive Routingmechanism to\nbypass the RAG pipeline for simple queries\nand dynamically assigns LLMs of varying\nsizes to internal tasks based on their com-\nplexity, achieving a superior balance between\nresponse quality, latency, and computational\ncost.\n\n\u2022 We propose a robust, two-pronged approach\nto guarantee faithfulness. This includes: (1) a\npre-generationStructured Evidence Assess-\nment (SEA), which performs a final analyt-\nical pass to verify that all required findings\nfrom the initial query deconstruction are fully\nsupported by the aggregated evidence, and (2)\naconstrained generation promptthat en-\nforces citation and prevents the model from\nintroducing external knowledge. This combi-\nnation ensures the final answer is both verifi-\nable and trustworthy.\nWe conducted extensive experiments on a suite\nof four challenging open-domain QA bench-\nmarks to evaluate the FAIR-RAG framework, en-\ncompassing complex multi-hop reasoning tasks\n(HotpotQA(Yang et al., 2018),2WikiMulti-\nHopQA,Musique) and a large-scale single-hop\nfactual dataset (TriviaQA). Our best-performing\nconfiguration,FAIR-RAG 3 (Adaptive LLMs),\nwas benchmarked against a wide range of strong\nbaselines, including sequential (Standard RAG),\nconditional (Adaptive-RAG), and other state-of-\nthe-art iterative methods (Iter-Retgen, Self-RAG).\nThe results clearly demonstrate the superiority of\nour approach. On theHotpotQAbenchmark, our\nmodel sets a new state-of-the-art with an F1-score\nof0.453, surpassing the strongest iterative baseline,\nIter-Retgen (0.370), by a significant margin of8.3\npoints. This pattern of superior performance is\nconsistent across other complex benchmarks: on\n2WikiMultiHopQA, FAIR-RAG achieves an F1\nof0.320, outperforming the next best method,Self-\nRAG (0.251), by6.9 points, and onMusique, it\nscores an F1 of0.264, which is7.4 pointshigher\nthanIter-Retgen (0.190).\nNotably, FAIR-RAG\u2019s architecture also excels\non simpler factual queries, achieving a state-of-the-\nart F1 score of0.731onTriviaQA, showcasing\nits versatility. Our analysis further validates FAIR-\nRAG\u2019s core principles: performance consistently\nimproves as the number of refinement iterations\nincreases from one to three (e.g., F1 on HotpotQA\nimproves from 0.398 for FAIR-RAG 1 to 0.447 for\nFAIR-RAG 3), confirming the value of the iterative\nevidence-gathering loop. Furthermore, the adap-\ntive LLM allocation strategy provides an additional\nperformance boost across all metrics. Finally, our\nframework consistently achieves the highest scores\non the semantic metricACC LLM (e.g.,0.847on\nTriviaQA), confirming that its improvements reflect\na deeper contextual understanding, not just lexical\noverlap. This robust performance validates the ef-\nfectiveness of our iterative, evidence-driven frame-\nwork in enhancing both the accuracy and faithful-\nness of LLM-based QA systems.\n2 Related Work\nThe paradigm of Retrieval-Augmented Genera-\ntion (RAG) has rapidly evolved from a straight-\nforward retrieve-then-read pipeline to more sophis-\nticated, dynamic frameworks. Our work, FAIR-\nRAG, builds upon and extends several key research\nthreads in this domain.\n2.1 Standard Retrieval-Augmented\nGeneration\nThe foundational concept of RAG is to enhance\nthe capabilities of LLMs by grounding them in\nexternal, non-parametric knowledge bases (Lewis\net al., 2020). Early and influential RAG models\ntypically employ a two-stage process: a retriever\nfirst fetches a set of relevant documents from a cor-\npus based on the input query, and then a reader\n(or generator), which is often an LLM, synthe-\nsizes the final response conditioned on both the\noriginal query and the retrieved documents. This\napproach has proven effective in mitigating fac-\ntual inconsistencies (hallucinations) and providing\nanswers based on up-to-date information, thereby\novercoming the static knowledge limitations of\nLLMs (Huang et al., 2025; Guu et al., 2020). How-\never, this single-shot retrieval mechanism is primar-\nily designed for single-hop queries where the an-\nswer can be found within a small set of initially re-\ntrieved documents. Consequently, its performance\ndegrades significantly on complex benchmarks like\nHotpotQA (Yang et al., 2018), which require multi-\nstep reasoning or evidence aggregation from dis-\nparate sources.\n2.2 Iterative and Multi-Step RAG\nTo address the shortcomings of standard RAG, a\nsignificant body of research has focused on iterative\nand multi-step approaches. These methods trans-\nform the single-shot process into a dynamic, multi-\nturn interaction. One prominent strategy involves\ndecomposing a complex question into simpler sub-\nqueries. While frameworks likeSelf-Ask(Press\net al., 2023) andSuRe(Kim et al., 2024) pioneer\nthis decompositional approach, their strategies dif-\nfer fundamentally from ours.SuRegenerates a\n\ncomplete reasoning skeletonupfront\u2014a static plan\nthat cannot adapt to the evidence retrieved in inter-\nmediate steps.Self-Askgenerates sub-queries se-\nquentially, but each new query is largely informed\nby theintermediate answerto the previous one.In\ncontrast, FAIR-RAG\u2019s decomposition is neither\nstatic nor solely dependent on prior answers; it\nis a dynamic and context-aware process.After\neach iteration, our framework uses the Structured\nEvidence Assessment (SEA) module to perform\na holistic analysis of theentirety of the retrieved\nevidence corpus. New sub-queries are then gener-\nated specifically to target theexplicitly identified\ninformational gaps. This gap-driven approach al-\nlows FAIR-RAG to adapt its evidence-gathering\nstrategy in response to what has been found (and\nwhat is still missing), leading to a more robust and\nfocused multi-step reasoning process than methods\nreliant on static plans or sequential, answer-driven\nprompting.\nAnother popular approach interleaves reasoning\nsteps with retrieval actions. This paradigm is ex-\nemplified by methods like ReAct (Yao et al., 2022)\nand IRCoT (Trivedi et al., 2023), which guide an\nLLM to generate explicit reasoning traces (e.g.,\nChain-of-Thought (Wei et al., 2022)), where each\nstep can trigger a retrieval action to gather neces-\nsary information. The key distinction lies in the\nscope and triggerfor retrieval. In methods like\nReAct and IRCoT, retrieval is typically alocal,\nstep-wise actiontriggered by the immediate need\nof the next thought in a chain. In contrast, FAIR-\nRAG\u2019s retrieval is driven by aholistic assessment\nof the entire evidence pool against the overarch-\ning query requirements, allowing it to identify and\naddress complex, non-sequential information gaps\nthat step-wise reasoning might overlook.\nMore directly related to our work, frameworks\nlike ITER-RETGEN (Shao et al., 2023) propose a\nsynergistic loop where the entire generated output\nfrom one iteration serves as the context to retrieve\nmore relevant knowledge for the next. While pow-\nerful, using unstructured generation as a query can\nbe suboptimal, as it may contain noise that misdi-\nrects the retriever. FAIR-RAG distinguishes itself\nby employing a more controlled mechanism: in-\nstead of using the entire previous output, it gener-\nates new, targeted sub-queries based on an explicit\nanalysis ofinformational gaps, leading to a more\nfocused and efficient evidence-gathering process.\nOther iterative frameworks have introduced\nforward-looking or corrective mechanisms to\nenhance retrieval precision. For instance,\nFLARE (Jiang et al., 2023) employs a proactive\nstrategy where the LLM anticipates future informa-\ntion needs during generation and triggers retrievals\naccordingly, effectively interleaving prediction and\nlookup steps. Similarly, Corrective RAG (Yan\net al., 2024) incorporates a post-retrieval correc-\ntion phase, using an evaluator to assess and refine\nretrieved documents based on their relevance and\nfactual consistency. While these methods improve\nupon standard iterative loops by adding predictive\nor corrective elements, they often rely on heuristic\ntriggers or post-hoc adjustments, which can still\noverlook systematic evidence gaps in highly com-\nplex queries. In contrast, FAIR-RAG\u2019s Structured\nEvidence Assessment (SEA) module provides a\nmore principled, checklist-driven analysis that ex-\nplicitly identifies and targets informational defi-\nciencies, enabling a targeted and iterative refine-\nment without dependence on generation-time pre-\ndictions.\n2.3 Adaptive and Faithfulness-Aware RAG\nA third stream of research focuses on making RAG\nsystems more adaptive and reliable. Adaptivity is\noften geared towards computational efficiency. For\ninstance,Adaptive-RAG(Jeong et al., 2024) intro-\nduces a classifier to pre-assess query complexity\nand route it to an appropriate strategy: no retrieval\nfor simple questions, single-step retrieval for mod-\nerate ones, or a multi-step approach for complex\nqueries. This routing is performed once at the be-\nginning. In contrast, FAIR-RAG\u2019s adaptivity is\ndynamic and occurswithinthe iterative process, as\nit continually adapts its query generation strategy\nbased on the evolving set of retrieved evidence.\nEnhancing the faithfulness of the generated out-\nput is another critical concern. Standard RAG mod-\nels do not explicitly guarantee that the generator\nwill adhere to the retrieved context. (Es et al., 2025)\nTo address this, SELF-RAG (Asai et al., 2023) fine-\ntunes an LLM to generate special \u201creflection to-\nkens\u201d, enabling it to critique its own output for\nrelevance and factual support in an inline fashion\nduring generation. While effective, this approach\nhas two limitations: its reliance on fine-tuning re-\nstricts applicability to off-the-shelf models, and\nits evaluation is inherentlytactical, assessing evi-\ndence on a step-by-step basis as the answer is being\ncomposed.\n\nFAIR-RAG addresses the faithfulness challenge\ndifferently, through an explicit, modular, and more\nstrategicStructured Evidence Assessment (SEA)\nmodule. Instead of an inline critique, SEA acts\nas a distinct analytical gating mechanismbefore\nfinal answer generation. It first deconstructs the\nuser\u2019s query into a checklist of required findings.\nIt then performs a holistic audit of theentireevi-\ndence corpus against this question-centric check-\nlist to identify confirmed facts and explicit \u201cintel-\nligence gaps.\u201d This distinction is crucial: whereas\nSELF-RAG prompts the model to ask, \u201cIs this next\npiece of evidence useful for my current generation\nstep?\u201d, SEA forces the model to first ask, \u201cIs the\nentiretyof my evidence sufficient to address all\nfacets of the user\u2019s original query?\u201d The identi-\nfied gaps from this strategic assessment provide a\nprecise, actionable signal for the subsequent query\nrefinement step, transforming the check from a pas-\nsive validation into an active steering mechanism.\nCrucially, unlike fine-tuning-based methods, our\nmodular SEA requires no model training, allowing\nfor greater flexibility and easier integration with\nvarious off-the-shelf language models.\nIn summary, while existing works have made\nsubstantial advancements, FAIR-RAG provides a\nnovel contribution by synergistically integrating\nthree core principles into a cohesive framework: (1)\na structured, gap-aware iterative refinement loop,\n(2) context-aware, adaptive sub-query generation,\nand (3) an explicit, modular faithfulness assessment\nthat requires no model fine-tuning.\n3 Methodology\nThe FAIR-RAG framework is designed as a multi-\nstage, iterative process that dynamically adapts its\nstrategy to the complexity of a user\u2019s query. Our\narchitecture transforms the standard, static RAG\npipeline into an intelligent, evidence-driven work-\nflow that progressively builds context to answer\ncomplex questions. The entire process, illustrated\nin Figure 1, can be divided into four main phases:\n(1) Initial Query Analysis and Adaptive Routing,\n(2) The Iterative Retrieval and Refinement Cycle,\n(3) Faithful Answer Generation, and (4) Dynamic\nResource Allocation. The entire inference proce-\ndure is detailed step-by-step in Algorithm 1. The\nprocess is initiated by theAdaptive Routingagent\n(Arouter), which classifies the query and selects the\nmost appropriate generator LLM ( Gselected) from\nthe predefined set of available models (G).\n3.1 Overall Architecture\nThe overall architecture of FAIR-RAG, illustrated\nin Figure 1, employs a dynamic, multi-step pro-\ncess to handle user queries. An initial routing step\nassesses query complexity. Simple queries are an-\nswered directly, while complex ones trigger an it-\nerative refinement loop. This core loop consists of\nadaptive query generation, hybrid retrieval, filter-\ning, and a Structured Evidence Assessment (SEA).\nThe SEA module determines if the collected evi-\ndence is sufficient. If not, the loop repeats with\nrefined queries targeting information gaps. Once\nsufficiency is met, a final, evidence-grounded an-\nswer is generated. The entire inference procedure\nis detailed step-by-step in Algorithm 1. All LLM\nagents in our pipeline are guided by meticulously\nengineered prompts. The full details and examples\nfor each prompt are provided in Appendix B.\n3.2 Initial Query Analysis and Adaptive\nRouting\nThe first stage of our framework is a lightweight yet\ncrucial analysis of the input query. An LLM agent\nclassifies the query into one of four categories to\ndetermine the subsequent workflow and resource\nallocation:\n\u2022 OBVIOUS:For queries whose answers are\nlikely stable and contained within the LLM\u2019s\nparametric knowledge (e.g., \u201cWhat is the cap-\nital of France?\u201d). These queries are routed\ndirectly to a large LLM for generation, by-\npassing the entire RAG pipeline for maximum\nefficiency.\n\u2022 SMALL:Simple factual queries that require\nretrieval but minimal reasoning. A smaller,\nmore efficient LLM is designated for the final\ngeneration step.\n\u2022 LARGE:Queries that require retrieval\nand synthesis of information from multiple\nsources. A larger, more capable LLM is se-\nlected for generation.\n\u2022 REASONING:Complex queries that demand\nmulti-hop reasoning, comparison, or deep\nanalysis. A state-of-the-art LLM with strong\nreasoning capabilities is allocated for the final\nanswer.\nThis initial routing mechanism serves a dual\npurpose: it acts as an efficiency-enhancing short-\n\nAlgorithm 1FAIR-RAG Inference (Prompts for agents are detailed in Appendix B.)\nRequire: Set of Generator LLMs G={G small, Glarge, Greasoning}, Hybrid Retriever Hretriever, Document Corpus C=\n{d1, . . . , dN}, Set of LLM AgentsA={A router, Adecompose, Afilter, Asuff, Arefine}\n1:Input:User queryx\n2:Output:Final, evidence-grounded answery final\n3: query_type, Gselected \u2190A router.classify(x)\u25b7Route\n4:ifquery_type==OBVIOUSthen\n5:y final \u2190G selected.generate(x)\u25b7Generate directly from parametric knowledge\n6:returny final\n7:end if\n8:E agg \u2190 \u2205 // Aggregated evidence\n9:Q previous \u2190 {x}\n10:fori\u21901to3do // Max 3 iterations\n11:ifi== 1then\n12:Q sub \u2190A decompose.generate(x)\u25b7Decompose initial query\n13:else\n14:Q sub \u2190A refine.generate(x, Qprevious,analysis_summary)\u25b7Refine query to fill gaps\n15:end if\n16:Q previous \u2190Q previous \u222aQ sub\n17:D candidate \u2190H retriever.retrieve_and_rerank(Qsub,C)\u25b7Retrieve\n18:E filtered \u2190A filter.filter(Dcandidate, x)\u25b7Filter irrelevant evidence\n19:E agg \u2190E agg \u222aE filtered\n20: analysis_summary,is_sufficient\u2190A suff.check(Eagg, x)\u25b7Assess sufficiency with SEA\n21:ifis_sufficient==Yesthen\n22:break// Exit loop if evidence is sufficient\n23:end if\n24:end for\n25:y final \u2190G selected.generate(x, Eagg,faith_constraints)\u25b7Generate answer\n26:returny final\ncut for simple queries and implements ourAdap-\ntive Model Selectionstrategy by pre-allocating the\nmost cost-effective generator model for the final\nstep. While this module is a key feature for prac-\ntical deployment, it was systematically controlled\nduring our benchmark evaluations to ensure a fair\ncomparison, as detailed in Section 4.4.\n3.3 The Iterative Retrieval and Refinement\nCycle\nThis cycle is the core of FAIR-RAG\u2019s ability to\nhandle complex information needs. It is designed\nto run for a maximum of three iterations to ensure\na balance between comprehensiveness and latency.\nEach iteration consists of the following steps:\n3.3.1 Adaptive Query Generation\nThe initial user query, if deemed complex, is first\nsubjected to semantic decomposition. An LLM\nagent breaks down the multifaceted query into a\nset of up to four distinct, keyword-rich, and seman-\ntically independent sub-queries. For example, the\nquery\u201cWhat were Alan Turing\u2019s main contribu-\ntions to computer science and his role in World\nWar II?\u201dis decomposed into targeted sub-queries\nlike\u201cAlan Turing\u2019s contributions to theoretical\ncomputer science\u201dand\u201cAlan Turing\u2019s role in\nbreaking the Enigma code.\u201dThis ensures that the\nretrieval process covers all conceptual facets of the\noriginal question.\n3.3.2 Hybrid Retrieval and Reranking\nFor each sub-query, we employ a hybrid retrieval\nstrategy to maximize recall. We perform both\ndense vector search (capturing semantic simi-\nlarity) (Karpukhin et al., 2020) and traditional\nkeyword-based sparse search (capturing exact\nmatches) (Robertson and Zaragoza, 2009). The\nresults from both methods are aggregated, and\nthe documents are re-ranked using theRecipro-\ncal Rank Fusion (RRF)algorithm (Cormack et al.,\n2009). RRF effectively combines the rankings from\nboth retrieval methods without requiring hyperpa-\nrameter tuning, producing a single, robustly ranked\nlist of the top-5 most relevant documents as candi-\ndate evidence.\n3.3.3 Evidence Filtering\nThe candidate evidence is then passed to a filtering\nmodule. An LLM agent evaluates each document\u2019s\nutility with respect to theoriginaluser query. Doc-\numents that are irrelevant, off-topic, or only tangen-\ntially related are discarded (Liu et al., 2023a). This\nstep is critical for increasing the signal-to-noise\nratio of the context provided to the final genera-\ntor, preventing the model from being distracted by\n\nFigure 1: Schematic overview of the FAIR-RAG architecture. The process starts with initial query analysis and\nadaptive language model selection (e.g., small, large, or reasoning LLM). For complex queries, it proceeds to query\ndecomposition, followed by an iterative refinement cycle involving hybrid retrieval and reranking, evidence filtering,\nand Structured Evidence Assessment (SEA). The loop iterates until evidence sufficiency is confirmed, culminating\nin faithful answer generation grounded in the aggregated evidence.\nnoisy or unhelpful information.\n3.3.4 Structured Evidence Assessment (SEA):\nThe Strategic Intelligence Analyst\nThe strategic core of our iterative refinement loop\nis the Structured Evidence Assessment (SEA) mod-\nule. Its primary function is not merely to verify\nevidence, but to perform a granulargap analysis\nthat generates an explicit, actionable signal for the\nnext iteration. For this critical task, we deliber-\nately chose achecklist-based methodologyover\nalternatives like abstractive summarization or di-\nrect question-answering. Abstractive summariza-\ntion, by design, often conceals the very gaps we\nneed to identify by creating a fluent narrative, while\ndirect QA yields binary outcomes that fail to pin-\npoint the precise location of missing information\nin a multi-fact query.\nOur approach operationalizes this checklist via\nan LLM agent prompted to act as aStrategic In-\ntelligence Analyst. Through carefully engineered\nprompts\u2014defining its role and providing few-shot\nexamples (see Appendix B)\u2014the agent firstde-\nconstructsthe user\u2019s query into a checklist of dis-\ncrete, required informational components or \u201cfind-\nings.\u201d It then systematicallyauditsthe collected\nevidence against this checklist, confirming which\nfindings are supported and identifying which re-\nmain as explicit\u201cintelligence gaps.\u201dThe evidence\nis deemed sufficient only if all required findings\nare confirmed.\nThe \u2018unchecked\u2019 items on this list constitute adi-\nrect, interpretable, and actionable signalfor the\nsubsequent Query Refinement module. This struc-\ntured, question-centric process provides a more\ncontrollable, reliable, and transparent mecha-\nnismfor gap analysis than less constrained meth-\nods, ensuring a rigorous evaluation that prevents\nthe system from being misled by large volumes of\ntangentially related information.\n\n3.3.5 Iterative Query Refinement\nIf the Structured Evidence Assessment (SEA) re-\nsults that the evidence is insufficient to correctly an-\nswer the question, the system activates the query re-\nfinement module, which is designed to be a highly\ntargeted intervention. An LLM agent uses the \u201cRe-\nmaining Gaps\u201d and \u201cConfirmed Findings\u201d from\nthe analyst\u2019s summary (generated in the previous\nstep) as its primary input. Its goal is to generate\nnew, laser-focused queries that are engineered to\nfindonlythe missing pieces of information. By\nleveraging the confirmed findings, the agent makes\nthe new queries more precise and avoids repeating\nprevious searches. For instance, if a query is\u201cIn\nwhat city was the lead scientist who broke the\nEnigma code buried?\u201dand the analyst\u2019s summary\nconfirms\u201cAlan Turing was the lead scientist\u201dbut\nidentifies\u201cAlan Turing\u2019s burial place\u201das a gap,\nthe refinement module will generate new, highly\ntargeted queries like\u201cAlan Turing burial place\u201d\nor\u201ccity where Alan Turing is buried.\u201dThese new\nqueries, now contextualized and specific, re-enter\nthe hybrid retrieval cycle (Step 3.3.2).\n3.4 Faithful Answer Generation\nOnce the Structured Evidence Assessment (SEA)\nreturns \u201cYes,\u201d the curated and validated evidence\nset is passed to the generator LLM selected in the\ninitial routing stage (Step 3.2). The generation\nprocess is governed by a meticulously engineered\nprompt that enforces astrict, evidence-only gen-\neration protocol. This constrained prompt is de-\nsigned to maximize faithfulness and minimize the\nrisk of hallucination by instructing the LLM to:\n\u2022 Base its answerexclusivelyon the provided\nevidence.\n\u2022 Avoid introducing any external information,\nparametric knowledge, or opinions.\n\u2022 Cite every claim by embedding reference to-\nkens (e.g., [1], [2]) that link to the source doc-\numents.\n\u2022 If the evidence is ultimately insufficient, state\nthis directly without attempting to speculate.\nThis highly constrained generation process is\nfundamental to mitigating hallucination and en-\nsures that the final output is not only accurate but\nalso transparent and fully traceable to its sources.\n3.5 Dynamic Resource Allocation and Prompt\nEngineering\nA key aspect of FAIR-RAG\u2019s design is the efficient\nuse of computational resources. We dynamically\nallocate LLMs of different sizes (e.g., SMALL,\nLARGE) for the various internal tasks based on\ntheir complexity. For instance, simpler tasks like\ntheInitial Query Analysis and Adaptive Routing\nare handled by a smaller model, while the highly\nnuanced task ofquery refinementis assigned to a\nmore capable model to ensure maximum precision.\nThis dynamic allocation optimizes the trade-off\nbetween performance, cost, and latency.\nFurthermore, all interactions with LLM agents\nare managed through astructured prompt engi-\nneering methodologydesigned to ensure reliabil-\nity and predictability [see Appendix B for details].\nOur prompts are consistently engineered to include\nseveral key components that guide the model\u2019s be-\nhavior:\n\u2022 Role and Context Definition:Each prompt\nbegins by establishing a clear role for the\nagent and the context of its task (e.g.,\u201cYou\nare a Strategic Intelligence Analyst. . . \u201d).\n\u2022 Task Specification:The primary goal or in-\ntent of the task is explicitly stated (e.g.,\u201cYour\nmission is to determine if the provided evi-\ndence is sufficient. . . \u201d).\n\u2022 Guided Reasoning (Scaffolding):We pro-\nvide clear, step-by-step instructions, analytical\nguidelines, or few-shot examples to structure\nthe model\u2019s reasoning process and ensure con-\nsistency.\n\u2022 Behavioral Constraints:Explicit rules are\nlaid out that the model must follow, govern-\ning its process and output (e.g.,\u201cYou MUST\nfollow this thinking process and output format\nexactly\u201d).\n\u2022 Output Formatting:The exact format for\nthe response is strictly defined to ensure\na machine-parseable and predictable output\n(e.g.,\u201cA single word: \u2018Yes\u2019 or \u2018No\u201d\u2019).\nThis robust, component-based prompting strat-\negy ensures that the LLM agents perform their des-\nignated roles reliably, contributing to the overall\nstability and performance of the framework (Liu\net al., 2021).\n\n4 Experimental Setup\nTo rigorously evaluate the performance of FAIR-\nRAG, we conduct a series of experiments on chal-\nlenging question-answering benchmarks. Our ex-\nperimental protocol is built upon the standardized\nand open-sourceFlashRAGtoolkit, ensuring a fair\nand reproducible comparison against existing meth-\nods.\n4.1 Datasets\nWe selected a carefully curated suite of four bench-\nmark datasets to assess the various capabilities of\nour framework, with a particular focus on complex,\nmulti-hop reasoning where standard RAG systems\noften underperform.\n\u2022 Multi-hop QA:We useHotpotQA,2Wiki-\nMultihopQA, andMusiQue. These datasets\nare specifically designed to require reason-\ning and synthesizing information across multi-\nple documents to arrive at an answer, making\nthem ideal for evaluating our iterative refine-\nment and adaptive query generation modules.\n\u2022 Open-Domain QA:We also useTriviaQA, a\npopular dataset for open-domain question an-\nswering, to ensure our model maintains strong\nperformance on simpler, fact-based queries.\nFor all experiments, we follow the standard prac-\ntice of using the official test split where available;\notherwise, we report results on the development\nsplit. Consistent with recent studies on computa-\ntionally intensive RAG models, and to manage the\nsubstantial API and computational costs associated\nwith iterative inference frameworks like ours, all\nevaluations are conducted on a randomly selected\nsubset of 1000 samples from each dataset.This\nsample size was deliberately chosen to strike a\ncritical balance between statistical robustness\nand experimental feasibility.It is large enough\nto ensure stable and meaningful performance com-\nparisons while enabling the comprehensive suite\nof ablation studies and baseline comparisons pre-\nsented in this work, which would be financially and\nlogistically intractable on the full datasets.\n4.2 Baselines\nWe compare FAIR-RAG against a comprehensive\nset of representative RAG baselines implemented\nwithin the FlashRAG framework. These baselines\ncover different architectural paradigms:\nDataset Task Type Source Samples\nHotpotQA multi-hop QA wiki 1000\n2WikiMultiHopQA multi-hop QA wiki 1000\nMusiQue multi-hop QA wiki 1000\nTriviaQA Open-Domain wiki/web 1000\nTable 1: Summary of Datasets\n\u2022 Standard RAG(Lewis et al., 2020): A con-\nventional retrieve-then-read pipeline serving\nas a fundamental baseline.\n\u2022 Iterative Methods:We includeIter-Retgen\n(ITRG)(Shao et al., 2023), which uses the\nprevious generation\u2019s output to retrieve new\ndocuments, andIRCoT(Trivedi et al., 2023),\nwhich integrates retrieval within a Chain-of-\nThought process.\n\u2022 Reasoning-based Methods:We compare\nagainstReAct(Yao et al., 2022), a popular\nagent-based framework that interleaves rea-\nsoning and action steps to solve problems.\n\u2022 Faithfulness-focused Methods: Self-\nRAG(Asai et al., 2023) is included as a\nstrong baseline that incorporates explicit\nreflection and self-critique steps to improve\nfaithfulness.\n\u2022Branching & Conditional Methods:We in-\ncludeSuRe(Kim et al., 2024), which gen-\nerates and ranks multiple candidate answers,\nandAdaptive-RAG(Jeong et al., 2024),\nwhich uses a classifier to conditionally route\nqueries through different execution paths.\n4.3 Evaluation Metrics\nTo provide a comprehensive assessment of our sys-\ntem\u2019s performance, we employ a suite of metrics\nthat capture both lexical accuracy and semantic\ncorrectness.\n4.3.1 Lexical-Based Metrics\nFollowing standard practice for question-answering\ntasks, we first report two traditional, token-based\nmetrics:\n\u2022 Exact Match (EM):This strict metric mea-\nsures the percentage of predictions that match\none of the ground-truth answers exactly, char-\nacter for character.\n\u2022 F1 Score:A more lenient metric that com-\nputes the harmonic mean of precision and re-\ncall at the token level. It accounts for partial\n\noverlaps and is less sensitive to minor phras-\ning differences than EM.\n4.3.2 Automated Evaluation using\nLLM-as-Judge\nLexical metrics like EM and F1 are often insuf-\nficient for evaluating generative models, as they\nunfairly penalize semantically correct answers that\nare phrased differently or contain additional, rele-\nvant context not present in the ground truth. To\novercome this limitation, we employ LLM-as-\nJudge methodologies for two distinct, nuanced eval-\nuation purposes, strategically selecting the judge\nmodel based on task complexity.\n\u2022 End-to-End Semantic Correctness\n(ACCLLM):For the large-scale evaluation\nof our main results (Table 2), we introduce\nLLM-as-Judge Accuracy (ACCLLM). This\nmetric requires a scalable and consistent\nbinary judgment on whether a final prediction\nis semantically equivalent to any ground-truth\nanswer. For this task, we use the highly\ncapable\u201cMeta-Llama-3-8B-Instruct\u201d\nmodel as the judge. (AI@Meta, 2024) The\nspecific prompt, designed for efficient \u201cYes\u201d\nor \u201cNo\u201d classification, is detailed in Appendix\nC.1 (Chiang et al., 2023; Zheng et al., 2023).\n\u2022 Component-Level Quality Score:Given\nthe nuanced, generative nature of interme-\ndiate outputs in our ablation study (e.g.,\nQuery Decomposition), a simple binary met-\nric is insufficient. Therefore, we employ an\nLLM-as-Judge methodology (Zheng et al.,\n2023), building on established frameworks\nlike G-Eval (Liu et al., 2023b) to ensure\na scalable and consistent assessment. For\nthis role, we selected Llama-4-Maverick-17B-\n128E-Instruct-FP8. (AI@Meta, 2025) This\nhighly capable model was specifically cho-\nsen not merely for its general performance,\nbut for its demonstrated aptitude in complex\nreasoning and nuanced instruction following.\nThese capabilities are critical for accurately\nassessing the quality of our internal compo-\nnents, where evaluation criteria are intricate\nand context-dependent. The reliability of this\nmodel as a proxy for human judgment in our\nspecific tasks is not an unsubstantiated claim;\nas we will demonstrate in our validation study\n(Section 5.2.1), our LLM-as-Judge\u2019s ratings\nshow a strong correlation with those of hu-\nman experts, confirming its suitability for this\nevaluation. To ensure high-quality, structured\nfeedback and mitigate potential biases, we de-\nsigned custom prompts for each component.\nThese prompts provide the judge with clear\ntask definitions, explicit scoring criteria on a\n1-to-5 scale, and illustrative examples. This\nrigorous, prompt-driven approach, detailed\nin Appendix C.2, ensures consistent and in-\nterpretable scores for a credible and multi-\nfaceted analysis of our pipeline\u2019s internal me-\nchanics.\n4.3.3 Reliability of LLM-as-Judge\nEvaluations\nTo establish the credibility of our dual LLM-as-\nJudge framework, we conducted two separate hu-\nman verification studies, one for each evaluation\ntype.\n\u2022 Verification of Binary Semantic Correct-\nness (ACCLLM):The reliability of the Llama-\n3-8B-Instruct judge, used for the ACC LLM\nmetric, was rigorously validated. A random\nsubset of300question-answer pairs was sam-\npled, stratified across all datasets. A human\nexpert,blinded to the LLM\u2019s original deci-\nsion to prevent bias, annotated these pairs for\nsemantic correctness. The human judgments\nshowed astrong degree of concordancewith\nthe LLM-as-Judge\u2019s binary \u201cYes/No\u201d outputs,\naligning in90%of the cases. This level of\nagreement is well within the range of typical\nhuman inter-annotator agreement for such a\nnuanced task. Therefore, we conclude that\nthe LLM-as-Judge serves as a reliable and\nscalable proxy for human evaluation in our\nexperiments.\n\u2022 Verification of Component-Level Quality\nScores:For the more nuanced 1-to-5 scale\nscoring performed by the powerful Llama-4-\nMaverick model in our ablation study, a simi-\nlar validation was conducted. A separate ran-\ndom subset of 100 generated outputs from\nthe component-level tasks (e.g., query refine-\nment) was evaluated by a human expert, again\nblinded to the LLM\u2019s score. The evaluation\ndemonstrated a95%agreement between hu-\nman and LLM judgments across these differ-\nent tasks. This confirms the judge\u2019s capability\n\nfor providing consistent, human-aligned qual-\nity assessments.\nThis dual validation confirms that our LLM-as-\nJudge methodologies, for both binary correctness\nand fine-grained quality scoring, serve as robust\nand reliable proxies for human evaluation, enabling\na credible and scalable assessment of our frame-\nwork\u2019s performance.\n4.4 Implementation Details\nTo ensure a controlled and fair comparison, all ex-\nperiments adhere to the global settings defined by\nthe FlashRAG framework, with specific adapta-\ntions for our models.\nMethodological Alignment for Fair Compari-\nson:A cornerstone of our evaluation is the prin-\nciple of fair comparison, ensuring that reported\nperformance gains are attributable to our core ar-\nchitectural innovations (i.e., the iterative refinement\ncycle) rather than peripheral components. Our full\nproposed architecture includes features designed\nfor optimal real-world efficiency, such as a hybrid\nretriever (Section 3.3.2) and an \u201cOBVIOUS\u201d query\nshortcut (Section 3.2). However, as these features\nare not standard in the baseline methods we com-\npare against, they could introduce an unfair advan-\ntage.\nTherefore, to create a level playing field,these\ntwo capabilities were systematically disabled\nduring all benchmark experiments for all meth-\nods, including our FAIR-RAG variants.Specifi-\ncally:\n\u2022 All models used a standardizeddense-only\nretriever.\n\u2022 The \u201cOBVIOUS\u201d query routing was deacti-\nvated, forcingevery query to pass through\nthe full RAG pipeline.\nThis rigorous alignment ensures that the ob-\nserved performance differences are a direct result\nof the models\u2019 reasoning and evidence-handling\ncapabilities.\nComponent Configuration:\n\u2022 Retriever:For all methods, we usee5-base-\nv2(Wang et al., 2024) as the sole dense re-\ntriever, configured to fetch the top 5 doc-\numents per query from the standard DPR\nWikipedia (Dec. 2018) (Karpukhin et al.,\n2020) corpus. The index is built using Faiss\n(Flat type) (Johnson et al., 2019) to ensure\naccuracy.\n\u2022 Generator Models:To benchmark against\na powerful and widely accessible model, we\nstandardize the generator for all baseline meth-\nods to be \u201cLlama-3-8B-Instruct\u201d, accessed\nvia API. A key exception is Self-RAG, for\nwhich we use its officially released, fine-tuned\nselfrag-llama-7b model to respect its original\ndesign.\n\u2022 FAIR-RAG Configuration:Our experimen-\ntal setup for FAIR-RAG is designed to ensure\na rigorous and fair comparison. We evaluate\ntwo primary configurations of our framework:\n\u2013 Uniform Model Configuration:To iso-\nlate the architectural benefits of our iter-\native approach, the FAIR-RAG 1-4 vari-\nants exclusively use Llama-3-8B-Instruct\nfor all internal tasks and for the final an-\nswer generation. This ensures a direct\nand fair comparison against all baseline\nmethods, which are also benchmarked us-\ning the same Llama-3-8B-Instruct model.\nIn this configuration, the adaptive LLM\nselection (SMALL, LARGE, REASON-\nING roles) is intentionally disabled, with\nall roles defaulting to the single model.\n\u2013 Adaptive LLM Configuration:To\ndemonstrate the full potential of our\nframework, we also report results for\nFAIR-RAG (Adaptive LLMs). This con-\nfiguration employs a dynamic, multi-\nagent allocation strategy to optimize the\ntrade-off between performance and cost:\n* For less complex internal tasks, such\nas query decomposition and Struc-\ntured Evidence Assessment (SEA),\nwe utilize Llama-3-8B-Instruct.\n* For more cognitively demanding\ntasks, including evidence filter-\ning, query refinement, and faith-\nful answer generation, we leverage\nthe more powerful Llama-3.1-70B-\nInstruct. (AI@Meta, 2024)\n* For tasks requiring deep reasoning,\nthe system routes to a specialized\nDeepSeek-R1 model. (DeepSeek-AI,\n2025)\n\nUnless otherwise specified, all other hyperpa-\nrameters adhere to the default settings of the un-\nderlying framework to maintain consistency across\nexperiments.\n5 Results\nThis section presents a comprehensive evaluation\nof FAIR-RAG. We first report the main end-to-end\nresults against a suite of strong baseline methods\n(Section 5.1). We then provide a deeper analysis\nof the framework\u2019s internal mechanics, including a\ncomponent-wise ablation study (Section 5.2.1) and\nan examination of the impact of iterative refinement\non answer quality and cost (Section 5.2.2).\n5.1 Main Results\nTable 2 presents the main performance comparison\nof FAIR-RAG and baseline methods across four di-\nverse question-answering benchmarks. Our frame-\nwork demonstrates state-of-the-art performance,\nparticularly on datasets that require complex, multi-\nstep reasoning. We report four key metrics:Exact\nMatch (EM)andF1-Scorefor lexical accuracy,\na script-basedAccuracy (ACC)which measures\nthe presence of the ground-truth answer string in\nthe generation, and our primary semantic metric,\nLLM-as-Judge Accuracy (ACCLLM). While EM,\nF1, and ACC are token-based,ACC LLM evaluates\nsemantic equivalence, offering a more robust as-\nsessment of generative answers.\nTo provide a fine-grained analysis of our frame-\nwork\u2019s iterative capabilities, we evaluate several\nconfigurations of FAIR-RAG in Table 2. The FAIR-\nRAG 1 to 4 variants correspond to the system\u2019s\nperformance with the maximum number of itera-\ntions capped at 1 to 4, respectively. To ensure a\nfair comparison against the baselines and to isolate\nthe impact of the iterative refinement cycle itself,\nthese variants utilize a single, consistent genera-\ntor model (Llama-3-8B-Instruct) across all stages.\nConversely, the (Adaptive LLMs) variants repre-\nsent our full, optimized framework, employing the\ndynamic allocation of different LLM agents (e.g.,\nsmall, large, reasoner) based on task complexity, as\ndetailed in our methodology.\nOur best-performing model,FAIR-RAG 3,\ndemonstrates leading performance within the\nparadigm of iterative and adaptive RAG. FAIR-\nRAG significantly outperforms strong representa-\ntive baselines from its architectural class across all\nmulti-hop benchmarks.It is critical to note that\nall comparisons were conducted under a unified\nand controlled experimental setup, ensuring a\nfair and reproducible evaluation.Within this rig-\norous framework, our results not only demonstrate\na consistent advantage in our direct head-to-head\ncomparisons but alsoset a new state-of-the-art\nperformance benchmark for this class of itera-\ntive methods on these datasets.While we refer-\nence previously published results from comparable\narchitectures (Shao et al., 2023; Asai et al., 2023;\nJeong et al., 2024), our primary claim of superiority\nis grounded in re-evaluating these methods under\nour standardized conditions to eliminate confound-\ning variables.\nThe most substantial gains are observed on the\nmulti-hop reasoning benchmarks. OnHotpotQA,\nour model achieves an F1 score of0.453, an abso-\nlute improvement of+8.3 pointsover the strongest\nbaseline, Iter-Retgen (0.370). Similarly, on2Wiki-\nMultiHopQAandMusique, FAIR-RAG achieves\nF1 scores of0.320and0.264, respectively, outper-\nforming the next-best method (Self-RAG) by a sig-\nnificant margin. These results strongly validate the\nefficacy of our core architectural contributions: the\nIterative Refinement Cycleand theStructured\nEvidence Assessment (SEA). These mechanisms\nempower FAIR-RAG to systematically deconstruct\ncomplex information needs, gather comprehensive\nevidence, and verify its sufficiency where single-\npass or less structured iterative methods fall short.\nNotably, FAIR-RAG also excels on the single-\nhop factual benchmark,TriviaQA, achieving an F1\nscore of0.731. This demonstrates that the frame-\nwork\u2019s sophisticated reasoning machinery does not\nimpose a penalty on simpler retrieval tasks and can\neffectively streamline its process, largely due to the\ninitialAdaptive Routingmodule.\nComparing different variants of our model, we\nsee a consistent performance increase from FAIR-\nRAG 1 to 4, indicating that the incremental en-\nhancements contribute positively. The introduction\nofAdaptive LLMsfurther boosts performance\nacross the board, confirming the benefits of dynam-\nically allocating computational resources based on\ntask complexity.\nTo complement the tabular data, Figure 2 pro-\nvides a visual comparison, plotting the F1 scores\n(bars) against the ACC LLM metric (lines) for all\nmethods across the four benchmarks. The figure\nvisually corroborates the superior performance of\nour FAIR-RAG framework, where its variants (in-\n\nType Method HotpotQA 2WikiMultiHopQA Musique TriviaQA\nEM F1 ACC ACC LLM EM F1 ACC ACC LLM EM F1 ACC ACC LLM EM F1 ACC ACC LLM\nSequential Standard RAG .238 .342 .328 .598 .086 .180 .282 .369 .074 .149 .137 .380 .570 .667 .675 .795\nBranching Sure .232 .346 .293 .646 .134 .198 .167 .408 .101 .169 .114 .397 .566 .675 .641 .799\nConditional Adaptive-RAG .144 .239 .368 .628 .041 .133 .340 .429 .038 .095 .160 .366 .480 .583 .670 .789\nReasoning ReAct .000 .028 .096 .503 .000 .062 .295 .467 .000 .016 .018 .376 .000 .046 .129 .667\nIterative\nIter-Retgen .265 .370 .353 .621 .104 .209 .310 .401 .115 .190 .178 .391 .581 .676 .689 .804\nSelf-RAG .174 .299 .321 .626 .123 .251 .333 .466 .073 .162 .132 .392 .385 .540 .639 .774\nIRCoT .006 .087 .399 .631 .001 .085.433 .488.001 .056 .210 .416 .016 .169 .674 .800\nIterative\nFAIR-RAG 1 .300 .398 .337 .628 .186 .288 .286 .414 .133 .216 .169 .418 .622 .706 .682 .821\nFAIR-RAG 2 .335 .447 .397 .689 .216 .325 .341 .458 .168 .253 .214 .465 .645 .732 .712 .837\nFAIR-RAG 3 .332 .447 .404 .697 .183 .305 .338 .450.178 .267 .221 .474 .631 .721 .701 .839\nFAIR-RAG 4.344 .456 .401 .697 .209 .333 .357 .486 .175 .266 .228 .475 .644 .728 .710 .837\nFAIR-RAG 2 (Adpt.) .331 .436 .384 .673 .183 .296 .313 .444 .158 .241 .207 .447 .640 .722 .704 .828\nFAIR-RAG 3 (Adpt.) .338 .453 .399 .694 .206 .320 .350 .452.178 .264 .222 .472 .645 .731 .710 .847\nTable 2: Main end-to-end performance comparison of FAIR-RAG against representative baselines across four\ndiverse question-answering benchmarks. The evaluation covers three complex multi-hop datasets (HotpotQA,\n2WikiMultiHopQA, Musique) and one open-domain factual dataset (TriviaQA). We report four metrics: Exact\nMatch (EM), F1-Score, script-based Accuracy (ACC), and LLM-as-Judge Accuracy (ACCLLM). Our FAIR-RAG\nvariants consistently achieve state-of-the-art performance, with the most significant improvements on the multi-hop\ntasks. The best-performing method for each metric is highlighted inbold. Underlined values denote results\nsurpassing the Self-RAG and Iter-Retgen baselines. All scores are based on 1000 test/dev samples.\ndicated by the patterned bars) consistently achieve\nthe highest F1 scores, particularly on the com-\nplex reasoning datasets of HotpotQA, 2WikiMulti-\nHopQA, and Musique.\nA key trend highlighted by the plots is thestrong\npositive correlation between the F1 score and\nthe ACCLLM. This suggests that the architectural\nimprovements within FAIR-RAG, which enhance\nthe accuracy of the Large Language Model\u2019s inter-\nnal processing and decision-making, are directly\nresponsible for the enhanced final answer accu-\nracy. This relationship is particularly evident in the\nHotpotQA and 2WikiMultiHopQA results, where\na noticeable uplift in the ACC LLM line for FAIR-\nRAG variants coincides with a significant increase\nin their corresponding F1 scores compared to the\nbaselines. Thus, the visualizations not only con-\nfirm FAIR-RAG\u2019s state-of-the-art performance but\nalso offer insight into the synergistic relationship\nbetween its internal reasoning accuracy and its final\noutput quality.\n5.2 Further Analysis\nBeyond the main end-to-end results, we conduct a\nseries of deeper analyses to deconstruct the sources\nof FAIR-RAG\u2019s performance. We perform a de-\ntailed component-wise evaluation to understand\nthe contribution of each module, analyze the spe-\ncific impact of the iterative refinement process, and\npresent a qualitative case study to illustrate the\nframework in action. For these fine-grained assess-\nments, we employ a sophisticatedLLM-as-Judge\nmethodology, using a powerful LLM to score the\noutput of each internal module against a set of pre-\ndefined criteria (Zheng et al., 2023). This task is\nguided by a structured prompt (see Appendix C for\ndetails).\n5.2.1 Component-wise Performance Analysis\n(Ablation Study)\nTo quantify the contribution of each key module\nwithin the FAIR-RAG pipeline, we conducted an\nablation study using an LLM-as-Judge method-\nology on 1000 samples from each dataset. The\nresults, summarized in Table 3, demonstrate the\nhigh functionality of each component, justifying\nits inclusion in the final architecture. For com-\nponents likeQuery DecompositionandQuery\nRefinement, the judge rated the output quality on a\n1-to-5 Likert scale. The rating was based on a holis-\ntic assessment of criteria includingrelevance(how\nwell it addresses the core need),specificity(how\nfocused the query is), andcoverage(whether it cap-\ntures all necessary facets of the information gap).\nThe average score across 1000 samples is reported.\nForEvidence Filtering, we report F1-Score based\non the LLM\u2019s ability to correctly classify docu-\nments as relevant or irrelevant. ForSEA, we report\nAccuracy based on its correctness in judging evi-\ndence sufficiency.\nThe analysis reveals several key insights:\n\u2022 Query Decomposition & Refinement are\nHighly Effective:The initialQuery De-\ncompositionmodule achieves a high average\nquality score across all datasets, peaking at\n4.33/5.0on TriviaQA. The subsequentQuery\nRefinementmodule scores even higher, with\nan average of4.45/5.0on HotpotQA. This\n\nFigure 2: Performance comparison of FAIR-RAG variants against baseline methods on four question-answering\nbenchmarks. Each subplot displays the F1 score (bars, left axis) and the LLM-as-Judge Accuracy (ACCLLM, line,\nright axis). Our FAIR-RAG models are highlighted with a hatched pattern. The results consistently demonstrate\nthe superiority of our framework, especially on complex multi-hop datasets (HotpotQA, 2WikiMultiHopQA, and\nMusiQue), where it significantly outperforms all baselines in F1 score while maintaining high semantic accuracy.\nAll evaluations are conducted on 1000 samples from each benchmark\u2019s development set.\nQuery Decomp. Evidence Filter SEA Query Refine.\nDataset Metric Value Metric Value Metric Value Metric Value\nHotpotQA Avg. Score 4.19 F1-Score 67.3% Accuracy 72.0% Avg. Score 4.45\n2WikiMultiHopQA Avg. Score 4.12 F1-Score 55.5% Accuracy 81.7% Avg. Score 4.39\nMusique Avg. Score 4.10 F1-Score 68.5% Accuracy 83.2% Avg. Score 4.42\nTriviaQA Avg. Score 4.33 F1-Score 76.1% Accuracy 54.4% Avg. Score 4.52\nTable 3: Component-level performance analysis of FAIR-RAG\u2019s core modules. This table isolates and evaluates the\neffectiveness of four key components: Query Decomposition, Evidence Filtering, Structured Evidence Assessment\n(SEA), and Query Refinement. The Avg. Score is based on a 1-to-5 Likert scale, while F1-Score and Accuracy are\nexpressed as percentages. All scores averaged over 1000 samples.\nvalidates that the LLM agents are proficient\nat both breaking down complex queries and\nintelligently generating new queries to fill in-\nformation gaps identified by the SEA module.\n\u2022 Evidence Filtering Presents a Precision-\nRecall Trade-off:The filtering module\u2019s per-\nformance varies, with F1 scores ranging from\n55.47%on 2WikiMultiHopQA to76.05%on\nTriviaQA. While the filter is effective at reduc-\ning context noise (precision), its aggressive na-\nture can sometimes prune useful information\n(recall). This highlights a classic trade-off and\npresents a clear avenue for future optimiza-\ntion.\n\u2022 Structured Evidence Assessment (SEA) is a\nChallenging but Crucial Task:The SEA\nmodule, which governs the iterative loop,\ndemonstrates strong performance on complex\nmulti-hop datasets, achieving an accuracy of\n81.73%on 2WikiMultiHopQA and83.19%\non Musique. Its lower accuracy on TriviaQA\n(54.38%) is expected, as the sufficiency deci-\nsion for single-hop factual questions can be\nmore ambiguous. These results confirm the\nmodule\u2019s value as the central control mecha-\nnism for the iterative process, proving highly\nreliable when it matters most.\nIt is important to note that a direct ablation study\n\nremoving the SEA and Query Refinement modules\nand replacing them with a fixed-iteration loop was\ndeliberately omitted. Such a stripped-down design,\nlacking adaptive control and explicit gap analysis,\nwould cease to be FAIR-RAG and instead would\nconceptually approximate the architectural princi-\nples of existing iterative baselines. For instance,\nwithout a targeted query refinement strategy driven\nby SEA, the system would likely resort to generat-\ning new queries from the previous answer, a core\nmechanism in ITER-RETGEN (Shao et al., 2023).\nFurthermore, by removing its primary mechanism\nfor evidence-aware self-correction, this simplified\nvariant would represent a less sophisticated control\nstrategy than that of models like Self-RAG (Asai\net al., 2023), which employ reflection at a more\ngranular level.\nGiven that FAIR-RAG already demonstrates a\nsignificant performance margin over these strong\nbaselines in our main experiments (see Table 2),\nour existing comparisons effectively serve as a val-\nidation of our adaptive, SEA-driven architecture\nversus these alternative control strategies. This\nconfirms that the observed performance gains are\nattributable to the synergistic and intelligent con-\ntrol of the SEA and Query Refinement modules,\nrather than merely the brute-force effect of repeated\niterations.\n5.2.2 Impact of Iterative Refinement\nA core hypothesis of this work is that iterative\nrefinement improves answer quality for complex\nquestions. We tested this by running the same set of\nquestions with the maximum number of iterations\nranging from 1 to 4. We then used an LLM-as-\nJudge to rank the resulting answers for each ques-\ntion. The results, along with efficiency metrics, are\npresented in Table 4.\nThe data reveals a clear and consistent pattern\nacross the three multi-hop datasets (HotpotQA,\n2WikiMultiHopQA, and Musique):\n\u2022 Optimal Performance at 2-3 Iterations:\nMoving from one to two iterations yields a\nsubstantial improvement in answer quality.\nOn 2WikiMultiHopQA, the average quality\nrank improves from3.08to2.31. The peak\nperformance is generally observed at either\nthe second or third iteration (e.g., an average\nrank of2.18at iteration 3 for 2WikiMulti-\nHopQA). This is accompanied by a highIm-\nprovement Rate, with the 2- or 3-iteration an-\nswer being judged superior to the 1-iteration\nanswer in approximately70%of cases for\n2WikiMultiHopQA.\n\u2022 Diminishing Returns:A fourth iteration con-\nsistently leads to a degradation in average an-\nswer quality across all complex datasets. This\nsuggests a point of diminishing returns where\nan additional retrieval cycle is more likely to\nintroduce noisy or tangentially related infor-\nmation that complicates the final synthesis\nstep.\n\u2022 Cost-Benefit Analysis:Each iteration adds\na considerable number of API calls and to-\nkens, increasing both latency and computa-\ntional cost. The optimal balance of 2-3 it-\nerations provides the best balance between\nanswer quality and resource consumption.\nConversely, on the simplerTriviaQAdataset,\nthe quality rank degrades with each additional it-\neration, confirming that for single-hop queries, the\ninitial retrieval is generally sufficient, and further\niterations are unnecessary and even detrimental.\nThis analysis confirms that iteration is crucial for\ncomplex reasoning, but an unrestrained number of\niterations is suboptimal. Our framework\u2019s default\nsetting of a maximum of 3 iterations is thereby em-\npirically justified as an effective balance between\nperformance and efficiency.\n5.2.3 A Complex Case Study: Comparative\nMulti-Hop Reasoning\nTo demonstrate the unique advantages of the FAIR-\nRAG architecture over other advanced RAG frame-\nworks, we analyze a hybridcomparative, multi-\nhop query. This type of query is particularly chal-\nlenging because it requires the system to conduct\ntwo parallel lines of multi-hop reasoning simulta-\nneously and then synthesize the results.\nThe query is: \u201cCompare the architectural\nstyles of the building that houses theMona Lisa\nand the museum in London that houses the\nRosetta Stone.\u201d\nStandard RAG Failure:A standard RAG sys-\ntem would treat this complex comparative query\nas a single, semantically overloaded search vector.\nThis unfocused approach is highly likely to fail for\ntwo primary reasons: First, it would struggle to si-\nmultaneously retrieve relevant, detailed documents\nfor both distinct lines of inquiry (the Louvre and\n\nDataset Max Iter. Avg. Answer Rank Improvement Rate Avg. API Calls Avg. Tokens/Query\n(Lower is better) (vs. Iter 1) (#)\nHotpotQA\n1 2.73 - 4.97 9,787\n22.23 58.50%6.64 14,332\n3 2.38 57.00% 7.83 17,281\n4 2.66 57.00% 9.01 20,299\n2WikiMultiHopQA\n1 3.08 - 4.99 9,823\n2 2.31 69.30% 7.10 15,413\n32.18 70.90%8.79 19,812\n4 2.43 67.30% 10.14 23,231\nMusique\n1 2.89 - 4.98 9,613\n22.2063.40% 7.25 15,688\n3 2.2863.70%9.01 20,162\n4 2.63 61.20% 10.56 24,218\nTriviaQA\n11.83- 4.97 9,572\n2 2.08 28.50% 5.97 12,071\n3 2.70 27.20% 6.76 14,003\n4 3.39 27.20% 7.25 15,186\nTable 4: Ablation study on the impact of the maximum number of refinement iterations on answer quality versus\ncomputational cost. The results reveal a clear point of diminishing returns. For complex multi-hop datasets,\noptimal performance (lowest Avg. Answer Rank) is achieved at 2 or 3 iterations, after which quality degrades\nwhile costs (API Calls, Tokens) continue to increase linearly. Conversely, for the simpler fact-based TriviaQA, any\niteration beyond the first proves detrimental. This analysis empirically justifies our framework\u2019s default setting of a\nthree-iteration maximum.\nthe British Museum). It might retrieve a general\ndocument about the Mona Lisa that lacks architec-\ntural details, or miss one of the entities entirely.\nSecond, and more fundamentally, standard RAG\nlacks the procedural logic to deconstruct the query,\npursue two parallel reasoning paths, and then syn-\nthesize the findings into a coherent comparison. It\nrelies on finding a single document that already\ncompares the two museums\u2019 architecture, which is\nhighly improbable. Consequently, a standard RAG\nwould likely produce a disjointed answer focusing\non only one of the entities, or fail entirely.\nWhy this query is difficult for other advanced\nRAGs:\n\u2022 AnITER-RETGENsystem, due to its inher-\nently single-threaded nature, might success-\nfully follow one reasoning path (e.g., find the\nLouvre, then its style). However, it lacks the\nmechanism to manage a parallel track simul-\ntaneously, causing it to lose the context of the\nsecond entity (the British Museum) and fail to\nproduce a coherent comparison.\n\u2022 AnAdaptive-RAGframework may correctly\nidentify the query as \u201ccomplex.\u201d However,\nit typically lacks a structured, multi-track\ndecomposition process. Without the abil-\nity to systematically pursue both information\nthreads in parallel before synthesis, its adap-\ntive strategy remains insufficient for true com-\nparative reasoning.\nFAIR-RAG in Action:\n\u2022 Iteration 1: Semantic Decomposition &\nParallel Initial Retrieval\n\u2013 Adaptive Sub-Queries:FAIR-RAG\u2019s\nfirst action is to decompose the compar-\native query into two distinct, parallel in-\nvestigative tracks:\n* Track A: [\u201cbuilding that houses the\nMona Lisa\u201d]\n* Track B: [\u201cmuseum in London that\nhouses the Rosetta Stone\u201d]\n\u2013 Retrieved Evidence:The system re-\ntrieves evidence for both tracks concur-\nrently:\n* Evidence A: \u201cThe Mona Lisa is\na half-length portrait painting by\nLeonardo da Vinci, on permanent dis-\nplay at theLouvre Museumin Paris,\nFrance.\u201d\n* Evidence B: \u201cThe Rosetta Stone is a\ngranodiorite stele on public display\nat theBritish Museumin London\nsince 1802.\u201d\n\u2013 Structured Evidence Assessment\n(SEA):\n* is_sufficient: \u2018No\u2019\n\nFigure 3: A qualitative case study demonstrating FAIR-RAG\u2019s two-iteration process for a complex comparative\nquery. In Iteration 1, the system decomposes the query and retrieves initial evidence identifying the museums but\nlacking the required architectural styles. The Structured Evidence Assessment (SEA) module correctly identifies this\ngap (Is Sufficient: No), triggering a second iteration. In Iteration 2, the system generates Refined Queries targeting\nthe missing information, successfully retrieves the necessary evidence, and confirms sufficiency. The process\nculminates in a Final Faithful Generation that synthesizes evidence from both iterations into a comprehensive\nanswer.\n* analysis_summary:The initial re-\ntrieval successfully resolved the pri-\nmary entities for both comparative\ntracks, identifying the Louvre Mu-\nseum and the British Museum. How-\never, the core \u2018Required Findings\u2019\nconcerning the architectural styles\nof these institutions remain unad-\ndressed. Therefore, critical informa-\ntion gaps prevent the synthesis of a\ncomplete answer.\n\u2022 Iteration 2: Parallel Query Refinement &\nEvidence Completion\n\u2013 Refined Queries:The refinement mod-\nule now uses the entities identified in Iter-\nation 1 to generate new, highly-focused\nqueries for each track:\n* Refined Query A: [\u201carchitectural\nstyle of the Louvre Museum\u201d]\n* Refined Query B: [\u201carchitectural\nstyle of the British Museum\u201d]\n\u2013 Retrieved Evidence:The new targeted\nqueries retrieve the final missing pieces\nof information:\n* Evidence A: \u201cThe architecture of the\nLouvre Palace is a complex blend\nof styles, from French Renaissance\nto Neoclassicism, famously comple-\nmented by I. M. Pei\u2019s Modernist\nglass pyramid.\u201d\n* Evidence B: \u201cThe main building of\nthe British Museum, designed by ar-\nchitect Sir Robert Smirke, is a mas-\nterpiece of theGreek Revivalstyle,\nfeaturing a grand quadrangle and a\nportico of 44 columns.\u201d\n\u2013 Structured Evidence Assessment\n\n(SEA):\n* is_sufficient: \u2018Yes\u2019\n* analysis_summary:The refined\nqueries successfully retrieved the fi-\nnal missing data points. All items on\nthe \u2018Required Findings\u2019 checklist\u2014\nthe identities of both museums and\ntheir respective architectural styles\u2014\nare now confirmed by the aggregated\nevidence. No information gaps re-\nmain, enabling a direct, evidence-\ngrounded comparison.\n\u2022Final Faithful Generation:The system syn-\nthesizes the evidence fromboth parallel\ntracksinto a single, structured, and fully-\ngrounded comparative answer:\n\u201cThe architectural styles of the buildings hous-\ning the Mona Lisa and the Rosetta Stone are\ndistinct. The Mona Lisa is housed in the\nLouvre Museumin Paris [1], which features\na complex blend of historical styles includ-\ningFrench Renaissance and Neoclassicism,\nalong with a famousModernistpyramid [3].\nThe Rosetta Stone is located in theBritish\nMuseumin London [2], which is a prime\nexample of theGreek Revivalarchitectural\nstyle [4].\u201d\nThis case study highlights FAIR-RAG\u2019s key\narchitectural advantage: its ability to systemati-\ncallydecomposea complex goal into parallel sub-\nproblems and then applyiterative refinementto\nsolve each one before synthesizing a comprehen-\nsive final answer. This structured, multi-threaded\nreasoning process is what enables it to succeed\nwhere other iterative or adaptive frameworks may\nfail.\n5.2.4 Failure Mode Analysis\nTo foster transparency and identify avenues for\nfuture improvement, we conducted a systematic\nfailure mode analysis on a sample of 200 unique\nerror instances drawn equally from our four bench-\nmark datasets: TriviaQA, MuSiQue, HotpotQA,\nand 2WikiMultihopQA. This analysis employed a\nhybrid methodology, combining LLM-based cat-\negorization with human expert validation to en-\nsure accuracy and depth. Our taxonomy distin-\nguishes between two fundamental sources of error:\n(1)Component-Level Failures, which stem from\nthe inherent limitations of the underlying modules\nFigure 4: Aggregate Distribution of Failure Sources.\nAnalysis of 200 error samples reveals a primary split\nbetween Component-Level Failures (63.5%) and Ar-\nchitectural Failures (36.5%). While architectural logic\noffers direct avenues for refinement, the majority of\nerrors stem from the inherent limitations of the founda-\ntional retrieval and generation models, identifying them\nas the principal bottleneck for the FAIR-RAG system.\n(i.e., the retriever and the generator LLM), and (2)\nArchitectural Failures, which are specific to the\ndecision-making logic of the FAIR-RAG frame-\nwork itself (i.e., Query Decomposition, Filtering,\nRefinement, and SEA).\nThis distinction is critical for understanding the\nsystem\u2019s bottlenecks. As shown in Figure 4, a sig-\nnificant majority of errors (63.5%) are Component-\nLevel, originating from the foundational tools our\nsystem is built upon. The remaining 36.5% are\nArchitectural, offering direct targets for refining\nFAIR-RAG\u2019s internal logic. This distribution un-\nderscores a key insight: while FAIR-RAG\u2019s itera-\ntive process is designed to mitigate the weaknesses\nof its components, the performance of these base\ncomponents remains the primary limiting factor in\noverall system accuracy.\n1. Component-Level Failures (63.5% of Errors):\nThe Foundational BottleneckThese errors are\nnot caused by FAIR-RAG\u2019s reasoning process but\nby the fundamental limitations of the tools it or-\nchestrates.\n\u2022 Retrieval Failure (32.5%):This was the sin-\ngle largest source of error across all datasets.\nThe retriever\u2019s inability to surface critical doc-\numents is a major obstacle, particularly for\nqueries requiring highly specific, long-tail fac-\ntual knowledge. The primary root cause was\nidentified as Knowledge Base Gaps, where the\nrequired information was simply absent from\nthe corpus. This was especially pronounced\n\nin TriviaQA, where nearly half of all failures\nwere retrieval-related due to the dataset\u2019s re-\nliance on obscure facts.\n\u2022 Generation Failure (31.0%):In these cases,\nthe correct evidence was successfully identi-\nfied and passed to the final generator, which\nstill produced a flawed answer. This category\nrepresents a significant challenge across all\ndatasets, highlighting the inherent faithfulness\nproblem in modern LLMs. Common failure\nsubtypes included: (i) Incorrect Entity Se-\nlection, where the model chose the wrong\nentity from a list of candidates in the evi-\ndence; (ii) Flawed Logical Inference, espe-\ncially in comparative questions (e.g., \u201cwho\nis younger?\u201d); and (iii) Misinterpretation of\nQuestion Granularity, such as providing a spe-\ncific year (\u201c1922\u201d) when a decade (\u201c1920s\u201d)\nwas requested.\n2. Architectural Failures (36.5% of Errors): Re-\nfining the FAIR-RAG LogicThese errors are di-\nrectly attributable to FAIR-RAG\u2019s internal decision-\nmaking modules and represent the most direct op-\nportunities for improving our framework.\n\u2022 SEA Error (24.5%):As the \u201cbrain\u201d of the\niterative process, failures in the Strategic Evi-\ndence Assessment module are particularly im-\npactful. These errors were significantly more\nprevalent in complex, multi-hop datasets like\nMuSiQue, HotpotQA, and 2WikiMultihopQA.\nThe most common subtypes were: (i) Faulty\nAnalysis of Evidence, where the SEA module\nfailed to make a correct logical inference from\nthe provided documents (e.g., misinterpreting\ncomplex genealogical relationships); and (ii)\nPremature Sufficiency Judgment, where the\nlogic incorrectly concluded that the gathered\nevidence was complete, thus halting the re-\nfinement loop too early.\n\u2022 Query Logic Failures (12.0% combined):\nThis group includes errors from the initial\nquery processing stages. Query Decompo-\nsition Errors (9.0%) were the most common,\noften stemming from an inability to challenge\nflawed premises within the user\u2019s question\n(e.g., processing a query with a historical\nanachronism) or generating overly broad sub-\nqueries. Query Refinement (1.5%) and Ev-\nidence Filtering Errors (1.5%) were exceed-\nFigure 5: Task-Dependent Distribution of Failure\nModes. The analysis highlights a strong correlation\nbetween task complexity and the primary failure bot-\ntleneck. For the factoid-centric TriviaQA, Retrieval\nFailures are dominant (47%). Conversely, for complex\nmulti-hop reasoning datasets like MuSiQue, HotpotQA,\nand 2WikiMultihopQA, the burden shifts towards rea-\nsoning, with SEA Errors becoming a major failure cate-\ngory (28-32%). This trend validates the critical role of\nthe strategic reasoning component (SEA) for success-\nfully navigating multi-step queries.\ningly rare, suggesting that these architectural\ncomponents are relatively robust.\nDataset-Specific Error DistributionsWhile the\naggregate view is informative, Figure 5 reveals\nthat the distribution of failure modes is highly de-\npendent on the nature of the task. For fact-based,\nsingle-hop datasets like TriviaQA, failures are over-\nwhelmingly concentrated in the Retrieval stage\n(47%). However, as query complexity increases in\nmulti-hop datasets (MuSiQue, HotpotQA, 2Wiki-\nMultihopQA), the burden shifts. In these cases,\nSEA Errors become significantly more prominent,\naccounting for 28-32% of all failures. This demon-\nstrates that for complex reasoning tasks, the pri-\nmary challenge moves beyond simply finding infor-\nmation to correctly reasoning about and managing\nit. This trend strongly validates the necessity of a\nsophisticated strategic control module like SEA in\nadvanced RAG systems.\n6 Conclusion\nIn this paper, we introducedFAIR-RAG, a novel,\nagentic framework designed to address a key limi-\ntation of existing Retrieval-Augmented Generation\nsystems: their unreliability in handling complex,\nmulti-hop queries. By architecting an evidence-\ndriven, iterative process, FAIR-RAG advances be-\nyond the static \u201cretrieve-then-read\u201d paradigm. Our\ncore contributions\u2014Adaptive Routing, theItera-\ntive Refinement Cycle, and the analytical gating\nmechanism of theStructured Evidence Assess-\nment (SEA)module\u2014work in synergy to progres-\n\nsively build and validate a comprehensive context\nbefore generation. The SEA\u2019s ability to systemati-\ncally deconstruct queries and identify information\ngaps provides a precise, actionable signal that di-\nrectly guides the query refinement process.\nOur extensive experiments demonstrate that\nFAIR-RAG\u2019s structured approach achieves leading\nperformance among comparable iterative and adap-\ntive RAG architectures across challenging multi-\nhop QA datasets like HotpotQA and 2WikiMul-\ntiHopQA. These results empirically validate our\ncentral hypothesis: that a procedural, multi-stage\nworkflow with explicit evidence assessment is es-\nsential for achieving high accuracy and faithfulness\nin knowledge-intensive tasks.\n6.1 Limitations\nDespite its strong performance, FAIR-RAG\npresents several inherent trade-offs and limitations\nthat warrant discussion:\n\u2022 Dependency on LLM Reasoning Fidelity\nand Prompt Engineering:The performance\nof FAIR-RAG\u2019s modular agents (e.g., query\nrefinement, SEA) is inherently bound by two\nkey factors: the reasoning capabilities of the\nunderlying LLMs and the meticulous design\nof the prompts that guide them. While our\nstructured prompting methodology\u2014which\nincorporates clear instructions, illustrative ex-\namples, and scaffolding techniques\u2014is de-\nsigned to ensure consistency and robustness,\nthe system\u2019s effectiveness remains sensitive\nto both the choice of the backbone model and\nthe specific phrasing of the prompts. This\ndual dependency is an intrinsic limitation of\ncurrent LLM-based agentic systems, where\nperformance gains are often tightly coupled\nwith advancements in both model architecture\nand sophisticated prompt engineering.\n\u2022 Comprehensiveness vs. Efficiency Trade-\noff:The iterative nature of FAIR-RAG, which\nis key to its high accuracy on complex queries,\nintroduces a natural trade-off with efficiency.\nAs shown in our analysis (Table 4), each re-\nfinement cycle increases overall latency and\ncomputational cost, making it more expensive\nthan single-shot RAG methods.\n\u2022 Potential for Error Propagation:As a multi-\nstage pipeline, errors in early stages can cas-\ncade. The SEA module, in particular, repre-\nsents a critical point of failure. An erroneous\nsufficiency judgment\u2014either a false positive\nthat terminates the loop prematurely or a false\nnegative that extends it unnecessarily\u2014can\nlead the evidence-gathering process astray.\n\u2022 Fixed Iteration Policy:The maximum of\nthree iterations is an empirically derived\nheuristic that balances performance and cost.\nHowever, a fixed limit lacks the flexibility to\nadapt to the varying complexity of individual\nqueries. Our results show this is a robust aver-\nage but may not be optimal for every specific\ncase.\n6.2 Future Work\nThe limitations of our current work open up several\npromising directions for future research to create\nmore efficient and adaptive systems:\n\u2022Distilling Task-Specific Expert Models:To\nmitigate the reliance on expensive, general-\npurpose LLMs, a promising direction is to\nfine-tune or distill smaller, specialized mod-\nels for each core task (Hinton et al., 2015).\nCreating dedicated expert models for query\nrefinement or evidence assessment could lead\nto a system that is faster, cheaper, and more\nrobust.\n\u2022 Learning a Dynamic Control Policy:The\niterative process can be framed as a sequential\ndecision-making problem. We propose explor-\ning Reinforcement Learning (RL) to train a\npolicy network that learns to dynamically con-\ntrol the workflow (Schick et al., 2023). At\neach step, this agent could decide whether to\nretrieve more information, refine the query,\nor proceed to generation, replacing the fixed-\nloop structure with a far more efficient and\nadaptive strategy.\n\u2022 Extension to Multimodal Reasoning:The\ncurrent FAIR-RAG framework operates exclu-\nsively on textual data. A natural and impactful\nextension would be to adapt its core principles\nof decomposition, iterative refinement, and\nstructured assessment to handle queries over\nmultimodal knowledge bases that include ta-\nbles, images, and structured data, creating a\nmore versatile and comprehensive question-\nanswering system (Alayrac et al., 2022).\n\nReferences\nAI@Meta. 2024. The llama 3 herd of models.arXiv\npreprint arXiv:2407.21783.\nAI@Meta. 2025. Llama 4: Maverick language models.\nIntroducing Llama 4 Scout and Llama 4 Maverick\n(official blog & model card).\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a visual language model for few-shot\nlearning.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\narXiv preprint arXiv:2310.11511.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nGordon V Cormack, Charles LA Clarke, and Stefan\nBuettcher. 2009. Reciprocal rank fusion outperforms\ncondorcet and individual rank learning methods. In\nProceedings of the 32nd international ACM SIGIR\nconference on Research and development in informa-\ntion retrieval, pages 758\u2013759.\nDeepSeek-AI. 2025. Deepseek-r1: An open\nlarge reasoning model family.arXiv preprint\narXiv:2501.12948.\nShahul Es, Jithin James, Luis Espinosa-Anke, and\nSteven Schockaert. 2025. Ragas: Automated evalua-\ntion of retrieval augmented generation.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.arXiv\npreprint arXiv:2002.08909.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2025. A survey on hallucination in large lan-\nguage models: Principles, taxonomy, challenges, and\nopen questions.ACM Transactions on Information\nSystems, 43(2):1\u201355.\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\nHwang, and Jong C. Park. 2024. Adaptive-rag:\nLearning to adapt retrieval-augmented large language\nmodels through question complexity.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation.ACM Comput-\ning Surveys, 55(12):1\u201338.\nZhengbao Jiang, Vladimir Lialin, Carroll Lin, Jane Liu,\nand Yelong Cheng. 2023. Flare: Forward-looking\nactive retrieval augmented generation.arXiv preprint\narXiv:2305.06983.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019.\nBillion-scale similarity search with GPUs.IEEE\nTransactions on Big Data, 7(3):535\u2013547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering.arXiv preprint\narXiv:2004.04906.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin\nPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha,\nand Jinwoo Shin. 2024. Sure: Summarizing re-\ntrievals using answer candidates for open-domain\nqa of llms.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Rodrigo Nogueira,\nHeinrich Paux, Pontus Stenetorp, Timo Rockt\u00e4schel,\nSebastian Riedel, et al. 2020. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks.Ad-\nvances in Neural Information Processing Systems,\n33:9459\u20139474.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023a. Lost in the middle: How language\nmodels use long contexts.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels.\nStephen Robertson and Hugo Zaragoza. 2009. The prob-\nabilistic relevance framework: Bm25 and beyond.\nFoundations and Trends\u00ae in Information Retrieval,\n3(4):333\u2013389.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nZihan Shao, Yue Zhang, Minchen Zhao, Wenxuan Chen,\nand Yang Zhang. 2023. Iter-retgen: Iterative retrieval-\naugmented generation for charge-based legal le-\nniency prediction.arXiv preprint arXiv:2310.03352.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2024. Text embeddings by weakly-\nsupervised contrastive pre-training.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models.Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nShi-Qi Yan, Kang Liu, Jia-Chen Li, Zhao-Xiang\nWang, Jie Zhang, and Lin Gui. 2024. Correc-\ntive retrieval augmented generation.arXiv preprint\narXiv:2401.15884.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning.arXiv preprint arXiv:1809.09600.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.arXiv preprint arXiv:2210.03629.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Brooks, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena.arXiv preprint arXiv:2306.05685.\nA Implementation Details and\nHyperparameters\nTo ensure full reproducibility of our experimen-\ntal results, this section provides a comprehensive\noverview of the models, configurations, and hy-\nperparameters used in our study. All experiments\nwere conducted within theFlashRAGframework,\na standardized library for RAG research. This\nensures that the underlying implementation de-\ntails, such as data loading and prompt templat-\ning, remain consistent across all compared meth-\nods. The source code and default configurations\ncan be found on the official project repository:\nhttps://github.com/ruc-nlpir/flashrag.\nThe specific configurations for our experiments\nare detailed in Table 5 below.\nCategory Parameter & Value\nGeneral Framework\nBase Framework: FlashRAG\nMax Iterations: 3\nRetriever Configuration\nModel: e5-base-v2\nDocuments Retrieved (top_k): 5\nIndex Type: Faiss (IndexFlatIP)\nPooling Method: mean\nQuery Max Length: 128\nGenerator Configuration\nBaseline: Llama-3-8B-Instruct\nSelf-RAG: selfrag-llama-7b\nFAIR-RAG: Llama-3-8B-Instruct\nMax New Tokens: 1024\nMax Input Length: 8000\nEvaluation Settings\nMetrics: em, f1, acc\nRandom Sample: True\nTable 5: Hyperparameter and Model Configuration De-\ntails. Unless specified otherwise, all hyperparameters\nsuch as temperature, top_p, and sequence lengths were\nkept at the default values provided by the FlashRAG\nframework to ensure a fair and controlled comparison\nacross all tested methodologies.\n\nB Full Prompts for the FAIR-RAG\nPipeline\nThis appendix reproduces the exact prompts that\nguide the behavior of the specialized agents within\nthe FAIR-RAG pipeline.\nB.1 Query Validation and Dynamic Model\nSelection\nThe following prompt is used by the initial agent\nto validate the user\u2019s query for clarity and safety,\nand to select the most appropriate execution model\n(e.g., simple RAG vs. full agentic pipeline).\nPROMPT = \"\"\"\n**Situation:** A user has submitted a question to a Question\nAnswering System that uses different processing\nstrategies based on query complexity.\n**Intent:**\nAnalyze the user's question to determine the optimal\nprocessing strategy required to generate the most\naccurate answer. The strategies are: Factual Retrieval\n(SMALL), Information Synthesis (LARGE), or Multi-Step\nDeduction (REASONER).\n**Scaffolding:**\nYou are a highly-calibrated query analysis agent. Your task\nis to classify the user's question into one of the\nthree categories below based on the cognitive process\nrequired to answer it. After \"Selected Label:\", output\nONLY the exact label.\n- **\"SMALL\" (Factual Retrieval):**\n- **Process:** Requires finding a single, self-contained\nfact. The answer is typically explicit and doesn't need\nmuch context.\n- **Example:** \"When was the Eiffel Tower completed?\" or\n\"Who is the CEO of NVIDIA?\"\n- **\"LARGE\" (Information Synthesis):**\n- **Process:** Requires combining, summarizing, or\ncomparing information from one or more documents to\nform a coherent, explanatory answer. This is the\nappropriate choice for most standard, open-ended\nquestions.\n- **Example:** \"What is the difference between nuclear\nfission and fusion?\" or \"Explain the impact of the\nprinting press on the Renaissance.\"\n- **\"REASONER\" (Multi-Step Deduction):**\n- **Process:** The answer is not explicitly stated and\nmust be inferred by chaining multiple pieces of\ninformation together (multi-hop reasoning) or by\nperforming calculations.\n- **Example:** \"What is the hometown of the director of\nthe movie that starred Tom Hanks and was released in\n1994?\" or \"If a car travels 150 km in 2 hours, what is\nits average speed in meters per second?\"\n**Analytical Guidelines for Classification:**\n1. \"SMALL\" (Factual Extraction):\n- Cognitive Task: Find and extract a specific, named\nentity or a short, self-contained fact (e.g., a name,\ndate, location, number, or title).\n- Key Indicators: The question can be answered with a\nsingle piece of information, often a proper noun or a\nspecific value. It typically starts with \"Who,\" \"When,\"\n\"Where,\" or \"What is the name of...\"\n- Example Questions: \"Who directed the movie Inception?\"\nor \"What year did the Berlin Wall fall?\"\n- Decision Rule: Classify as SMALL if the expected\nanswer is a concise, singular fact that requires no\nfurther explanation or combination of information.\n2. \"LARGE\" (Information Synthesis & Elaboration):\n- Cognitive Task: Gather, combine, and summarize\ninformation from one or multiple sources to form a\ncohesive, descriptive answer. This involves explaining\nconcepts, comparing entities, or describing processes.\n- Key Indicators: The question asks for an explanation,\ndescription, comparison, or summary. It often contains\nkeywords like \"Explain,\" \"Describe,\" \"Compare,\" \"What\nis the difference between,\" or \"Summarize the impact\nof...\"\n- Example Questions: \"What are the main differences\nbetween crocodiles and alligators?\" or \"Explain the\nprimary causes of the Industrial Revolution.\"\n- Decision Rule: Classify as LARGE if the expected\nanswer is a paragraph or a detailed sentence that\ncombines multiple facts into a comprehensive\nexplanation.\n3. \"REASONER\" (Logical Inference & Multi-Step Deduction):\n- Cognitive Task: Connect multiple, separate pieces of\ninformation to infer a new fact that is not explicitly\nstated in any single document. This often involves a\nchain of logic or a sequence of dependent questions.\n- Key Indicators: The question requires finding an\nintermediate answer to proceed to the next step. It\noften involves relationships between different entities\nor requires a calculation.\n- Example Questions: \"What was the nationality of the\nlead actress in the movie directed by the person who\nmade Titanic?\" or \"Which team won the FIFA World Cup in\nthe year the lead singer of Queen was born?\"\n- Decision Rule: Classify as REASONER if the answer\ncannot be found directly but must be constructed by\nfirst finding Fact A, then using Fact A to find Fact B.\n**User Question:** \"{user_query}\"\n**Constraints:**\n- Respond with ONLY one of the three labels: SMALL, LARGE, or\nREASONER.\n- Do not provide any explanations or additional text.\n- The label MUST be on a new line after \"Selected Label:\".\n**Output:**\nSelected Label:\n\"\"\"\nB.2 Query Decomposition\nThis prompt instructs the decomposition agent to\nbreak down a complex, multi-faceted user query\ninto a set of simpler, semantically distinct sub-\nqueries for parallel or sequential retrieval.\nPROMPT = \"\"\"\n**Situation:** You are an expert query analyst for a general\nknowledge Question-Answering system. A user has asked a\nquestion that might be complex, comparative, or\nmulti-faceted. Your task is to decompose this question\ninto a set of precise, meaningful, and distinct\n\nsub-queries to ensure the retrieval system can find\ncomprehensive and accurate evidence from a database.\n**Intent:** Decompose the original user question into its\ncore semantic components. Transform these components\ninto short, keyword-rich, and meaningful search phrases\nin English. The goal is to generate queries that, when\nsearched, will collectively cover all aspects of the\noriginal question.\n**Scaffolding:**\nFirst, understand the principles of effective decomposition:\n1. **Identify Distinct Concepts:** Separate the main\nsubjects, actions, conditions, and comparisons in the\nquery.\n2. **Use Synonyms & Related Terms:** Think about different\nways a concept might be phrased in the database (e.g.,\n\"interaction\" can be searched as \"relationship\" or\n\"cooperation\").\n3. **Create Meaningful Phrases:** Instead of single keywords,\ngenerate short phrases that preserve the context of the\nsub-question.\n4. **Cover All Angles:** Ensure every part of the original\nquestion is represented by at least one sub-query.\nNow, study the following example carefully to understand how\nto apply these principles.\n--- EXAMPLE ---\n**Original User Query:** \"What was Albert Einstein's view on\nquantum mechanics and how did he interact with Niels\nBohr about it?\"\n**Rationale/Analysis (This is your thought process):**\nThe query has two main, distinct parts:\n1. Einstein's **opinion/view** about quantum mechanics.\n2. Einstein's **interaction** with Niels Bohr on the topic.\nA good search needs to find evidence for both aspects\nseparately. Simply searching for \"Einstein quantum\nmechanics\" might not retrieve documents that\nspecifically discuss his \"view\" or \"interaction\".\nTherefore, I should create targeted queries for each\nconcept.\n**Optimized Queries (Output):**\n- Einstein's opinion on quantum mechanics\n- Einstein Bohr debates on quantum theory\n- Collaboration between Einstein and Bohr\n- Einstein's criticism of quantum mechanics\n--- END OF EXAMPLE ---\nNow, apply this exact methodology to decompose the following\nquery.\n**User Query:** \"{user_query}\"\n**Constraints:**\n- The output must be a list of meaningful search phrases.\n- Each phrase must be on a new line, prefixed with a hyphen\n(-).\n- Queries must be in English.\n- Generate an optimized list of 1 to 4 sub-queries. Create\nONLY as many as are **truly necessary** to cover all\naspects of the original question.\n**Output:** (just write Optimized Queries and do not explain\nany more and do not say \"Here are the optimized\nqueries:\" or something like that.)\nOptimized Queries: (A list of optimized queries)\n\"\"\"\nB.3 Evidence Filtering\nThis prompt guides the filtering agent to assess\nthe relevance and quality of the retrieved evidence\nchunks against a given (sub-)query, discarding ir-\nrelevant, redundant, or low-quality information.\nPROMPT = \"\"\"\nYou are filtering retrieved documents for a\nquestion-answering system. Your goal is to KEEP all\ndocuments that could contribute to answering the query.\n**IMPORTANT PRINCIPLES:**\n1. BE INCLUSIVE: When in doubt, KEEP the document.\n2. A document is useful if it contains factual information\nabout the entities/topics in the query.\n3. Even partial information is valuable (e.g., a document\nabout Terry Richardson without birthdate is still\nuseful for a query about his age).\n4. Documents are only \"Not Useful\" if they are about\nCOMPLETELY DIFFERENT entities or topics.\n**Original User Query:** \"{original_query}\"\n**Retrieved Documents (Batch {batch_number}):**\n{numbered_candidates_text_for_prompt}\n**TASK:** Identify ONLY documents that are completely\nirrelevant. A document is irrelevant ONLY if:\n- It's about a different person/entity with a similar name\n(e.g., Tony Richardson vs Terry Richardson).\n- It's about a completely unrelated topic.\n- It contains no information about any entity mentioned in\nthe query.\n**OUTPUT FORMAT:**\n- List ONLY the document IDs that should be removed\n- If all documents are potentially useful, output: None\n- Format: [doc_X], [doc_Y] or None\n**You MUST KEEP documents that have:**\n- Documents about the correct person/entity, even without\nspecific dates/facts.\n- Biographical information (birth dates, career details,\nachievements).\n- Relationships or connections between queried entities.\n- Specific facts relevant to the query type (dates for\ntemporal queries, attributes for comparisons).\n- Contextual information that helps understand the entities.\n**Examples of what to REMOVE:**\n- Documents about different people with similar names.\n- Documents about unrelated topics.\n- Duplicate documents (keep the most informative version).\n**Output:** (A list of **irrelevant** temporary document IDs,\nor \"None\")\nUnhelpful Document IDs:\n\"\"\"\n\nB.4 Structured Evidence Assessment (SEA)\nThe Structured Evidence Assessment (SEA) agent\nuses the following prompt to analyse the evidence\nand determine if the gathered and filtered evidence\nis adequate to form a complete and faithful answer\nto the user\u2019s query.\nPROMPT = \"\"\"\n**Role:** You are a Strategic Intelligence Analyst. Your\nmission is to determine if the provided evidence is\nsufficient to accurately answer the user's question by\nfollowing a sequential analysis.\n**Core Mission:** Your entire process must be\nquestion-centric, not evidence-centric. You will\ndeconstruct the user's query into a checklist of\nrequired information, and then systematically verify\neach item against the evidence. You MUST ignore all\ninformation, however interesting, that is not on your\nchecklist.\n**You MUST follow this thinking process and output format\nexactly:**\n**1. Mission Deconstruction:**\n- **Main Goal:** [State briefly the primary objective of the\nuser's question and what the user's question requires\nyou to find]\n- **Required Findings:** [List the specific, individual\npieces of information needed to answer the question. A\n\"finding\" can be a direct fact or a logical inference\nfrom clues.]\n**2. Intelligence Synthesis & Analysis:**\n- **Confirmed Findings:** [Go through your \"Required\nFindings\" checklist. For each item, state what the\nevidence confirms. If the finding is not stated\ndirectly, explain the logical inference you made from\nthe provided clues. You MUST only mention facts that\ncan contribute to answering the question's required\ncomponents (checklist). You MUST ignore any evidence,\nentities, or facts-even if interesting-that do not help\nanswer the specific components of the user's question.\nDo not mention irrelevant people or topics in your\nanalysis. You are an expert. If the evidence provides\nstrong, logical clues (e.g., a person's birthplace in a\ncountry, a job title within an industry), you MUST make\nthe logical inference (e.g., determining nationality,\nprofession). Do not use weak phrases like \"it does not\nexplicitly state.\"]\n- **Remaining Gaps:** [If there is missing information,\nclearly state what crucial information is still\nmissing, formulating it as a requirement for the next\nphase that creates new queries to search more. else\nNone]\n**3. Final Assessment:**\n- **Conclusion:** [The final answer may not be explicitly\nstated in a single sentence. You are an expert. If the\nevidence provides strong, logical clues (e.g., a\nperson's birthplace in a country, a job title within an\nindustry), you MUST make the logical inference (e.g.,\ndetermining nationality, profession). Do not use weak\nphrases like \"it does not explicitly state.\"]\n- **Sufficient:** [A single word: \"Yes\" if the \"Remaining\nGaps\" list is empty, or \"No\" if any required finding is\nstill missing.]\n--- EXAMPLES ---\n**--- Example 1 (Insufficient Evidence - Clear Gap) ---**\n**Original Question:** \"What was the official box office\ngross for the film directed by the creator of the TV\nseries *'Seinfeld'*?\"\n**Evidence:**\n- \"Larry David, the creator of the acclaimed TV series\n*'Seinfeld'*, also wrote and directed the 2013 film\n*'Clear History'*.\"\n- \"The film *'Clear History'* starred Larry David and Jon\nHamm and was released on HBO.\"\n- \"The 2019 film *'Joker'* had a box office gross of over $1\nbillion.\"\n**Your Output for Example 1:**\n**1. Mission Deconstruction:**\n- **Main Goal:** To find the box office gross for the film\ndirected by the creator of *'Seinfeld'*.\n- **Required Findings:** A: The identity of the creator of\n*'Seinfeld'*; B: The name of the film they directed; C:\nThe official box office gross of that film.\n**2. Intelligence Synthesis & Analysis:**\n- **Confirmed Findings:** A: The evidence confirms the\ncreator is **Larry David**. B: The evidence confirms\nthe film he directed is **'Clear History'**.\n- **Remaining Gaps:** C: The official box office gross for\nthe film *'Clear History'*.\n**3. Final Assessment:**\n- **Conclusion:** We have identified the director **Larry\nDavid** and the film he directed is **'Clear\nHistory'**. but the evidence lacks The official box\noffice gross for the film *'Clear History'* to answer\nthe question.\n- **Sufficient:** No\n**--- Example 2 (Sufficient Evidence - Inference Required)\n---**\n**Original Question:** \"Who is older, the author of\n*'Dracula'* or the lead actor from the 1931 film\nadaptation?\"\n**Evidence:**\n- \"Bram Stoker, the Irish author, wrote the classic horror\nnovel *'Dracula'* in 1897.\"\n- \"Stoker was born in Dublin, Ireland, in November 1847.\"\n- \"The 1931 film adaptation of *'Dracula'* famously starred\nHungarian-American actor Bela Lugosi in the title role.\"\n- \"Bela Lugosi's date of birth is recorded as October 20,\n1882.\"\n**Your Output for Example 2:**\n**1. Mission Deconstruction:**\n- **Main Goal:** To compare the ages of the author of\n*'Dracula'* and the lead actor of the 1931 film.\n- **Required Findings:** A: The birth year of the author of\n*'Dracula'*; B: The birth year of the lead actor of the\n1931 film.\n**2. Intelligence Synthesis & Analysis:**\n- **Confirmed Findings:** A: The evidence states the author\nis **Bram Stoker**, who was born in **1847**. B: The\nevidence states the lead actor is **Bela Lugosi**, who\nwas born in **1882**.\n- **Remaining Gaps:** None.\n**3. Final Assessment:**\n- **Conclusion:** We have found the birth year of the author\nof *'Dracula'* is **Bram Stoker**, who was born in\n**1847**. the lead actor of the 1931 film is **Bela\n\nLugosi**, who was born in **1882**. We have found the\nbirth years for both required individuals and can\ntherefore perform the age comparison.\n- **Sufficient:** Yes\n--- END OF EXAMPLES ---\nNow, perform this task for the following:\n**Original Question:**\n\"{original_query}\"\n**Evidence:**\n{combined_evidence}\n\"\"\"\nB.5 Query Refinement\nIf the evidence is found to be insufficient by the\nStructured Evidence Assessment (SEA) agent, this\nprompt is used to generate a refined or a new follow-\nup query to retrieve the missing information.\nPROMPT = \"\"\"\n**Situation:** An initial analysis of the evidence has\nconfirmed some facts but also identified specific\ninformation that is still missing and required to\nanswer the user's original query.\n**Intent:** Generate a new, optimized list of search queries\nthat are laser-focused on finding ONLY the missing\npieces of information identified in the \"Analysis\nSummary\".\n**Scaffolding & Logic:**\n- **USE the known facts** from the summary to make the new\nqueries more precise (e.g., use a person's name once\nit's known).\n- **TARGET the missing information** from the summary\ndirectly. Each new query should aim to resolve one of\nthe identified gaps.\n- **AVOID repeating** or rephrasing previous queries.\n--- ADVANCED EXAMPLE ---\n**Original Question:** \"How old is the youngest child of the\ndirector of the film *Inception*?\"\n**Analysis Summary:**\nBased on the evidence, we know that the director of\n*Inception* is **Christopher Nolan**. Christopher Nolan\nis married to producer Emma Thomas. Christopher Nolan\nhas children. However, the provided documents contain\n**no specific information about his children**, such as\ntheir names and birth dates. To answer the question, we\nstill need to find: **the names and ages of Christopher\nNolan's children** to identify the youngest.\n**Previous Queries:**\n- director of Inception\n- Christopher Nolan films\n**Your Output for Example:**\nImproved Queries:\n- Christopher Nolan children names\n- Christopher Nolan children birth dates\n- Youngest child of Christopher Nolan and Emma Thomas\n--- END OF EXAMPLE ---\nNow, apply this exact logic to the following inputs:\n**Original Question:** {original_query}\n**Analysis Summary:**\n{analysis_summary}\n**Previous Queries:**\n{combined_previous_queries}\n**Constraints:**\n- Generate an optimized list of 1 to 4 sub-queries. Create\nonly as many as are truly necessary.\n- Queries must be simple, independent, meaningful, and\nkeyword-focused.\n- **Leverage the \"Known Facts\" to create highly targeted\nqueries.** For example, once the summary confirms the\ndirector is'Christopher Nolan', the next query should\nbe \"Christopher Nolan children ages\", not a generic\n\"director of Inception children ages\".\n**Output:** (A list of new, targeted queries. Do not explain\nanything and do not say \"Here are the optimized\nqueries:\" or something like that.)\nImproved Queries:\n\"\"\"\nB.6 Faithful Answer Generation\nThis is the final and most comprehensive prompt,\ninstructing the generation model to synthesize the\nfiltered evidence into a faithful, accurate, and well-\nstructured answer. It strictly constrains the model\nto use only the provided sources and to avoid any\nform of hallucination.\nPROMPT = \"\"\"Answer the question based on the given documents.\nONLY give me the answer and do not output any other words.\nThe following are given documents.\nThe retrieval documents are listed as follows:\n{combined_evidence}\nQuestion:\n{original_query}\nAnswer:\n\"\"\"\nC LLM-as-Judge Evaluation Prompts\nThis appendix contains the complete and\nunabridged prompts used in ourLLM-as-Judge\nevaluation framework. To ensure full transparency\nand enable the replication of our evaluation\nmethodology, we provide the exact instructions\ngiven to the judge model for each assessment\ncriterion. Each prompt is carefully designed to\nelicit a consistent and unbiased evaluation of a\nspecific quality aspect of the generated responses.\n\nThe evaluation is performed by providing the\njudge model with the original query, the retrieved\ncontext, and the generated answer, along with one\nof the following instructional prompts.\nC.1 Binary Semantic Correctness (ACC LLM)\nTo ensure the reproducibility of our semantic accu-\nracy evaluation, this section details the prompt used\nfor theLLM-as-Judge Accuracy (ACC LLM)met-\nric, as reported in Table 2. The prompt instructs the\nLLM to act as an impartial judge, comparing the\nmodel\u2019s generated answer against a set of ground-\ntruth answers. It provides a binary \"Yes\" or \"No\"\njudgment in a structured JSON format, enabling\nautomated and consistent evaluation of semantic\ncorrectness.\nACC_PROMPT = \"\"\"You are an impartial judge. Evaluate whether\nthe model's prediction correctly answers the given\nquestion. The prediction is correct if it implies ANY\nof the ground-truth answers provided.\n- Question:\n{question}\n- Ground-truth Answers (The prediction is correct if it\nmatches ANY of these):\n{answer}\n- Prediction:\n{model_output}\nDoes the Prediction imply any of the Ground-truth Answers?\nRespond with a JSON object containing a single key \"judgment\"\nwith a value of \"Yes\" or \"No\".\nExample: {\"judgment\": \"Yes\"}\n\"\"\"\nC.2 Component-Level Quality Scoring\nThis section provides the prompt template used for\nourcomponent-level ablation study, with results\npresented in Table 3. Unlike the binary correct-\nness evaluation in Appendix C.1, this prompt is\ndesigned for a more nuanced quality assessment.\nIt instructs the LLM-as-Judge to evaluate the out-\nput of specific generative modules (i.e., Query De-\ncomposition and Query Refinement) and assign a\nquality score on a 1-to-5 Likert scale. This fine-\ngrained analysis allows us to isolate and measure\nthe efficacy of individual components within the\nFAIR-RAG pipeline.\nPROMPTS = {\n\"query_decomposition\": \"\"\"\nYou are an expert AI evaluator specializing in search\nand query analysis. Your task is to assess the quality\nof query decomposition.\nEvaluate the generated sub-queries based on the\noriginal user question using the following criteria:\n1. **Relevance:** How directly related is each\nsub-query to the main question?\n2. **Coverage:** Do the sub-queries collectively\ncover all essential aspects of the main question?\n3. **Efficiency:** Are the sub-queries concise,\nfocused, and well-formed for a search engine?\n[User Question]:\n\"{question}\"\n[Generated Sub-Queries]:\n{sub_queries}\nProvide your assessment in the following JSON format:\n{{\n\"score\": <A numeric score from 1.0 (Very Poor) to\n5.0 (Excellent) based on the criteria above>,\n\"reasoning\": \"<A very brief explanation for your\nscore>\"\n}}\n\"\"\",\n\"filter_efficacy\": \"\"\"\nYou are an expert auditor for an AI's document\nfiltering module. Your task is to meticulously evaluate\nthe filter's decisions by strictly adhering to the\n**exact instructions and principles** it was originally\ngiven.\n[User Question]:\n\"{question}\"\n[The Filter's Original Instructions & Principles]:\nThe filter's goal was to identify and discard \"Not\nUseful\" documents. It operated under the following\nrules:\n1. **Primary Principle:** **\"BE INCLUSIVE: When in\ndoubt, KEEP the document.\"**\n2. **Definition of \"Useful\":** A document is\nconsidered useful if it contains factual information\nabout the entities or topics in the query. **Even\npartial information is valuable.**\n3. **Definition of \"Not Useful\":** A document is\nonly \"Not Useful\" if it is about **completely\ndifferent** entities/topics or contains no relevant\ninformation.\n4. **Specific \"KEEP\" Criteria:** The filter was\nexplicitly instructed to **KEEP** documents containing:\n- The correct person/entity, even without\nall specific facts.\n- Biographical information (birth dates,\ncareer details, achievements).\n- Relationships or connections between\nqueried entities.\n- Specific facts relevant to the query type\n(e.g., dates).\n- General contextual information that helps\nunderstand the entities.\n5. **Specific \"REMOVE\" Criteria:** The filter was\ngiven examples of what to **REMOVE**:\n\n- Documents about different people with\nsimilar names.\n- Documents about completely unrelated\ntopics.\n- Duplicate documents.\nYour audit must strictly follow all of the same\nrules, especially the **\"BE INCLUSIVE\"** principle.\n[Documents the Filter KEPT]:\n{kept_docs}\n[Documents the Filter DISCARDED]:\n{discarded_docs}\n**Your Audit Task:**\nReferencing the filter's original instructions above,\nidentify its errors:\n1. **Precision Errors (Incorrectly Kept):** Review\nthe KEPT list. Identify the IDs of any documents that\nare **clearly \"Not Useful\"** and should have been\ndiscarded. If a document is borderline but meets any of\nthe \"KEEP\" criteria, the filter was **correct** to keep\nit.\n2. **Recall Errors (Incorrectly Discarded):** Review\nthe DISCARDED list. Identify the IDs of any documents\nthat were **unambiguously \"Useful\"** based on the\ncriteria and should have been kept.\nProvide your audit findings in the following strict\nJSON format. If no errors are found in a category,\nprovide an empty list.\n{{\n\"incorrectly_kept_ids\": [\"<ID of any'Not Useful'\ndocument found in the KEPT list>\", ...],\n\"incorrectly_discarded_ids\": [\"<ID of any\n'Useful'document found in the DISCARDED list>\", ...]\n}}\n\"\"\",\n\"sufficiency_check\": \"\"\"\n**Role:** You are a pragmatic and efficient QA\nEvaluator. Your goal is to determine if the provided\nevidence is \"good enough\" to satisfactorily answer the\nuser's question.\n**Core Task:**\nYour task is to assess if the main goal of the user's\nquestion can be achieved with the given evidence. You\nmust distinguish between \"critical\" missing information\nand \"nice-to-have\" details.\n**Guiding Principles:**\n1. **Focus on the Primary Intent:** First, identify\nthe core question(s) the user is asking. What is the\nmost important piece of information they are looking\nfor?\n2. **Assess Evidence Against Intent:** Check if the\nevidence contains the necessary facts to fulfill this\nprimary intent.\n3. **Pragmatism Rule:**\n- The evidence is **\"Sufficient\" (Yes)** if\nthe main question can be answered, even if peripheral\ndetails or deeper context is missing.\n- The evidence is **\"Insufficient\" (No)**\nonly if a **critical piece of information**, essential\nto forming the main answer, is absent.\n--- EXAMPLE ---\n**User Question:** \"What was the main outcome of the\nBattle of Badr and which year did it take place?\"\n**Evidence:**\n- \"The Battle of Badr was a decisive victory for the\nearly Muslims.\"\n- \"Key leaders of the Quraysh were defeated in the\nengagement.\"\n- \"The victory at Badr greatly strengthened the\npolitical and military position of the Islamic\ncommunity in Medina.\"\n**Your Analysis for Example:**\nThe evidence clearly confirms the \"main outcome\" (a\ndecisive victory for Muslims). However, a critical part\nof the question, \"which year did it take place?\", is\ncompletely missing from the evidence. Therefore, a\ncomplete answer cannot be formed.\n**Your Output for Example:**\n{{\n\"reasoning\": \"The evidence confirms the outcome of\nthe battle (a decisive victory) but a critical piece of\nrequested information, the year of the battle, is\ncompletely missing.\",\n\"is_sufficient\": false\n}}\n--- END OF EXAMPLE ---\nNow, apply this pragmatic logic to the following:\n[User Question]:\n\"{question}\"\n[Collected Evidence]:\n{evidence}\nProvide your final assessment in the following strict\nJSON format:\n{{\n\"reasoning\": \"<A brief analysis of what can be\nanswered and what critical information, if any, is\nstill missing.>\",\n\"is_sufficient\": <true or false>\n}}\n\"\"\",\n\"query_refinement\": \"\"\"\nYou are an expert AI systems evaluator. A RAG system\ndetermined its initial evidence was insufficient and\ngenerated new sub-queries to find missing information.\nYour task is to evaluate the quality of these new\nqueries.\n[User Question]:\n\"{question}\"\n[Insufficient Initial Evidence]:\n{evidence}\n[Newly Generated Sub-Queries for Refinement]:\n{new_queries}\nAssess how effectively the new sub-queries target the\ninformation gaps in the initial evidence to help answer\n\nthe main question.\nProvide your assessment in the following JSON format:\n{{\n\"score\": <A numeric score from 1.0 (Poorly\ntargeted) to 5.0 (Excellent, precisely targets gaps)>,\n\"reasoning\": \"<A very brief explanation for your\nscore>\"\n}}\n\"\"\",\n\"final_context_relevance\": \"\"\"\nYou are an expert information retrieval evaluator.\nYour task is to score the relevance of each document in\nthe final context that was used to generate an answer.\n[User Question]:\n\"{question}\"\n[Final Context Used for Generation\n(final_relevant_evidence)]:\n{final_evidence}\nFor each document in the final context, provide a\nrelevance score.\nProvide your assessment in the following JSON format:\n{{\n\"relevance_scores\": [\n{{ \"doc_id\": \"<_id of doc 1>\", \"score\":\n<numeric score from 1.0 (Irrelevant) to 5.0 (Highly\nRelevant)> }},\n{{ \"doc_id\": \"<_id of doc 2>\", \"score\":\n<numeric score from 1.0 (Irrelevant) to 5.0 (Highly\nRelevant)> }}\n]\n}}\n\"\"\",\n\"faithfulness\": \"\"\"\nYou are an expert in AI safety and fact-checking,\nspecializing in the evaluation of Retrieval-Augmented\nGeneration (RAG) systems. Your task is to evaluate the\nanswer's faithfulness to the provided evidence with\nnuance.\n- A faithful answer must be fully grounded in the\nprovided context. However, this does not mean it must\nbe a simple copy-paste of the text. **Valid synthesis,\nsummarization, and logical inference based *only* on\nthe provided information are considered faithful and\ndesirable.**\n- A statement is only considered **\"Unfaithful\"** if\nit introduces new, verifiable information that is\n**absent** from the context or if it **contradicts**\nthe context.\n[User Question (for context)]:\n\"{question}\"\n[Provided Context (final_relevant_evidence)]:\n{final_evidence}\n[Generated Answer]:\n\"{final_answer}\"\n**Your Task:**\n1. Analyze each claim within the [Generated Answer].\n2. For each claim, determine if it is directly\nstated, a valid synthesis/inference from the context,\nor an unfaithful statement (introducing new facts).\n3. Based on this analysis, provide an overall verdict\naccording to the rubric below.\n**Verdict Rubric:**\n- **'Fully Faithful'**: All claims in the answer are\neither directly stated in the context or are valid\nlogical conclusions/summaries derived *only* from the\ninformation present in the context.\n- **'Partially Faithful'**: The answer is mostly\nfaithful, but contains minor, non-critical claims or\ndetails that cannot be inferred from the context.\n- **'Not Faithful'**: The answer contains significant\nor central factual claims that are not supported by, or\nactively contradict, the context.\nProvide your verdict in the following strict JSON\nformat:\n{{\n\"faithfulness_verdict\": \"<One of three strings:\n'Fully Faithful','Partially Faithful', or'Not\nFaithful'>\",\n\"reasoning\": \"<If not fully faithful, specify\nwhich claims in the answer are unsupported by the\ncontext. Explain if it's an invalid inference or a\ncompletely new fact.>\"\n}}\n\"\"\",\n\"iterative_improvement\": \"\"\"\nYou are an expert AI quality evaluator. For a single\nquestion, you are given four answers generated by the\nsame system but with different levels of iterative\nrefinement (1, 2, 3, and 4 iterations). Your task is to\nrank these answers from best to worst.\n[User Question]:\n\"{question}\"\n[Answer from 1 Iteration (iter_1)]:\n\"{answer_1}\"\n[Answer from 2 Iterations (iter_2)]:\n\"{answer_2}\"\n[Answer from 3 Iterations (iter_3)]:\n\"{answer_3}\"\n[Answer from 4 Iterations (iter_4)]:\n\"{answer_4}\"\nRank these three answers from best (Rank 1) to worst\n(Rank 4).\nProvide your ranking in the following JSON format:\n{{\n\"ranking\": [\"<ID of the best answer, e.g.,\n'iter_3'>\", \"<ID of the second-best answer, e.g.,\n'iter_4'>\", \"<ID of the third-best answer, e.g.,\n'iter_2'>\",\"<ID of the worst answer, e.g.,'iter_1'>\"],\n\"reasoning\": \"<A very brief explanation for your\nranking, noting whether more iterations led to a clear\nimprovement>\"\n\n}}\n\"\"\"\n}\nD Failure Mode Analysis Prompt\nThis appendix details the prompts engineered for\nour LLM-assisted failure mode analysis, as de-\nscribed in Section 5.2.4. To ensure a systematic\nand reproducible evaluation, we designed a two-\npart prompt structure. The PROMPT_SYSTEM\nprompt establishes the LLM\u2019s persona as an \u201cex-\npert evaluation researcher\u201d and enforces a strict\nJSON output schema based on a predefined fail-\nure taxonomy. The PROMPT_USER_TEMPLATE\nthen provides the data batch and requires the model\nto ground its diagnosis in specific evidence from\nthe logs, ensuring each classification is structured,\nexplainable, and actionable.\nFAILURE_ANALYSIS_PROMPT = \"\"\"\n**ROLE:** You are an expert RAG (Retrieval-Augmented Generation)\nsystem diagnostician. Your task is to perform a meticulous root\ncause analysis on a failed query-answer pair from an advanced,\niterative RAG system.\n**CONTEXT:** The system has already produced an answer that was\ngraded as incorrect. You have been given the complete execution\ntrace for this failed sample. Your goal is to identify the single,\nprimary point of failure within the RAG pipeline.\n**FAILURE CATEGORIES:**\nYou must classify the failure into one of the following six\ncategories. Read these definitions carefully.\n1. **Query Decomposition Error:** The initial user question was\nnot broken down into effective, specific sub-queries. The\nsub-queries were irrelevant, missed key aspects of the original\nquestion, or sent the retrieval process in the wrong direction\nfrom the very beginning.\n2. **Retrieval Failure:** The retriever, despite having\nwell-formed sub-queries, failed to find and return the relevant\ndocuments from the knowledge base. The correct information was\nsimply not present in the`[All Retrieved Documents (Unfiltered)]`\nset.\n3. **Evidence Filtering Error:** The correct information WAS\nsuccessfully retrieved by the retriever, but the subsequent\nfiltering/reranking step mistakenly discarded the crucial\ndocuments. Look for correct information in`[Discarded Documents]`\nthat should have been kept.\n4. **SEA Error (Strategic Analyst Error):** The system's\n'Strategic Intelligence Analyst'module failed in its reasoning.\nThis can manifest in several ways:\n- **A) Flawed Deconstruction:** The'Required Findings'\nchecklist in its analysis was incorrect or missed the main\npoint of the user's question.\n- **B) Faulty Analysis:** The module failed to make a correct\nlogical inference from the evidence, hallucinated a'Confirmed\nFinding'that wasn't supported, or incorrectly identified the\n'Remaining Gaps'.\n- **C) Contradictory Verdict:** The detailed analysis pointed\nto missing information, but the final verdict was mistakenly\n'Sufficient: Yes', causing the system to stop searching\nprematurely.\n5. **Query Refinement Error:** After correctly identifying that\nthe initial evidence was insufficient, the system failed to\ngenerate effective new sub-queries to target the specific\ninformation gaps. The new queries were redundant, vague, or did\nnot address the missing pieces identified by the SEA module.\n6. **Generation Failure:** All preceding steps worked correctly.\nThe final set of evidence (`[Final Relevant Evidence]`) contained\nall the necessary information to form a correct answer. However,\nthe language model failed during the final synthesis step by\nhallucinating, making incorrect logical inferences, or\nmisinterpreting the provided evidence.\n**PRIMARY FAILURE RULE:**\nIdentify the **earliest, most fundamental error** in the pipeline.\nFor example, if Retrieval failed to find good documents, the\nGeneration will also fail, but the root cause is **Retrieval\nFailure**.\n**EXECUTION TRACE FOR ANALYSIS:**\n[User Question]:\n\"{question}\"\n[Ground Truth Answer (The correct answer)]:\n\"{ground_truth_answer}\"\n[Generated (Incorrect) Answer]:\n\"{final_answer}\"\n--- RAG Pipeline Details ---\n[Initial Sub-Queries Generated]:\n{sub_queries}\n[All Retrieved Documents (Unfiltered)]:\n{all_retrieved_docs}\n[Discarded Documents (By Filter)]:\n{discarded_docs}\n[Final Relevant Evidence (Used for Generation)]:\n{final_evidence}\n--- Iteration Reports ---\n{iteration_reports_formatted}\n--- YOUR TASK ---\nBased on all the provided information and adhering strictly to\nthe definitions, provide your analysis in the following JSON format.\n{\n\"failure_category\": \"<The value for this key MUST be one of the\nfollowing exact strings:'Query Decomposition Error','Retrieval\nFailure','Evidence Filtering Error','SEA Error','Query\nRefinement Error','Generation Failure'. Do NOT add any extra\ntext or explanations.>\",\n\"reasoning\": \"<Provide a concise, step-by-step justification\nfor your choice of category. Reference specific parts of the\nexecution trace (e.g.,'The SEA module's analysis incorrectly\nstated it confirmed the actor's birth year, but the evidence\nonly mentioned their nationality').>\",\n\"root_cause_analysis\": \"<Go one level deeper. Why did this\nerror likely happen? (e.g.,'The SEA prompt might be too\ncomplex, leading to reasoning errors,'or'The filtering model\nmay be poorly calibrated for short documents').>\",\n\"suggested_improvement\": \"<Propose a concrete, actionable\nsolution to fix or mitigate this specific type of error in the\nfuture. (e.g.,'Simplify the SEA prompt by removing the persona\nand focusing on a checklist,'or'Fine-tune the reranker with\nmore examples of this type').>\"\n}\n\"\"\"",
  "authors": [
    "Mohammad Aghajani Asl",
    "Majid Asgari-Bidhendi",
    "Behrooz Minaei-Bidgoli"
  ],
  "summary": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.",
  "pdf_url": "https://arxiv.org/pdf/2510.22344v1",
  "entry_id": "http://arxiv.org/abs/2510.22344v1",
  "published": "2025-10-25",
  "updated": "2025-10-25",
  "comment": "30 pages, 5 figures, 5 tables. Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Agentic AI, Multi-hop Question Answering, Faithfulness",
  "journal_ref": null,
  "doi": null,
  "primary_category": "cs.CL",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.IR"
  ],
  "links": [
    {
      "href": "https://arxiv.org/abs/2510.22344v1",
      "rel": "alternate",
      "title": null
    },
    {
      "href": "https://arxiv.org/pdf/2510.22344v1",
      "rel": "related",
      "title": "pdf"
    }
  ],
  "full_text": "FAIR-RAG: Faithful Adaptive Iterative Refinement for\nRetrieval-Augmented Generation\nMohammad Aghajani Asl\nSharif University of Technology\nm.aghajani@physics.sharif.edu\nMajid Asgari-Bidhendi\nIran University of Science and Technology\nNoor Avaran Jelvehaye Maanaei Najm Co., Ltd.\nmajid.asgari@gmail.com\nBehrooz Minaei-Bidgoli\nIran University of Science and Technology\nb_minaei@iust.ac.ir\nAbstract\nWhile Retrieval-Augmented Generation (RAG)\nmitigates hallucination and knowledge stale-\nness in Large Language Models (LLMs), exist-\ning frameworks often falter on complex, multi-\nhop queries that require synthesizing informa-\ntion from disparate sources. Current advanced\nRAG methods, employing iterative or adaptive\nstrategies, still lack a robust mechanism to sys-\ntematically identify and fill evidence gaps, of-\nten propagating noise or failing to gather a com-\nprehensive context. In this paper, we introduce\nFAIR-RAG, a novel agentic framework that\ntransforms the standard RAG pipeline into a\ndynamic, evidence-driven reasoning process.\nAt the core of FAIR-RAG is an Iterative Refine-\nment Cycle governed by a novel module we\nterm Structured Evidence Assessment (SEA).\nThe SEA acts as an analytical gating mecha-\nnism: it deconstructs the initial query into a\nchecklist of required findings and systemati-\ncally audits the aggregated evidence to iden-\ntify confirmed facts and, critically, explicit in-\nformational gaps. These identified gaps pro-\nvide a precise, actionable signal to an Adap-\ntive Query Refinement agent, which then gen-\nerates new, targeted sub-queries to retrieve the\nmissing information. This cycle repeats until\nthe evidence is verified as sufficient, ensuring\na comprehensive context for a final, strictly\nfaithful generation step. We conduct exten-\nsive experiments on challenging multi-hop QA\nbenchmarks, including HotpotQA, 2WikiMul-\ntiHopQA, and MusiQue.Under a unified and\ncontrolled experimental setup,FAIR-RAG\nsignificantly outperforms strong representative\nbaselines. On HotpotQA, it achieves an F1-\nscore of 0.453\u2014an absolute improvement of\n8.3 points over the strongest iterative baseline\u2014\nestablishing a new state-of-the-art for this\nclass of methods on these benchmarks.Our\nwork demonstrates that a structured, evidence-\ndriven refinement process with explicit gap\nanalysis is crucial for unlocking reliable and\naccurate reasoning in advanced RAG systems\nfor complex, knowledge-intensive tasks.\n1 Introduction\nLarge Language Models (LLMs) have demon-\nstrated remarkable capabilities across a wide range\nof natural language processing tasks, including\nquestion-answering (QA) (Brown et al., 2020;\nChowdhery et al., 2022). However, their knowl-\nedge is inherently static, confined to the data they\nwere trained on, which leads to factual inaccura-\ncies and an inability to reason about events beyond\ntheir training cut-off date. Furthermore, LLMs\nare prone to \u201challucination,\u201d generating plausible\nyet factually incorrect information, which severely\nlimits their reliability in knowledge-intensive ap-\nplications (Ji et al., 2023). To mitigate these is-\nsues, Retrieval-Augmented Generation (RAG) has\nemerged as a prominent paradigm. By ground-\ning the generation process on information retrieved\nfrom external knowledge bases, RAG systems aim\nto produce more accurate, timely, and verifiable\nresponses (Lewis et al., 2020).\nDespite its advantages, the standard \u201cretrieve-\nthen-read\u201d RAG pipeline often falls short when\nfaced with real-world user queries, which are fre-\nquently complex and cannot be answered through\na single-shot retrieval step. For instance, a query\nsuch as\u201cWhich movie, directed by the same per-\nson who directed Inception, won an Oscar for\narXiv:2510.22344v1  [cs.CL]  25 Oct 2025\n\nBest Cinematography?\u201drequires multi-hop rea-\nsoning (Yang et al., 2018): first identifying the\ndirector ofInception(Christopher Nolan) and then\nsearching for his movies that have won the spec-\nified award. Standard RAG frameworks struggle\nwith such multi-hop queries, as well as comparative\nor analytical questions that require synthesizing in-\nformation from multiple sources. Moreover, they\nare not robust to suboptimal user query formula-\ntions and often fail to enforce that the generated\nanswer remains strictly faithful to the retrieved evi-\ndence, thus perpetuating the risk of hallucination.\nTo address these limitations, several advanced\nRAG methodologies have been proposed.Itera-\ntive approaches, such as ITER-RETGEN (Shao\net al., 2023), refine the retrieved information over\nmultiple cycles by using the previously generated\noutput as context for the next retrieval step.Adap-\ntive approaches, like Adaptive-RAG (Jeong et al.,\n2024), aim for efficiency by dynamically selecting\nthe retrieval strategy (e.g., no retrieval, single-step,\nor multi-step) based on an initial assessment of the\nquery\u2019s complexity. Other frameworks like SELF-\nRAG (Asai et al., 2023) introduce self-reflection\nmechanisms, training the LLM to generate special\ntokens that control the retrieval and critique pro-\ncess on the fly. Nonetheless, a gap remains for a\nframework that synergistically combines iterative\nevidence refinement with adaptive query genera-\ntion and an explicit, modular faithfulness check.\nExisting iterative methods can propagate noise by\nusing entire generations as queries, while adaptive\nmethods may not sufficiently refine the evidence\nneeded for complex queries after the initial routing.\nIn this paper, we introduceFAIR-RAG, a\nnovel framework forFaithful,Adaptive,Iterative\nRefinement inRetrieval-AugmentedGeneration.\nFAIR-RAG is designed to robustly handle complex\nqueries by orchestrating a dynamic, multi-stage\nworkflow. The framework begins with anAdaptive\nRoutingmodule that analyzes query complexity to\ndetermine an optimal execution path, either by di-\nrectly answering simple queries or by allocating the\nappropriate computational resources for complex\nones. For non-trivial queries, FAIR-RAG initiates\na cyclical process designed to progressively build\nand validate a context. At the core of this cycle is\nanIterative Refinementloop where LLM agents\nintelligently decompose information needs, retrieve\nevidence, and filter out noise. Crucially, each cy-\ncle culminates in aStructured Evidence Assess-\nment (SEA)module, which acts as an analytical\ngating mechanism. This module emulates a cog-\nnitive workflow by first deconstructing the user\u2019s\nquery into a checklist of required findings. It then\nsystematically synthesizes the retrieved evidence\nagainst this checklist, verifying what is confirmed\nand, most importantly, explicitly identifying any\n\u201cRemaining Gaps.\u201dThese identified gaps provide\na precise, actionable signal for theAdaptive Query\nRefinementmodule, which then generates new, tar-\ngeted queries specifically designed to retrieve the\nmissing information. This evidence-centric loop\ncontinues until sufficiency is achieved, ensuring\nthat the finalFaithful Answer Generationstep\nis strictly grounded in a comprehensive and ver-\nified knowledge context, substantially enhancing\ntrustworthiness and reducing hallucination.\nOur main contributions are as follows:\n\u2022 We introduce a novel agentic RAG architec-\nture centered on anIterative Refinement\nloop. This evidence-centric cycle is governed\nby an analytical gating mechanism we term\nStructured Evidence Assessment (SEA). By\nsystematically deconstructing the query and\nidentifying specific information gaps, the SEA\nmodule intelligently guides subsequent itera-\ntions. This process progressively builds and\nvalidates a comprehensive context, enabling\nthe system to robustly handle complex, multi-\nfaceted, and multi-hop queries where single-\npass retrieval would fail.\n\u2022 We design a sophisticated, two-stage query\nstrategy. It begins withsemantic decomposi-\ntionto ensure all facets of the initial query are\naddressed, and more importantly, incorporates\nanAdaptive Query Refinementmechanism\nthat analyzes evidence gaps to intelligently\ngenerate new queries, effectively reasoning\nabout what information is still missing.\n\u2022 We implement an integrated approach to re-\nsource optimization throughdynamic re-\nsource allocation. Our framework employs\nan initialAdaptive Routingmechanism to\nbypass the RAG pipeline for simple queries\nand dynamically assigns LLMs of varying\nsizes to internal tasks based on their com-\nplexity, achieving a superior balance between\nresponse quality, latency, and computational\ncost.\n\n\u2022 We propose a robust, two-pronged approach\nto guarantee faithfulness. This includes: (1) a\npre-generationStructured Evidence Assess-\nment (SEA), which performs a final analyt-\nical pass to verify that all required findings\nfrom the initial query deconstruction are fully\nsupported by the aggregated evidence, and (2)\naconstrained generation promptthat en-\nforces citation and prevents the model from\nintroducing external knowledge. This combi-\nnation ensures the final answer is both verifi-\nable and trustworthy.\nWe conducted extensive experiments on a suite\nof four challenging open-domain QA bench-\nmarks to evaluate the FAIR-RAG framework, en-\ncompassing complex multi-hop reasoning tasks\n(HotpotQA(Yang et al., 2018),2WikiMulti-\nHopQA,Musique) and a large-scale single-hop\nfactual dataset (TriviaQA). Our best-performing\nconfiguration,FAIR-RAG 3 (Adaptive LLMs),\nwas benchmarked against a wide range of strong\nbaselines, including sequential (Standard RAG),\nconditional (Adaptive-RAG), and other state-of-\nthe-art iterative methods (Iter-Retgen, Self-RAG).\nThe results clearly demonstrate the superiority of\nour approach. On theHotpotQAbenchmark, our\nmodel sets a new state-of-the-art with an F1-score\nof0.453, surpassing the strongest iterative baseline,\nIter-Retgen (0.370), by a significant margin of8.3\npoints. This pattern of superior performance is\nconsistent across other complex benchmarks: on\n2WikiMultiHopQA, FAIR-RAG achieves an F1\nof0.320, outperforming the next best method,Self-\nRAG (0.251), by6.9 points, and onMusique, it\nscores an F1 of0.264, which is7.4 pointshigher\nthanIter-Retgen (0.190).\nNotably, FAIR-RAG\u2019s architecture also excels\non simpler factual queries, achieving a state-of-the-\nart F1 score of0.731onTriviaQA, showcasing\nits versatility. Our analysis further validates FAIR-\nRAG\u2019s core principles: performance consistently\nimproves as the number of refinement iterations\nincreases from one to three (e.g., F1 on HotpotQA\nimproves from 0.398 for FAIR-RAG 1 to 0.447 for\nFAIR-RAG 3), confirming the value of the iterative\nevidence-gathering loop. Furthermore, the adap-\ntive LLM allocation strategy provides an additional\nperformance boost across all metrics. Finally, our\nframework consistently achieves the highest scores\non the semantic metricACC LLM (e.g.,0.847on\nTriviaQA), confirming that its improvements reflect\na deeper contextual understanding, not just lexical\noverlap. This robust performance validates the ef-\nfectiveness of our iterative, evidence-driven frame-\nwork in enhancing both the accuracy and faithful-\nness of LLM-based QA systems.\n2 Related Work\nThe paradigm of Retrieval-Augmented Genera-\ntion (RAG) has rapidly evolved from a straight-\nforward retrieve-then-read pipeline to more sophis-\nticated, dynamic frameworks. Our work, FAIR-\nRAG, builds upon and extends several key research\nthreads in this domain.\n2.1 Standard Retrieval-Augmented\nGeneration\nThe foundational concept of RAG is to enhance\nthe capabilities of LLMs by grounding them in\nexternal, non-parametric knowledge bases (Lewis\net al., 2020). Early and influential RAG models\ntypically employ a two-stage process: a retriever\nfirst fetches a set of relevant documents from a cor-\npus based on the input query, and then a reader\n(or generator), which is often an LLM, synthe-\nsizes the final response conditioned on both the\noriginal query and the retrieved documents. This\napproach has proven effective in mitigating fac-\ntual inconsistencies (hallucinations) and providing\nanswers based on up-to-date information, thereby\novercoming the static knowledge limitations of\nLLMs (Huang et al., 2025; Guu et al., 2020). How-\never, this single-shot retrieval mechanism is primar-\nily designed for single-hop queries where the an-\nswer can be found within a small set of initially re-\ntrieved documents. Consequently, its performance\ndegrades significantly on complex benchmarks like\nHotpotQA (Yang et al., 2018), which require multi-\nstep reasoning or evidence aggregation from dis-\nparate sources.\n2.2 Iterative and Multi-Step RAG\nTo address the shortcomings of standard RAG, a\nsignificant body of research has focused on iterative\nand multi-step approaches. These methods trans-\nform the single-shot process into a dynamic, multi-\nturn interaction. One prominent strategy involves\ndecomposing a complex question into simpler sub-\nqueries. While frameworks likeSelf-Ask(Press\net al., 2023) andSuRe(Kim et al., 2024) pioneer\nthis decompositional approach, their strategies dif-\nfer fundamentally from ours.SuRegenerates a\n\ncomplete reasoning skeletonupfront\u2014a static plan\nthat cannot adapt to the evidence retrieved in inter-\nmediate steps.Self-Askgenerates sub-queries se-\nquentially, but each new query is largely informed\nby theintermediate answerto the previous one.In\ncontrast, FAIR-RAG\u2019s decomposition is neither\nstatic nor solely dependent on prior answers; it\nis a dynamic and context-aware process.After\neach iteration, our framework uses the Structured\nEvidence Assessment (SEA) module to perform\na holistic analysis of theentirety of the retrieved\nevidence corpus. New sub-queries are then gener-\nated specifically to target theexplicitly identified\ninformational gaps. This gap-driven approach al-\nlows FAIR-RAG to adapt its evidence-gathering\nstrategy in response to what has been found (and\nwhat is still missing), leading to a more robust and\nfocused multi-step reasoning process than methods\nreliant on static plans or sequential, answer-driven\nprompting.\nAnother popular approach interleaves reasoning\nsteps with retrieval actions. This paradigm is ex-\nemplified by methods like ReAct (Yao et al., 2022)\nand IRCoT (Trivedi et al., 2023), which guide an\nLLM to generate explicit reasoning traces (e.g.,\nChain-of-Thought (Wei et al., 2022)), where each\nstep can trigger a retrieval action to gather neces-\nsary information. The key distinction lies in the\nscope and triggerfor retrieval. In methods like\nReAct and IRCoT, retrieval is typically alocal,\nstep-wise actiontriggered by the immediate need\nof the next thought in a chain. In contrast, FAIR-\nRAG\u2019s retrieval is driven by aholistic assessment\nof the entire evidence pool against the overarch-\ning query requirements, allowing it to identify and\naddress complex, non-sequential information gaps\nthat step-wise reasoning might overlook.\nMore directly related to our work, frameworks\nlike ITER-RETGEN (Shao et al., 2023) propose a\nsynergistic loop where the entire generated output\nfrom one iteration serves as the context to retrieve\nmore relevant knowledge for the next. While pow-\nerful, using unstructured generation as a query can\nbe suboptimal, as it may contain noise that misdi-\nrects the retriever. FAIR-RAG distinguishes itself\nby employing a more controlled mechanism: in-\nstead of using the entire previous output, it gener-\nates new, targeted sub-queries based on an explicit\nanalysis ofinformational gaps, leading to a more\nfocused and efficient evidence-gathering process.\nOther iterative frameworks have introduced\nforward-looking or corrective mechanisms to\nenhance retrieval precision. For instance,\nFLARE (Jiang et al., 2023) employs a proactive\nstrategy where the LLM anticipates future informa-\ntion needs during generation and triggers retrievals\naccordingly, effectively interleaving prediction and\nlookup steps. Similarly, Corrective RAG (Yan\net al., 2024) incorporates a post-retrieval correc-\ntion phase, using an evaluator to assess and refine\nretrieved documents based on their relevance and\nfactual consistency. While these methods improve\nupon standard iterative loops by adding predictive\nor corrective elements, they often rely on heuristic\ntriggers or post-hoc adjustments, which can still\noverlook systematic evidence gaps in highly com-\nplex queries. In contrast, FAIR-RAG\u2019s Structured\nEvidence Assessment (SEA) module provides a\nmore principled, checklist-driven analysis that ex-\nplicitly identifies and targets informational defi-\nciencies, enabling a targeted and iterative refine-\nment without dependence on generation-time pre-\ndictions.\n2.3 Adaptive and Faithfulness-Aware RAG\nA third stream of research focuses on making RAG\nsystems more adaptive and reliable. Adaptivity is\noften geared towards computational efficiency. For\ninstance,Adaptive-RAG(Jeong et al., 2024) intro-\nduces a classifier to pre-assess query complexity\nand route it to an appropriate strategy: no retrieval\nfor simple questions, single-step retrieval for mod-\nerate ones, or a multi-step approach for complex\nqueries. This routing is performed once at the be-\nginning. In contrast, FAIR-RAG\u2019s adaptivity is\ndynamic and occurswithinthe iterative process, as\nit continually adapts its query generation strategy\nbased on the evolving set of retrieved evidence.\nEnhancing the faithfulness of the generated out-\nput is another critical concern. Standard RAG mod-\nels do not explicitly guarantee that the generator\nwill adhere to the retrieved context. (Es et al., 2025)\nTo address this, SELF-RAG (Asai et al., 2023) fine-\ntunes an LLM to generate special \u201creflection to-\nkens\u201d, enabling it to critique its own output for\nrelevance and factual support in an inline fashion\nduring generation. While effective, this approach\nhas two limitations: its reliance on fine-tuning re-\nstricts applicability to off-the-shelf models, and\nits evaluation is inherentlytactical, assessing evi-\ndence on a step-by-step basis as the answer is being\ncomposed.\n\nFAIR-RAG addresses the faithfulness challenge\ndifferently, through an explicit, modular, and more\nstrategicStructured Evidence Assessment (SEA)\nmodule. Instead of an inline critique, SEA acts\nas a distinct analytical gating mechanismbefore\nfinal answer generation. It first deconstructs the\nuser\u2019s query into a checklist of required findings.\nIt then performs a holistic audit of theentireevi-\ndence corpus against this question-centric check-\nlist to identify confirmed facts and explicit \u201cintel-\nligence gaps.\u201d This distinction is crucial: whereas\nSELF-RAG prompts the model to ask, \u201cIs this next\npiece of evidence useful for my current generation\nstep?\u201d, SEA forces the model to first ask, \u201cIs the\nentiretyof my evidence sufficient to address all\nfacets of the user\u2019s original query?\u201d The identi-\nfied gaps from this strategic assessment provide a\nprecise, actionable signal for the subsequent query\nrefinement step, transforming the check from a pas-\nsive validation into an active steering mechanism.\nCrucially, unlike fine-tuning-based methods, our\nmodular SEA requires no model training, allowing\nfor greater flexibility and easier integration with\nvarious off-the-shelf language models.\nIn summary, while existing works have made\nsubstantial advancements, FAIR-RAG provides a\nnovel contribution by synergistically integrating\nthree core principles into a cohesive framework: (1)\na structured, gap-aware iterative refinement loop,\n(2) context-aware, adaptive sub-query generation,\nand (3) an explicit, modular faithfulness assessment\nthat requires no model fine-tuning.\n3 Methodology\nThe FAIR-RAG framework is designed as a multi-\nstage, iterative process that dynamically adapts its\nstrategy to the complexity of a user\u2019s query. Our\narchitecture transforms the standard, static RAG\npipeline into an intelligent, evidence-driven work-\nflow that progressively builds context to answer\ncomplex questions. The entire process, illustrated\nin Figure 1, can be divided into four main phases:\n(1) Initial Query Analysis and Adaptive Routing,\n(2) The Iterative Retrieval and Refinement Cycle,\n(3) Faithful Answer Generation, and (4) Dynamic\nResource Allocation. The entire inference proce-\ndure is detailed step-by-step in Algorithm 1. The\nprocess is initiated by theAdaptive Routingagent\n(Arouter), which classifies the query and selects the\nmost appropriate generator LLM ( Gselected) from\nthe predefined set of available models (G).\n3.1 Overall Architecture\nThe overall architecture of FAIR-RAG, illustrated\nin Figure 1, employs a dynamic, multi-step pro-\ncess to handle user queries. An initial routing step\nassesses query complexity. Simple queries are an-\nswered directly, while complex ones trigger an it-\nerative refinement loop. This core loop consists of\nadaptive query generation, hybrid retrieval, filter-\ning, and a Structured Evidence Assessment (SEA).\nThe SEA module determines if the collected evi-\ndence is sufficient. If not, the loop repeats with\nrefined queries targeting information gaps. Once\nsufficiency is met, a final, evidence-grounded an-\nswer is generated. The entire inference procedure\nis detailed step-by-step in Algorithm 1. All LLM\nagents in our pipeline are guided by meticulously\nengineered prompts. The full details and examples\nfor each prompt are provided in Appendix B.\n3.2 Initial Query Analysis and Adaptive\nRouting\nThe first stage of our framework is a lightweight yet\ncrucial analysis of the input query. An LLM agent\nclassifies the query into one of four categories to\ndetermine the subsequent workflow and resource\nallocation:\n\u2022 OBVIOUS:For queries whose answers are\nlikely stable and contained within the LLM\u2019s\nparametric knowledge (e.g., \u201cWhat is the cap-\nital of France?\u201d). These queries are routed\ndirectly to a large LLM for generation, by-\npassing the entire RAG pipeline for maximum\nefficiency.\n\u2022 SMALL:Simple factual queries that require\nretrieval but minimal reasoning. A smaller,\nmore efficient LLM is designated for the final\ngeneration step.\n\u2022 LARGE:Queries that require retrieval\nand synthesis of information from multiple\nsources. A larger, more capable LLM is se-\nlected for generation.\n\u2022 REASONING:Complex queries that demand\nmulti-hop reasoning, comparison, or deep\nanalysis. A state-of-the-art LLM with strong\nreasoning capabilities is allocated for the final\nanswer.\nThis initial routing mechanism serves a dual\npurpose: it acts as an efficiency-enhancing short-\n\nAlgorithm 1FAIR-RAG Inference (Prompts for agents are detailed in Appendix B.)\nRequire: Set of Generator LLMs G={G small, Glarge, Greasoning}, Hybrid Retriever Hretriever, Document Corpus C=\n{d1, . . . , dN}, Set of LLM AgentsA={A router, Adecompose, Afilter, Asuff, Arefine}\n1:Input:User queryx\n2:Output:Final, evidence-grounded answery final\n3: query_type, Gselected \u2190A router.classify(x)\u25b7Route\n4:ifquery_type==OBVIOUSthen\n5:y final \u2190G selected.generate(x)\u25b7Generate directly from parametric knowledge\n6:returny final\n7:end if\n8:E agg \u2190 \u2205 // Aggregated evidence\n9:Q previous \u2190 {x}\n10:fori\u21901to3do // Max 3 iterations\n11:ifi== 1then\n12:Q sub \u2190A decompose.generate(x)\u25b7Decompose initial query\n13:else\n14:Q sub \u2190A refine.generate(x, Qprevious,analysis_summary)\u25b7Refine query to fill gaps\n15:end if\n16:Q previous \u2190Q previous \u222aQ sub\n17:D candidate \u2190H retriever.retrieve_and_rerank(Qsub,C)\u25b7Retrieve\n18:E filtered \u2190A filter.filter(Dcandidate, x)\u25b7Filter irrelevant evidence\n19:E agg \u2190E agg \u222aE filtered\n20: analysis_summary,is_sufficient\u2190A suff.check(Eagg, x)\u25b7Assess sufficiency with SEA\n21:ifis_sufficient==Yesthen\n22:break// Exit loop if evidence is sufficient\n23:end if\n24:end for\n25:y final \u2190G selected.generate(x, Eagg,faith_constraints)\u25b7Generate answer\n26:returny final\ncut for simple queries and implements ourAdap-\ntive Model Selectionstrategy by pre-allocating the\nmost cost-effective generator model for the final\nstep. While this module is a key feature for prac-\ntical deployment, it was systematically controlled\nduring our benchmark evaluations to ensure a fair\ncomparison, as detailed in Section 4.4.\n3.3 The Iterative Retrieval and Refinement\nCycle\nThis cycle is the core of FAIR-RAG\u2019s ability to\nhandle complex information needs. It is designed\nto run for a maximum of three iterations to ensure\na balance between comprehensiveness and latency.\nEach iteration consists of the following steps:\n3.3.1 Adaptive Query Generation\nThe initial user query, if deemed complex, is first\nsubjected to semantic decomposition. An LLM\nagent breaks down the multifaceted query into a\nset of up to four distinct, keyword-rich, and seman-\ntically independent sub-queries. For example, the\nquery\u201cWhat were Alan Turing\u2019s main contribu-\ntions to computer science and his role in World\nWar II?\u201dis decomposed into targeted sub-queries\nlike\u201cAlan Turing\u2019s contributions to theoretical\ncomputer science\u201dand\u201cAlan Turing\u2019s role in\nbreaking the Enigma code.\u201dThis ensures that the\nretrieval process covers all conceptual facets of the\noriginal question.\n3.3.2 Hybrid Retrieval and Reranking\nFor each sub-query, we employ a hybrid retrieval\nstrategy to maximize recall. We perform both\ndense vector search (capturing semantic simi-\nlarity) (Karpukhin et al., 2020) and traditional\nkeyword-based sparse search (capturing exact\nmatches) (Robertson and Zaragoza, 2009). The\nresults from both methods are aggregated, and\nthe documents are re-ranked using theRecipro-\ncal Rank Fusion (RRF)algorithm (Cormack et al.,\n2009). RRF effectively combines the rankings from\nboth retrieval methods without requiring hyperpa-\nrameter tuning, producing a single, robustly ranked\nlist of the top-5 most relevant documents as candi-\ndate evidence.\n3.3.3 Evidence Filtering\nThe candidate evidence is then passed to a filtering\nmodule. An LLM agent evaluates each document\u2019s\nutility with respect to theoriginaluser query. Doc-\numents that are irrelevant, off-topic, or only tangen-\ntially related are discarded (Liu et al., 2023a). This\nstep is critical for increasing the signal-to-noise\nratio of the context provided to the final genera-\ntor, preventing the model from being distracted by\n\nFigure 1: Schematic overview of the FAIR-RAG architecture. The process starts with initial query analysis and\nadaptive language model selection (e.g., small, large, or reasoning LLM). For complex queries, it proceeds to query\ndecomposition, followed by an iterative refinement cycle involving hybrid retrieval and reranking, evidence filtering,\nand Structured Evidence Assessment (SEA). The loop iterates until evidence sufficiency is confirmed, culminating\nin faithful answer generation grounded in the aggregated evidence.\nnoisy or unhelpful information.\n3.3.4 Structured Evidence Assessment (SEA):\nThe Strategic Intelligence Analyst\nThe strategic core of our iterative refinement loop\nis the Structured Evidence Assessment (SEA) mod-\nule. Its primary function is not merely to verify\nevidence, but to perform a granulargap analysis\nthat generates an explicit, actionable signal for the\nnext iteration. For this critical task, we deliber-\nately chose achecklist-based methodologyover\nalternatives like abstractive summarization or di-\nrect question-answering. Abstractive summariza-\ntion, by design, often conceals the very gaps we\nneed to identify by creating a fluent narrative, while\ndirect QA yields binary outcomes that fail to pin-\npoint the precise location of missing information\nin a multi-fact query.\nOur approach operationalizes this checklist via\nan LLM agent prompted to act as aStrategic In-\ntelligence Analyst. Through carefully engineered\nprompts\u2014defining its role and providing few-shot\nexamples (see Appendix B)\u2014the agent firstde-\nconstructsthe user\u2019s query into a checklist of dis-\ncrete, required informational components or \u201cfind-\nings.\u201d It then systematicallyauditsthe collected\nevidence against this checklist, confirming which\nfindings are supported and identifying which re-\nmain as explicit\u201cintelligence gaps.\u201dThe evidence\nis deemed sufficient only if all required findings\nare confirmed.\nThe \u2018unchecked\u2019 items on this list constitute adi-\nrect, interpretable, and actionable signalfor the\nsubsequent Query Refinement module. This struc-\ntured, question-centric process provides a more\ncontrollable, reliable, and transparent mecha-\nnismfor gap analysis than less constrained meth-\nods, ensuring a rigorous evaluation that prevents\nthe system from being misled by large volumes of\ntangentially related information.\n\n3.3.5 Iterative Query Refinement\nIf the Structured Evidence Assessment (SEA) re-\nsults that the evidence is insufficient to correctly an-\nswer the question, the system activates the query re-\nfinement module, which is designed to be a highly\ntargeted intervention. An LLM agent uses the \u201cRe-\nmaining Gaps\u201d and \u201cConfirmed Findings\u201d from\nthe analyst\u2019s summary (generated in the previous\nstep) as its primary input. Its goal is to generate\nnew, laser-focused queries that are engineered to\nfindonlythe missing pieces of information. By\nleveraging the confirmed findings, the agent makes\nthe new queries more precise and avoids repeating\nprevious searches. For instance, if a query is\u201cIn\nwhat city was the lead scientist who broke the\nEnigma code buried?\u201dand the analyst\u2019s summary\nconfirms\u201cAlan Turing was the lead scientist\u201dbut\nidentifies\u201cAlan Turing\u2019s burial place\u201das a gap,\nthe refinement module will generate new, highly\ntargeted queries like\u201cAlan Turing burial place\u201d\nor\u201ccity where Alan Turing is buried.\u201dThese new\nqueries, now contextualized and specific, re-enter\nthe hybrid retrieval cycle (Step 3.3.2).\n3.4 Faithful Answer Generation\nOnce the Structured Evidence Assessment (SEA)\nreturns \u201cYes,\u201d the curated and validated evidence\nset is passed to the generator LLM selected in the\ninitial routing stage (Step 3.2). The generation\nprocess is governed by a meticulously engineered\nprompt that enforces astrict, evidence-only gen-\neration protocol. This constrained prompt is de-\nsigned to maximize faithfulness and minimize the\nrisk of hallucination by instructing the LLM to:\n\u2022 Base its answerexclusivelyon the provided\nevidence.\n\u2022 Avoid introducing any external information,\nparametric knowledge, or opinions.\n\u2022 Cite every claim by embedding reference to-\nkens (e.g., [1], [2]) that link to the source doc-\numents.\n\u2022 If the evidence is ultimately insufficient, state\nthis directly without attempting to speculate.\nThis highly constrained generation process is\nfundamental to mitigating hallucination and en-\nsures that the final output is not only accurate but\nalso transparent and fully traceable to its sources.\n3.5 Dynamic Resource Allocation and Prompt\nEngineering\nA key aspect of FAIR-RAG\u2019s design is the efficient\nuse of computational resources. We dynamically\nallocate LLMs of different sizes (e.g., SMALL,\nLARGE) for the various internal tasks based on\ntheir complexity. For instance, simpler tasks like\ntheInitial Query Analysis and Adaptive Routing\nare handled by a smaller model, while the highly\nnuanced task ofquery refinementis assigned to a\nmore capable model to ensure maximum precision.\nThis dynamic allocation optimizes the trade-off\nbetween performance, cost, and latency.\nFurthermore, all interactions with LLM agents\nare managed through astructured prompt engi-\nneering methodologydesigned to ensure reliabil-\nity and predictability [see Appendix B for details].\nOur prompts are consistently engineered to include\nseveral key components that guide the model\u2019s be-\nhavior:\n\u2022 Role and Context Definition:Each prompt\nbegins by establishing a clear role for the\nagent and the context of its task (e.g.,\u201cYou\nare a Strategic Intelligence Analyst. . . \u201d).\n\u2022 Task Specification:The primary goal or in-\ntent of the task is explicitly stated (e.g.,\u201cYour\nmission is to determine if the provided evi-\ndence is sufficient. . . \u201d).\n\u2022 Guided Reasoning (Scaffolding):We pro-\nvide clear, step-by-step instructions, analytical\nguidelines, or few-shot examples to structure\nthe model\u2019s reasoning process and ensure con-\nsistency.\n\u2022 Behavioral Constraints:Explicit rules are\nlaid out that the model must follow, govern-\ning its process and output (e.g.,\u201cYou MUST\nfollow this thinking process and output format\nexactly\u201d).\n\u2022 Output Formatting:The exact format for\nthe response is strictly defined to ensure\na machine-parseable and predictable output\n(e.g.,\u201cA single word: \u2018Yes\u2019 or \u2018No\u201d\u2019).\nThis robust, component-based prompting strat-\negy ensures that the LLM agents perform their des-\nignated roles reliably, contributing to the overall\nstability and performance of the framework (Liu\net al., 2021).\n\n4 Experimental Setup\nTo rigorously evaluate the performance of FAIR-\nRAG, we conduct a series of experiments on chal-\nlenging question-answering benchmarks. Our ex-\nperimental protocol is built upon the standardized\nand open-sourceFlashRAGtoolkit, ensuring a fair\nand reproducible comparison against existing meth-\nods.\n4.1 Datasets\nWe selected a carefully curated suite of four bench-\nmark datasets to assess the various capabilities of\nour framework, with a particular focus on complex,\nmulti-hop reasoning where standard RAG systems\noften underperform.\n\u2022 Multi-hop QA:We useHotpotQA,2Wiki-\nMultihopQA, andMusiQue. These datasets\nare specifically designed to require reason-\ning and synthesizing information across multi-\nple documents to arrive at an answer, making\nthem ideal for evaluating our iterative refine-\nment and adaptive query generation modules.\n\u2022 Open-Domain QA:We also useTriviaQA, a\npopular dataset for open-domain question an-\nswering, to ensure our model maintains strong\nperformance on simpler, fact-based queries.\nFor all experiments, we follow the standard prac-\ntice of using the official test split where available;\notherwise, we report results on the development\nsplit. Consistent with recent studies on computa-\ntionally intensive RAG models, and to manage the\nsubstantial API and computational costs associated\nwith iterative inference frameworks like ours, all\nevaluations are conducted on a randomly selected\nsubset of 1000 samples from each dataset.This\nsample size was deliberately chosen to strike a\ncritical balance between statistical robustness\nand experimental feasibility.It is large enough\nto ensure stable and meaningful performance com-\nparisons while enabling the comprehensive suite\nof ablation studies and baseline comparisons pre-\nsented in this work, which would be financially and\nlogistically intractable on the full datasets.\n4.2 Baselines\nWe compare FAIR-RAG against a comprehensive\nset of representative RAG baselines implemented\nwithin the FlashRAG framework. These baselines\ncover different architectural paradigms:\nDataset Task Type Source Samples\nHotpotQA multi-hop QA wiki 1000\n2WikiMultiHopQA multi-hop QA wiki 1000\nMusiQue multi-hop QA wiki 1000\nTriviaQA Open-Domain wiki/web 1000\nTable 1: Summary of Datasets\n\u2022 Standard RAG(Lewis et al., 2020): A con-\nventional retrieve-then-read pipeline serving\nas a fundamental baseline.\n\u2022 Iterative Methods:We includeIter-Retgen\n(ITRG)(Shao et al., 2023), which uses the\nprevious generation\u2019s output to retrieve new\ndocuments, andIRCoT(Trivedi et al., 2023),\nwhich integrates retrieval within a Chain-of-\nThought process.\n\u2022 Reasoning-based Methods:We compare\nagainstReAct(Yao et al., 2022), a popular\nagent-based framework that interleaves rea-\nsoning and action steps to solve problems.\n\u2022 Faithfulness-focused Methods: Self-\nRAG(Asai et al., 2023) is included as a\nstrong baseline that incorporates explicit\nreflection and self-critique steps to improve\nfaithfulness.\n\u2022Branching & Conditional Methods:We in-\ncludeSuRe(Kim et al., 2024), which gen-\nerates and ranks multiple candidate answers,\nandAdaptive-RAG(Jeong et al., 2024),\nwhich uses a classifier to conditionally route\nqueries through different execution paths.\n4.3 Evaluation Metrics\nTo provide a comprehensive assessment of our sys-\ntem\u2019s performance, we employ a suite of metrics\nthat capture both lexical accuracy and semantic\ncorrectness.\n4.3.1 Lexical-Based Metrics\nFollowing standard practice for question-answering\ntasks, we first report two traditional, token-based\nmetrics:\n\u2022 Exact Match (EM):This strict metric mea-\nsures the percentage of predictions that match\none of the ground-truth answers exactly, char-\nacter for character.\n\u2022 F1 Score:A more lenient metric that com-\nputes the harmonic mean of precision and re-\ncall at the token level. It accounts for partial\n\noverlaps and is less sensitive to minor phras-\ning differences than EM.\n4.3.2 Automated Evaluation using\nLLM-as-Judge\nLexical metrics like EM and F1 are often insuf-\nficient for evaluating generative models, as they\nunfairly penalize semantically correct answers that\nare phrased differently or contain additional, rele-\nvant context not present in the ground truth. To\novercome this limitation, we employ LLM-as-\nJudge methodologies for two distinct, nuanced eval-\nuation purposes, strategically selecting the judge\nmodel based on task complexity.\n\u2022 End-to-End Semantic Correctness\n(ACCLLM):For the large-scale evaluation\nof our main results (Table 2), we introduce\nLLM-as-Judge Accuracy (ACCLLM). This\nmetric requires a scalable and consistent\nbinary judgment on whether a final prediction\nis semantically equivalent to any ground-truth\nanswer. For this task, we use the highly\ncapable\u201cMeta-Llama-3-8B-Instruct\u201d\nmodel as the judge. (AI@Meta, 2024) The\nspecific prompt, designed for efficient \u201cYes\u201d\nor \u201cNo\u201d classification, is detailed in Appendix\nC.1 (Chiang et al., 2023; Zheng et al., 2023).\n\u2022 Component-Level Quality Score:Given\nthe nuanced, generative nature of interme-\ndiate outputs in our ablation study (e.g.,\nQuery Decomposition), a simple binary met-\nric is insufficient. Therefore, we employ an\nLLM-as-Judge methodology (Zheng et al.,\n2023), building on established frameworks\nlike G-Eval (Liu et al., 2023b) to ensure\na scalable and consistent assessment. For\nthis role, we selected Llama-4-Maverick-17B-\n128E-Instruct-FP8. (AI@Meta, 2025) This\nhighly capable model was specifically cho-\nsen not merely for its general performance,\nbut for its demonstrated aptitude in complex\nreasoning and nuanced instruction following.\nThese capabilities are critical for accurately\nassessing the quality of our internal compo-\nnents, where evaluation criteria are intricate\nand context-dependent. The reliability of this\nmodel as a proxy for human judgment in our\nspecific tasks is not an unsubstantiated claim;\nas we will demonstrate in our validation study\n(Section 5.2.1), our LLM-as-Judge\u2019s ratings\nshow a strong correlation with those of hu-\nman experts, confirming its suitability for this\nevaluation. To ensure high-quality, structured\nfeedback and mitigate potential biases, we de-\nsigned custom prompts for each component.\nThese prompts provide the judge with clear\ntask definitions, explicit scoring criteria on a\n1-to-5 scale, and illustrative examples. This\nrigorous, prompt-driven approach, detailed\nin Appendix C.2, ensures consistent and in-\nterpretable scores for a credible and multi-\nfaceted analysis of our pipeline\u2019s internal me-\nchanics.\n4.3.3 Reliability of LLM-as-Judge\nEvaluations\nTo establish the credibility of our dual LLM-as-\nJudge framework, we conducted two separate hu-\nman verification studies, one for each evaluation\ntype.\n\u2022 Verification of Binary Semantic Correct-\nness (ACCLLM):The reliability of the Llama-\n3-8B-Instruct judge, used for the ACC LLM\nmetric, was rigorously validated. A random\nsubset of300question-answer pairs was sam-\npled, stratified across all datasets. A human\nexpert,blinded to the LLM\u2019s original deci-\nsion to prevent bias, annotated these pairs for\nsemantic correctness. The human judgments\nshowed astrong degree of concordancewith\nthe LLM-as-Judge\u2019s binary \u201cYes/No\u201d outputs,\naligning in90%of the cases. This level of\nagreement is well within the range of typical\nhuman inter-annotator agreement for such a\nnuanced task. Therefore, we conclude that\nthe LLM-as-Judge serves as a reliable and\nscalable proxy for human evaluation in our\nexperiments.\n\u2022 Verification of Component-Level Quality\nScores:For the more nuanced 1-to-5 scale\nscoring performed by the powerful Llama-4-\nMaverick model in our ablation study, a simi-\nlar validation was conducted. A separate ran-\ndom subset of 100 generated outputs from\nthe component-level tasks (e.g., query refine-\nment) was evaluated by a human expert, again\nblinded to the LLM\u2019s score. The evaluation\ndemonstrated a95%agreement between hu-\nman and LLM judgments across these differ-\nent tasks. This confirms the judge\u2019s capability\n\nfor providing consistent, human-aligned qual-\nity assessments.\nThis dual validation confirms that our LLM-as-\nJudge methodologies, for both binary correctness\nand fine-grained quality scoring, serve as robust\nand reliable proxies for human evaluation, enabling\na credible and scalable assessment of our frame-\nwork\u2019s performance.\n4.4 Implementation Details\nTo ensure a controlled and fair comparison, all ex-\nperiments adhere to the global settings defined by\nthe FlashRAG framework, with specific adapta-\ntions for our models.\nMethodological Alignment for Fair Compari-\nson:A cornerstone of our evaluation is the prin-\nciple of fair comparison, ensuring that reported\nperformance gains are attributable to our core ar-\nchitectural innovations (i.e., the iterative refinement\ncycle) rather than peripheral components. Our full\nproposed architecture includes features designed\nfor optimal real-world efficiency, such as a hybrid\nretriever (Section 3.3.2) and an \u201cOBVIOUS\u201d query\nshortcut (Section 3.2). However, as these features\nare not standard in the baseline methods we com-\npare against, they could introduce an unfair advan-\ntage.\nTherefore, to create a level playing field,these\ntwo capabilities were systematically disabled\nduring all benchmark experiments for all meth-\nods, including our FAIR-RAG variants.Specifi-\ncally:\n\u2022 All models used a standardizeddense-only\nretriever.\n\u2022 The \u201cOBVIOUS\u201d query routing was deacti-\nvated, forcingevery query to pass through\nthe full RAG pipeline.\nThis rigorous alignment ensures that the ob-\nserved performance differences are a direct result\nof the models\u2019 reasoning and evidence-handling\ncapabilities.\nComponent Configuration:\n\u2022 Retriever:For all methods, we usee5-base-\nv2(Wang et al., 2024) as the sole dense re-\ntriever, configured to fetch the top 5 doc-\numents per query from the standard DPR\nWikipedia (Dec. 2018) (Karpukhin et al.,\n2020) corpus. The index is built using Faiss\n(Flat type) (Johnson et al., 2019) to ensure\naccuracy.\n\u2022 Generator Models:To benchmark against\na powerful and widely accessible model, we\nstandardize the generator for all baseline meth-\nods to be \u201cLlama-3-8B-Instruct\u201d, accessed\nvia API. A key exception is Self-RAG, for\nwhich we use its officially released, fine-tuned\nselfrag-llama-7b model to respect its original\ndesign.\n\u2022 FAIR-RAG Configuration:Our experimen-\ntal setup for FAIR-RAG is designed to ensure\na rigorous and fair comparison. We evaluate\ntwo primary configurations of our framework:\n\u2013 Uniform Model Configuration:To iso-\nlate the architectural benefits of our iter-\native approach, the FAIR-RAG 1-4 vari-\nants exclusively use Llama-3-8B-Instruct\nfor all internal tasks and for the final an-\nswer generation. This ensures a direct\nand fair comparison against all baseline\nmethods, which are also benchmarked us-\ning the same Llama-3-8B-Instruct model.\nIn this configuration, the adaptive LLM\nselection (SMALL, LARGE, REASON-\nING roles) is intentionally disabled, with\nall roles defaulting to the single model.\n\u2013 Adaptive LLM Configuration:To\ndemonstrate the full potential of our\nframework, we also report results for\nFAIR-RAG (Adaptive LLMs). This con-\nfiguration employs a dynamic, multi-\nagent allocation strategy to optimize the\ntrade-off between performance and cost:\n* For less complex internal tasks, such\nas query decomposition and Struc-\ntured Evidence Assessment (SEA),\nwe utilize Llama-3-8B-Instruct.\n* For more cognitively demanding\ntasks, including evidence filter-\ning, query refinement, and faith-\nful answer generation, we leverage\nthe more powerful Llama-3.1-70B-\nInstruct. (AI@Meta, 2024)\n* For tasks requiring deep reasoning,\nthe system routes to a specialized\nDeepSeek-R1 model. (DeepSeek-AI,\n2025)\n\nUnless otherwise specified, all other hyperpa-\nrameters adhere to the default settings of the un-\nderlying framework to maintain consistency across\nexperiments.\n5 Results\nThis section presents a comprehensive evaluation\nof FAIR-RAG. We first report the main end-to-end\nresults against a suite of strong baseline methods\n(Section 5.1). We then provide a deeper analysis\nof the framework\u2019s internal mechanics, including a\ncomponent-wise ablation study (Section 5.2.1) and\nan examination of the impact of iterative refinement\non answer quality and cost (Section 5.2.2).\n5.1 Main Results\nTable 2 presents the main performance comparison\nof FAIR-RAG and baseline methods across four di-\nverse question-answering benchmarks. Our frame-\nwork demonstrates state-of-the-art performance,\nparticularly on datasets that require complex, multi-\nstep reasoning. We report four key metrics:Exact\nMatch (EM)andF1-Scorefor lexical accuracy,\na script-basedAccuracy (ACC)which measures\nthe presence of the ground-truth answer string in\nthe generation, and our primary semantic metric,\nLLM-as-Judge Accuracy (ACCLLM). While EM,\nF1, and ACC are token-based,ACC LLM evaluates\nsemantic equivalence, offering a more robust as-\nsessment of generative answers.\nTo provide a fine-grained analysis of our frame-\nwork\u2019s iterative capabilities, we evaluate several\nconfigurations of FAIR-RAG in Table 2. The FAIR-\nRAG 1 to 4 variants correspond to the system\u2019s\nperformance with the maximum number of itera-\ntions capped at 1 to 4, respectively. To ensure a\nfair comparison against the baselines and to isolate\nthe impact of the iterative refinement cycle itself,\nthese variants utilize a single, consistent genera-\ntor model (Llama-3-8B-Instruct) across all stages.\nConversely, the (Adaptive LLMs) variants repre-\nsent our full, optimized framework, employing the\ndynamic allocation of different LLM agents (e.g.,\nsmall, large, reasoner) based on task complexity, as\ndetailed in our methodology.\nOur best-performing model,FAIR-RAG 3,\ndemonstrates leading performance within the\nparadigm of iterative and adaptive RAG. FAIR-\nRAG significantly outperforms strong representa-\ntive baselines from its architectural class across all\nmulti-hop benchmarks.It is critical to note that\nall comparisons were conducted under a unified\nand controlled experimental setup, ensuring a\nfair and reproducible evaluation.Within this rig-\norous framework, our results not only demonstrate\na consistent advantage in our direct head-to-head\ncomparisons but alsoset a new state-of-the-art\nperformance benchmark for this class of itera-\ntive methods on these datasets.While we refer-\nence previously published results from comparable\narchitectures (Shao et al., 2023; Asai et al., 2023;\nJeong et al., 2024), our primary claim of superiority\nis grounded in re-evaluating these methods under\nour standardized conditions to eliminate confound-\ning variables.\nThe most substantial gains are observed on the\nmulti-hop reasoning benchmarks. OnHotpotQA,\nour model achieves an F1 score of0.453, an abso-\nlute improvement of+8.3 pointsover the strongest\nbaseline, Iter-Retgen (0.370). Similarly, on2Wiki-\nMultiHopQAandMusique, FAIR-RAG achieves\nF1 scores of0.320and0.264, respectively, outper-\nforming the next-best method (Self-RAG) by a sig-\nnificant margin. These results strongly validate the\nefficacy of our core architectural contributions: the\nIterative Refinement Cycleand theStructured\nEvidence Assessment (SEA). These mechanisms\nempower FAIR-RAG to systematically deconstruct\ncomplex information needs, gather comprehensive\nevidence, and verify its sufficiency where single-\npass or less structured iterative methods fall short.\nNotably, FAIR-RAG also excels on the single-\nhop factual benchmark,TriviaQA, achieving an F1\nscore of0.731. This demonstrates that the frame-\nwork\u2019s sophisticated reasoning machinery does not\nimpose a penalty on simpler retrieval tasks and can\neffectively streamline its process, largely due to the\ninitialAdaptive Routingmodule.\nComparing different variants of our model, we\nsee a consistent performance increase from FAIR-\nRAG 1 to 4, indicating that the incremental en-\nhancements contribute positively. The introduction\nofAdaptive LLMsfurther boosts performance\nacross the board, confirming the benefits of dynam-\nically allocating computational resources based on\ntask complexity.\nTo complement the tabular data, Figure 2 pro-\nvides a visual comparison, plotting the F1 scores\n(bars) against the ACC LLM metric (lines) for all\nmethods across the four benchmarks. The figure\nvisually corroborates the superior performance of\nour FAIR-RAG framework, where its variants (in-\n\nType Method HotpotQA 2WikiMultiHopQA Musique TriviaQA\nEM F1 ACC ACC LLM EM F1 ACC ACC LLM EM F1 ACC ACC LLM EM F1 ACC ACC LLM\nSequential Standard RAG .238 .342 .328 .598 .086 .180 .282 .369 .074 .149 .137 .380 .570 .667 .675 .795\nBranching Sure .232 .346 .293 .646 .134 .198 .167 .408 .101 .169 .114 .397 .566 .675 .641 .799\nConditional Adaptive-RAG .144 .239 .368 .628 .041 .133 .340 .429 .038 .095 .160 .366 .480 .583 .670 .789\nReasoning ReAct .000 .028 .096 .503 .000 .062 .295 .467 .000 .016 .018 .376 .000 .046 .129 .667\nIterative\nIter-Retgen .265 .370 .353 .621 .104 .209 .310 .401 .115 .190 .178 .391 .581 .676 .689 .804\nSelf-RAG .174 .299 .321 .626 .123 .251 .333 .466 .073 .162 .132 .392 .385 .540 .639 .774\nIRCoT .006 .087 .399 .631 .001 .085.433 .488.001 .056 .210 .416 .016 .169 .674 .800\nIterative\nFAIR-RAG 1 .300 .398 .337 .628 .186 .288 .286 .414 .133 .216 .169 .418 .622 .706 .682 .821\nFAIR-RAG 2 .335 .447 .397 .689 .216 .325 .341 .458 .168 .253 .214 .465 .645 .732 .712 .837\nFAIR-RAG 3 .332 .447 .404 .697 .183 .305 .338 .450.178 .267 .221 .474 .631 .721 .701 .839\nFAIR-RAG 4.344 .456 .401 .697 .209 .333 .357 .486 .175 .266 .228 .475 .644 .728 .710 .837\nFAIR-RAG 2 (Adpt.) .331 .436 .384 .673 .183 .296 .313 .444 .158 .241 .207 .447 .640 .722 .704 .828\nFAIR-RAG 3 (Adpt.) .338 .453 .399 .694 .206 .320 .350 .452.178 .264 .222 .472 .645 .731 .710 .847\nTable 2: Main end-to-end performance comparison of FAIR-RAG against representative baselines across four\ndiverse question-answering benchmarks. The evaluation covers three complex multi-hop datasets (HotpotQA,\n2WikiMultiHopQA, Musique) and one open-domain factual dataset (TriviaQA). We report four metrics: Exact\nMatch (EM), F1-Score, script-based Accuracy (ACC), and LLM-as-Judge Accuracy (ACCLLM). Our FAIR-RAG\nvariants consistently achieve state-of-the-art performance, with the most significant improvements on the multi-hop\ntasks. The best-performing method for each metric is highlighted inbold. Underlined values denote results\nsurpassing the Self-RAG and Iter-Retgen baselines. All scores are based on 1000 test/dev samples.\ndicated by the patterned bars) consistently achieve\nthe highest F1 scores, particularly on the com-\nplex reasoning datasets of HotpotQA, 2WikiMulti-\nHopQA, and Musique.\nA key trend highlighted by the plots is thestrong\npositive correlation between the F1 score and\nthe ACCLLM. This suggests that the architectural\nimprovements within FAIR-RAG, which enhance\nthe accuracy of the Large Language Model\u2019s inter-\nnal processing and decision-making, are directly\nresponsible for the enhanced final answer accu-\nracy. This relationship is particularly evident in the\nHotpotQA and 2WikiMultiHopQA results, where\na noticeable uplift in the ACC LLM line for FAIR-\nRAG variants coincides with a significant increase\nin their corresponding F1 scores compared to the\nbaselines. Thus, the visualizations not only con-\nfirm FAIR-RAG\u2019s state-of-the-art performance but\nalso offer insight into the synergistic relationship\nbetween its internal reasoning accuracy and its final\noutput quality.\n5.2 Further Analysis\nBeyond the main end-to-end results, we conduct a\nseries of deeper analyses to deconstruct the sources\nof FAIR-RAG\u2019s performance. We perform a de-\ntailed component-wise evaluation to understand\nthe contribution of each module, analyze the spe-\ncific impact of the iterative refinement process, and\npresent a qualitative case study to illustrate the\nframework in action. For these fine-grained assess-\nments, we employ a sophisticatedLLM-as-Judge\nmethodology, using a powerful LLM to score the\noutput of each internal module against a set of pre-\ndefined criteria (Zheng et al., 2023). This task is\nguided by a structured prompt (see Appendix C for\ndetails).\n5.2.1 Component-wise Performance Analysis\n(Ablation Study)\nTo quantify the contribution of each key module\nwithin the FAIR-RAG pipeline, we conducted an\nablation study using an LLM-as-Judge method-\nology on 1000 samples from each dataset. The\nresults, summarized in Table 3, demonstrate the\nhigh functionality of each component, justifying\nits inclusion in the final architecture. For com-\nponents likeQuery DecompositionandQuery\nRefinement, the judge rated the output quality on a\n1-to-5 Likert scale. The rating was based on a holis-\ntic assessment of criteria includingrelevance(how\nwell it addresses the core need),specificity(how\nfocused the query is), andcoverage(whether it cap-\ntures all necessary facets of the information gap).\nThe average score across 1000 samples is reported.\nForEvidence Filtering, we report F1-Score based\non the LLM\u2019s ability to correctly classify docu-\nments as relevant or irrelevant. ForSEA, we report\nAccuracy based on its correctness in judging evi-\ndence sufficiency.\nThe analysis reveals several key insights:\n\u2022 Query Decomposition & Refinement are\nHighly Effective:The initialQuery De-\ncompositionmodule achieves a high average\nquality score across all datasets, peaking at\n4.33/5.0on TriviaQA. The subsequentQuery\nRefinementmodule scores even higher, with\nan average of4.45/5.0on HotpotQA. This\n\nFigure 2: Performance comparison of FAIR-RAG variants against baseline methods on four question-answering\nbenchmarks. Each subplot displays the F1 score (bars, left axis) and the LLM-as-Judge Accuracy (ACCLLM, line,\nright axis). Our FAIR-RAG models are highlighted with a hatched pattern. The results consistently demonstrate\nthe superiority of our framework, especially on complex multi-hop datasets (HotpotQA, 2WikiMultiHopQA, and\nMusiQue), where it significantly outperforms all baselines in F1 score while maintaining high semantic accuracy.\nAll evaluations are conducted on 1000 samples from each benchmark\u2019s development set.\nQuery Decomp. Evidence Filter SEA Query Refine.\nDataset Metric Value Metric Value Metric Value Metric Value\nHotpotQA Avg. Score 4.19 F1-Score 67.3% Accuracy 72.0% Avg. Score 4.45\n2WikiMultiHopQA Avg. Score 4.12 F1-Score 55.5% Accuracy 81.7% Avg. Score 4.39\nMusique Avg. Score 4.10 F1-Score 68.5% Accuracy 83.2% Avg. Score 4.42\nTriviaQA Avg. Score 4.33 F1-Score 76.1% Accuracy 54.4% Avg. Score 4.52\nTable 3: Component-level performance analysis of FAIR-RAG\u2019s core modules. This table isolates and evaluates the\neffectiveness of four key components: Query Decomposition, Evidence Filtering, Structured Evidence Assessment\n(SEA), and Query Refinement. The Avg. Score is based on a 1-to-5 Likert scale, while F1-Score and Accuracy are\nexpressed as percentages. All scores averaged over 1000 samples.\nvalidates that the LLM agents are proficient\nat both breaking down complex queries and\nintelligently generating new queries to fill in-\nformation gaps identified by the SEA module.\n\u2022 Evidence Filtering Presents a Precision-\nRecall Trade-off:The filtering module\u2019s per-\nformance varies, with F1 scores ranging from\n55.47%on 2WikiMultiHopQA to76.05%on\nTriviaQA. While the filter is effective at reduc-\ning context noise (precision), its aggressive na-\nture can sometimes prune useful information\n(recall). This highlights a classic trade-off and\npresents a clear avenue for future optimiza-\ntion.\n\u2022 Structured Evidence Assessment (SEA) is a\nChallenging but Crucial Task:The SEA\nmodule, which governs the iterative loop,\ndemonstrates strong performance on complex\nmulti-hop datasets, achieving an accuracy of\n81.73%on 2WikiMultiHopQA and83.19%\non Musique. Its lower accuracy on TriviaQA\n(54.38%) is expected, as the sufficiency deci-\nsion for single-hop factual questions can be\nmore ambiguous. These results confirm the\nmodule\u2019s value as the central control mecha-\nnism for the iterative process, proving highly\nreliable when it matters most.\nIt is important to note that a direct ablation study\n\nremoving the SEA and Query Refinement modules\nand replacing them with a fixed-iteration loop was\ndeliberately omitted. Such a stripped-down design,\nlacking adaptive control and explicit gap analysis,\nwould cease to be FAIR-RAG and instead would\nconceptually approximate the architectural princi-\nples of existing iterative baselines. For instance,\nwithout a targeted query refinement strategy driven\nby SEA, the system would likely resort to generat-\ning new queries from the previous answer, a core\nmechanism in ITER-RETGEN (Shao et al., 2023).\nFurthermore, by removing its primary mechanism\nfor evidence-aware self-correction, this simplified\nvariant would represent a less sophisticated control\nstrategy than that of models like Self-RAG (Asai\net al., 2023), which employ reflection at a more\ngranular level.\nGiven that FAIR-RAG already demonstrates a\nsignificant performance margin over these strong\nbaselines in our main experiments (see Table 2),\nour existing comparisons effectively serve as a val-\nidation of our adaptive, SEA-driven architecture\nversus these alternative control strategies. This\nconfirms that the observed performance gains are\nattributable to the synergistic and intelligent con-\ntrol of the SEA and Query Refinement modules,\nrather than merely the brute-force effect of repeated\niterations.\n5.2.2 Impact of Iterative Refinement\nA core hypothesis of this work is that iterative\nrefinement improves answer quality for complex\nquestions. We tested this by running the same set of\nquestions with the maximum number of iterations\nranging from 1 to 4. We then used an LLM-as-\nJudge to rank the resulting answers for each ques-\ntion. The results, along with efficiency metrics, are\npresented in Table 4.\nThe data reveals a clear and consistent pattern\nacross the three multi-hop datasets (HotpotQA,\n2WikiMultiHopQA, and Musique):\n\u2022 Optimal Performance at 2-3 Iterations:\nMoving from one to two iterations yields a\nsubstantial improvement in answer quality.\nOn 2WikiMultiHopQA, the average quality\nrank improves from3.08to2.31. The peak\nperformance is generally observed at either\nthe second or third iteration (e.g., an average\nrank of2.18at iteration 3 for 2WikiMulti-\nHopQA). This is accompanied by a highIm-\nprovement Rate, with the 2- or 3-iteration an-\nswer being judged superior to the 1-iteration\nanswer in approximately70%of cases for\n2WikiMultiHopQA.\n\u2022 Diminishing Returns:A fourth iteration con-\nsistently leads to a degradation in average an-\nswer quality across all complex datasets. This\nsuggests a point of diminishing returns where\nan additional retrieval cycle is more likely to\nintroduce noisy or tangentially related infor-\nmation that complicates the final synthesis\nstep.\n\u2022 Cost-Benefit Analysis:Each iteration adds\na considerable number of API calls and to-\nkens, increasing both latency and computa-\ntional cost. The optimal balance of 2-3 it-\nerations provides the best balance between\nanswer quality and resource consumption.\nConversely, on the simplerTriviaQAdataset,\nthe quality rank degrades with each additional it-\neration, confirming that for single-hop queries, the\ninitial retrieval is generally sufficient, and further\niterations are unnecessary and even detrimental.\nThis analysis confirms that iteration is crucial for\ncomplex reasoning, but an unrestrained number of\niterations is suboptimal. Our framework\u2019s default\nsetting of a maximum of 3 iterations is thereby em-\npirically justified as an effective balance between\nperformance and efficiency.\n5.2.3 A Complex Case Study: Comparative\nMulti-Hop Reasoning\nTo demonstrate the unique advantages of the FAIR-\nRAG architecture over other advanced RAG frame-\nworks, we analyze a hybridcomparative, multi-\nhop query. This type of query is particularly chal-\nlenging because it requires the system to conduct\ntwo parallel lines of multi-hop reasoning simulta-\nneously and then synthesize the results.\nThe query is: \u201cCompare the architectural\nstyles of the building that houses theMona Lisa\nand the museum in London that houses the\nRosetta Stone.\u201d\nStandard RAG Failure:A standard RAG sys-\ntem would treat this complex comparative query\nas a single, semantically overloaded search vector.\nThis unfocused approach is highly likely to fail for\ntwo primary reasons: First, it would struggle to si-\nmultaneously retrieve relevant, detailed documents\nfor both distinct lines of inquiry (the Louvre and\n\nDataset Max Iter. Avg. Answer Rank Improvement Rate Avg. API Calls Avg. Tokens/Query\n(Lower is better) (vs. Iter 1) (#)\nHotpotQA\n1 2.73 - 4.97 9,787\n22.23 58.50%6.64 14,332\n3 2.38 57.00% 7.83 17,281\n4 2.66 57.00% 9.01 20,299\n2WikiMultiHopQA\n1 3.08 - 4.99 9,823\n2 2.31 69.30% 7.10 15,413\n32.18 70.90%8.79 19,812\n4 2.43 67.30% 10.14 23,231\nMusique\n1 2.89 - 4.98 9,613\n22.2063.40% 7.25 15,688\n3 2.2863.70%9.01 20,162\n4 2.63 61.20% 10.56 24,218\nTriviaQA\n11.83- 4.97 9,572\n2 2.08 28.50% 5.97 12,071\n3 2.70 27.20% 6.76 14,003\n4 3.39 27.20% 7.25 15,186\nTable 4: Ablation study on the impact of the maximum number of refinement iterations on answer quality versus\ncomputational cost. The results reveal a clear point of diminishing returns. For complex multi-hop datasets,\noptimal performance (lowest Avg. Answer Rank) is achieved at 2 or 3 iterations, after which quality degrades\nwhile costs (API Calls, Tokens) continue to increase linearly. Conversely, for the simpler fact-based TriviaQA, any\niteration beyond the first proves detrimental. This analysis empirically justifies our framework\u2019s default setting of a\nthree-iteration maximum.\nthe British Museum). It might retrieve a general\ndocument about the Mona Lisa that lacks architec-\ntural details, or miss one of the entities entirely.\nSecond, and more fundamentally, standard RAG\nlacks the procedural logic to deconstruct the query,\npursue two parallel reasoning paths, and then syn-\nthesize the findings into a coherent comparison. It\nrelies on finding a single document that already\ncompares the two museums\u2019 architecture, which is\nhighly improbable. Consequently, a standard RAG\nwould likely produce a disjointed answer focusing\non only one of the entities, or fail entirely.\nWhy this query is difficult for other advanced\nRAGs:\n\u2022 AnITER-RETGENsystem, due to its inher-\nently single-threaded nature, might success-\nfully follow one reasoning path (e.g., find the\nLouvre, then its style). However, it lacks the\nmechanism to manage a parallel track simul-\ntaneously, causing it to lose the context of the\nsecond entity (the British Museum) and fail to\nproduce a coherent comparison.\n\u2022 AnAdaptive-RAGframework may correctly\nidentify the query as \u201ccomplex.\u201d However,\nit typically lacks a structured, multi-track\ndecomposition process. Without the abil-\nity to systematically pursue both information\nthreads in parallel before synthesis, its adap-\ntive strategy remains insufficient for true com-\nparative reasoning.\nFAIR-RAG in Action:\n\u2022 Iteration 1: Semantic Decomposition &\nParallel Initial Retrieval\n\u2013 Adaptive Sub-Queries:FAIR-RAG\u2019s\nfirst action is to decompose the compar-\native query into two distinct, parallel in-\nvestigative tracks:\n* Track A: [\u201cbuilding that houses the\nMona Lisa\u201d]\n* Track B: [\u201cmuseum in London that\nhouses the Rosetta Stone\u201d]\n\u2013 Retrieved Evidence:The system re-\ntrieves evidence for both tracks concur-\nrently:\n* Evidence A: \u201cThe Mona Lisa is\na half-length portrait painting by\nLeonardo da Vinci, on permanent dis-\nplay at theLouvre Museumin Paris,\nFrance.\u201d\n* Evidence B: \u201cThe Rosetta Stone is a\ngranodiorite stele on public display\nat theBritish Museumin London\nsince 1802.\u201d\n\u2013 Structured Evidence Assessment\n(SEA):\n* is_sufficient: \u2018No\u2019\n\nFigure 3: A qualitative case study demonstrating FAIR-RAG\u2019s two-iteration process for a complex comparative\nquery. In Iteration 1, the system decomposes the query and retrieves initial evidence identifying the museums but\nlacking the required architectural styles. The Structured Evidence Assessment (SEA) module correctly identifies this\ngap (Is Sufficient: No), triggering a second iteration. In Iteration 2, the system generates Refined Queries targeting\nthe missing information, successfully retrieves the necessary evidence, and confirms sufficiency. The process\nculminates in a Final Faithful Generation that synthesizes evidence from both iterations into a comprehensive\nanswer.\n* analysis_summary:The initial re-\ntrieval successfully resolved the pri-\nmary entities for both comparative\ntracks, identifying the Louvre Mu-\nseum and the British Museum. How-\never, the core \u2018Required Findings\u2019\nconcerning the architectural styles\nof these institutions remain unad-\ndressed. Therefore, critical informa-\ntion gaps prevent the synthesis of a\ncomplete answer.\n\u2022 Iteration 2: Parallel Query Refinement &\nEvidence Completion\n\u2013 Refined Queries:The refinement mod-\nule now uses the entities identified in Iter-\nation 1 to generate new, highly-focused\nqueries for each track:\n* Refined Query A: [\u201carchitectural\nstyle of the Louvre Museum\u201d]\n* Refined Query B: [\u201carchitectural\nstyle of the British Museum\u201d]\n\u2013 Retrieved Evidence:The new targeted\nqueries retrieve the final missing pieces\nof information:\n* Evidence A: \u201cThe architecture of the\nLouvre Palace is a complex blend\nof styles, from French Renaissance\nto Neoclassicism, famously comple-\nmented by I. M. Pei\u2019s Modernist\nglass pyramid.\u201d\n* Evidence B: \u201cThe main building of\nthe British Museum, designed by ar-\nchitect Sir Robert Smirke, is a mas-\nterpiece of theGreek Revivalstyle,\nfeaturing a grand quadrangle and a\nportico of 44 columns.\u201d\n\u2013 Structured Evidence Assessment\n\n(SEA):\n* is_sufficient: \u2018Yes\u2019\n* analysis_summary:The refined\nqueries successfully retrieved the fi-\nnal missing data points. All items on\nthe \u2018Required Findings\u2019 checklist\u2014\nthe identities of both museums and\ntheir respective architectural styles\u2014\nare now confirmed by the aggregated\nevidence. No information gaps re-\nmain, enabling a direct, evidence-\ngrounded comparison.\n\u2022Final Faithful Generation:The system syn-\nthesizes the evidence fromboth parallel\ntracksinto a single, structured, and fully-\ngrounded comparative answer:\n\u201cThe architectural styles of the buildings hous-\ning the Mona Lisa and the Rosetta Stone are\ndistinct. The Mona Lisa is housed in the\nLouvre Museumin Paris [1], which features\na complex blend of historical styles includ-\ningFrench Renaissance and Neoclassicism,\nalong with a famousModernistpyramid [3].\nThe Rosetta Stone is located in theBritish\nMuseumin London [2], which is a prime\nexample of theGreek Revivalarchitectural\nstyle [4].\u201d\nThis case study highlights FAIR-RAG\u2019s key\narchitectural advantage: its ability to systemati-\ncallydecomposea complex goal into parallel sub-\nproblems and then applyiterative refinementto\nsolve each one before synthesizing a comprehen-\nsive final answer. This structured, multi-threaded\nreasoning process is what enables it to succeed\nwhere other iterative or adaptive frameworks may\nfail.\n5.2.4 Failure Mode Analysis\nTo foster transparency and identify avenues for\nfuture improvement, we conducted a systematic\nfailure mode analysis on a sample of 200 unique\nerror instances drawn equally from our four bench-\nmark datasets: TriviaQA, MuSiQue, HotpotQA,\nand 2WikiMultihopQA. This analysis employed a\nhybrid methodology, combining LLM-based cat-\negorization with human expert validation to en-\nsure accuracy and depth. Our taxonomy distin-\nguishes between two fundamental sources of error:\n(1)Component-Level Failures, which stem from\nthe inherent limitations of the underlying modules\nFigure 4: Aggregate Distribution of Failure Sources.\nAnalysis of 200 error samples reveals a primary split\nbetween Component-Level Failures (63.5%) and Ar-\nchitectural Failures (36.5%). While architectural logic\noffers direct avenues for refinement, the majority of\nerrors stem from the inherent limitations of the founda-\ntional retrieval and generation models, identifying them\nas the principal bottleneck for the FAIR-RAG system.\n(i.e., the retriever and the generator LLM), and (2)\nArchitectural Failures, which are specific to the\ndecision-making logic of the FAIR-RAG frame-\nwork itself (i.e., Query Decomposition, Filtering,\nRefinement, and SEA).\nThis distinction is critical for understanding the\nsystem\u2019s bottlenecks. As shown in Figure 4, a sig-\nnificant majority of errors (63.5%) are Component-\nLevel, originating from the foundational tools our\nsystem is built upon. The remaining 36.5% are\nArchitectural, offering direct targets for refining\nFAIR-RAG\u2019s internal logic. This distribution un-\nderscores a key insight: while FAIR-RAG\u2019s itera-\ntive process is designed to mitigate the weaknesses\nof its components, the performance of these base\ncomponents remains the primary limiting factor in\noverall system accuracy.\n1. Component-Level Failures (63.5% of Errors):\nThe Foundational BottleneckThese errors are\nnot caused by FAIR-RAG\u2019s reasoning process but\nby the fundamental limitations of the tools it or-\nchestrates.\n\u2022 Retrieval Failure (32.5%):This was the sin-\ngle largest source of error across all datasets.\nThe retriever\u2019s inability to surface critical doc-\numents is a major obstacle, particularly for\nqueries requiring highly specific, long-tail fac-\ntual knowledge. The primary root cause was\nidentified as Knowledge Base Gaps, where the\nrequired information was simply absent from\nthe corpus. This was especially pronounced\n\nin TriviaQA, where nearly half of all failures\nwere retrieval-related due to the dataset\u2019s re-\nliance on obscure facts.\n\u2022 Generation Failure (31.0%):In these cases,\nthe correct evidence was successfully identi-\nfied and passed to the final generator, which\nstill produced a flawed answer. This category\nrepresents a significant challenge across all\ndatasets, highlighting the inherent faithfulness\nproblem in modern LLMs. Common failure\nsubtypes included: (i) Incorrect Entity Se-\nlection, where the model chose the wrong\nentity from a list of candidates in the evi-\ndence; (ii) Flawed Logical Inference, espe-\ncially in comparative questions (e.g., \u201cwho\nis younger?\u201d); and (iii) Misinterpretation of\nQuestion Granularity, such as providing a spe-\ncific year (\u201c1922\u201d) when a decade (\u201c1920s\u201d)\nwas requested.\n2. Architectural Failures (36.5% of Errors): Re-\nfining the FAIR-RAG LogicThese errors are di-\nrectly attributable to FAIR-RAG\u2019s internal decision-\nmaking modules and represent the most direct op-\nportunities for improving our framework.\n\u2022 SEA Error (24.5%):As the \u201cbrain\u201d of the\niterative process, failures in the Strategic Evi-\ndence Assessment module are particularly im-\npactful. These errors were significantly more\nprevalent in complex, multi-hop datasets like\nMuSiQue, HotpotQA, and 2WikiMultihopQA.\nThe most common subtypes were: (i) Faulty\nAnalysis of Evidence, where the SEA module\nfailed to make a correct logical inference from\nthe provided documents (e.g., misinterpreting\ncomplex genealogical relationships); and (ii)\nPremature Sufficiency Judgment, where the\nlogic incorrectly concluded that the gathered\nevidence was complete, thus halting the re-\nfinement loop too early.\n\u2022 Query Logic Failures (12.0% combined):\nThis group includes errors from the initial\nquery processing stages. Query Decompo-\nsition Errors (9.0%) were the most common,\noften stemming from an inability to challenge\nflawed premises within the user\u2019s question\n(e.g., processing a query with a historical\nanachronism) or generating overly broad sub-\nqueries. Query Refinement (1.5%) and Ev-\nidence Filtering Errors (1.5%) were exceed-\nFigure 5: Task-Dependent Distribution of Failure\nModes. The analysis highlights a strong correlation\nbetween task complexity and the primary failure bot-\ntleneck. For the factoid-centric TriviaQA, Retrieval\nFailures are dominant (47%). Conversely, for complex\nmulti-hop reasoning datasets like MuSiQue, HotpotQA,\nand 2WikiMultihopQA, the burden shifts towards rea-\nsoning, with SEA Errors becoming a major failure cate-\ngory (28-32%). This trend validates the critical role of\nthe strategic reasoning component (SEA) for success-\nfully navigating multi-step queries.\ningly rare, suggesting that these architectural\ncomponents are relatively robust.\nDataset-Specific Error DistributionsWhile the\naggregate view is informative, Figure 5 reveals\nthat the distribution of failure modes is highly de-\npendent on the nature of the task. For fact-based,\nsingle-hop datasets like TriviaQA, failures are over-\nwhelmingly concentrated in the Retrieval stage\n(47%). However, as query complexity increases in\nmulti-hop datasets (MuSiQue, HotpotQA, 2Wiki-\nMultihopQA), the burden shifts. In these cases,\nSEA Errors become significantly more prominent,\naccounting for 28-32% of all failures. This demon-\nstrates that for complex reasoning tasks, the pri-\nmary challenge moves beyond simply finding infor-\nmation to correctly reasoning about and managing\nit. This trend strongly validates the necessity of a\nsophisticated strategic control module like SEA in\nadvanced RAG systems.\n6 Conclusion\nIn this paper, we introducedFAIR-RAG, a novel,\nagentic framework designed to address a key limi-\ntation of existing Retrieval-Augmented Generation\nsystems: their unreliability in handling complex,\nmulti-hop queries. By architecting an evidence-\ndriven, iterative process, FAIR-RAG advances be-\nyond the static \u201cretrieve-then-read\u201d paradigm. Our\ncore contributions\u2014Adaptive Routing, theItera-\ntive Refinement Cycle, and the analytical gating\nmechanism of theStructured Evidence Assess-\nment (SEA)module\u2014work in synergy to progres-\n\nsively build and validate a comprehensive context\nbefore generation. The SEA\u2019s ability to systemati-\ncally deconstruct queries and identify information\ngaps provides a precise, actionable signal that di-\nrectly guides the query refinement process.\nOur extensive experiments demonstrate that\nFAIR-RAG\u2019s structured approach achieves leading\nperformance among comparable iterative and adap-\ntive RAG architectures across challenging multi-\nhop QA datasets like HotpotQA and 2WikiMul-\ntiHopQA. These results empirically validate our\ncentral hypothesis: that a procedural, multi-stage\nworkflow with explicit evidence assessment is es-\nsential for achieving high accuracy and faithfulness\nin knowledge-intensive tasks.\n6.1 Limitations\nDespite its strong performance, FAIR-RAG\npresents several inherent trade-offs and limitations\nthat warrant discussion:\n\u2022 Dependency on LLM Reasoning Fidelity\nand Prompt Engineering:The performance\nof FAIR-RAG\u2019s modular agents (e.g., query\nrefinement, SEA) is inherently bound by two\nkey factors: the reasoning capabilities of the\nunderlying LLMs and the meticulous design\nof the prompts that guide them. While our\nstructured prompting methodology\u2014which\nincorporates clear instructions, illustrative ex-\namples, and scaffolding techniques\u2014is de-\nsigned to ensure consistency and robustness,\nthe system\u2019s effectiveness remains sensitive\nto both the choice of the backbone model and\nthe specific phrasing of the prompts. This\ndual dependency is an intrinsic limitation of\ncurrent LLM-based agentic systems, where\nperformance gains are often tightly coupled\nwith advancements in both model architecture\nand sophisticated prompt engineering.\n\u2022 Comprehensiveness vs. Efficiency Trade-\noff:The iterative nature of FAIR-RAG, which\nis key to its high accuracy on complex queries,\nintroduces a natural trade-off with efficiency.\nAs shown in our analysis (Table 4), each re-\nfinement cycle increases overall latency and\ncomputational cost, making it more expensive\nthan single-shot RAG methods.\n\u2022 Potential for Error Propagation:As a multi-\nstage pipeline, errors in early stages can cas-\ncade. The SEA module, in particular, repre-\nsents a critical point of failure. An erroneous\nsufficiency judgment\u2014either a false positive\nthat terminates the loop prematurely or a false\nnegative that extends it unnecessarily\u2014can\nlead the evidence-gathering process astray.\n\u2022 Fixed Iteration Policy:The maximum of\nthree iterations is an empirically derived\nheuristic that balances performance and cost.\nHowever, a fixed limit lacks the flexibility to\nadapt to the varying complexity of individual\nqueries. Our results show this is a robust aver-\nage but may not be optimal for every specific\ncase.\n6.2 Future Work\nThe limitations of our current work open up several\npromising directions for future research to create\nmore efficient and adaptive systems:\n\u2022Distilling Task-Specific Expert Models:To\nmitigate the reliance on expensive, general-\npurpose LLMs, a promising direction is to\nfine-tune or distill smaller, specialized mod-\nels for each core task (Hinton et al., 2015).\nCreating dedicated expert models for query\nrefinement or evidence assessment could lead\nto a system that is faster, cheaper, and more\nrobust.\n\u2022 Learning a Dynamic Control Policy:The\niterative process can be framed as a sequential\ndecision-making problem. We propose explor-\ning Reinforcement Learning (RL) to train a\npolicy network that learns to dynamically con-\ntrol the workflow (Schick et al., 2023). At\neach step, this agent could decide whether to\nretrieve more information, refine the query,\nor proceed to generation, replacing the fixed-\nloop structure with a far more efficient and\nadaptive strategy.\n\u2022 Extension to Multimodal Reasoning:The\ncurrent FAIR-RAG framework operates exclu-\nsively on textual data. A natural and impactful\nextension would be to adapt its core principles\nof decomposition, iterative refinement, and\nstructured assessment to handle queries over\nmultimodal knowledge bases that include ta-\nbles, images, and structured data, creating a\nmore versatile and comprehensive question-\nanswering system (Alayrac et al., 2022).\n\nReferences\nAI@Meta. 2024. The llama 3 herd of models.arXiv\npreprint arXiv:2407.21783.\nAI@Meta. 2025. Llama 4: Maverick language models.\nIntroducing Llama 4 Scout and Llama 4 Maverick\n(official blog & model card).\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a visual language model for few-shot\nlearning.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\narXiv preprint arXiv:2310.11511.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nGordon V Cormack, Charles LA Clarke, and Stefan\nBuettcher. 2009. Reciprocal rank fusion outperforms\ncondorcet and individual rank learning methods. In\nProceedings of the 32nd international ACM SIGIR\nconference on Research and development in informa-\ntion retrieval, pages 758\u2013759.\nDeepSeek-AI. 2025. Deepseek-r1: An open\nlarge reasoning model family.arXiv preprint\narXiv:2501.12948.\nShahul Es, Jithin James, Luis Espinosa-Anke, and\nSteven Schockaert. 2025. Ragas: Automated evalua-\ntion of retrieval augmented generation.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.arXiv\npreprint arXiv:2002.08909.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2025. A survey on hallucination in large lan-\nguage models: Principles, taxonomy, challenges, and\nopen questions.ACM Transactions on Information\nSystems, 43(2):1\u201355.\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\nHwang, and Jong C. Park. 2024. Adaptive-rag:\nLearning to adapt retrieval-augmented large language\nmodels through question complexity.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation.ACM Comput-\ning Surveys, 55(12):1\u201338.\nZhengbao Jiang, Vladimir Lialin, Carroll Lin, Jane Liu,\nand Yelong Cheng. 2023. Flare: Forward-looking\nactive retrieval augmented generation.arXiv preprint\narXiv:2305.06983.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019.\nBillion-scale similarity search with GPUs.IEEE\nTransactions on Big Data, 7(3):535\u2013547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering.arXiv preprint\narXiv:2004.04906.\nJaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin\nPark, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha,\nand Jinwoo Shin. 2024. Sure: Summarizing re-\ntrievals using answer candidates for open-domain\nqa of llms.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Rodrigo Nogueira,\nHeinrich Paux, Pontus Stenetorp, Timo Rockt\u00e4schel,\nSebastian Riedel, et al. 2020. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks.Ad-\nvances in Neural Information Processing Systems,\n33:9459\u20139474.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023a. Lost in the middle: How language\nmodels use long contexts.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels.\nStephen Robertson and Hugo Zaragoza. 2009. The prob-\nabilistic relevance framework: Bm25 and beyond.\nFoundations and Trends\u00ae in Information Retrieval,\n3(4):333\u2013389.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nZihan Shao, Yue Zhang, Minchen Zhao, Wenxuan Chen,\nand Yang Zhang. 2023. Iter-retgen: Iterative retrieval-\naugmented generation for charge-based legal le-\nniency prediction.arXiv preprint arXiv:2310.03352.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2024. Text embeddings by weakly-\nsupervised contrastive pre-training.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models.Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nShi-Qi Yan, Kang Liu, Jia-Chen Li, Zhao-Xiang\nWang, Jie Zhang, and Lin Gui. 2024. Correc-\ntive retrieval augmented generation.arXiv preprint\narXiv:2401.15884.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning.arXiv preprint arXiv:1809.09600.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.arXiv preprint arXiv:2210.03629.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Brooks, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena.arXiv preprint arXiv:2306.05685.\nA Implementation Details and\nHyperparameters\nTo ensure full reproducibility of our experimen-\ntal results, this section provides a comprehensive\noverview of the models, configurations, and hy-\nperparameters used in our study. All experiments\nwere conducted within theFlashRAGframework,\na standardized library for RAG research. This\nensures that the underlying implementation de-\ntails, such as data loading and prompt templat-\ning, remain consistent across all compared meth-\nods. The source code and default configurations\ncan be found on the official project repository:\nhttps://github.com/ruc-nlpir/flashrag.\nThe specific configurations for our experiments\nare detailed in Table 5 below.\nCategory Parameter & Value\nGeneral Framework\nBase Framework: FlashRAG\nMax Iterations: 3\nRetriever Configuration\nModel: e5-base-v2\nDocuments Retrieved (top_k): 5\nIndex Type: Faiss (IndexFlatIP)\nPooling Method: mean\nQuery Max Length: 128\nGenerator Configuration\nBaseline: Llama-3-8B-Instruct\nSelf-RAG: selfrag-llama-7b\nFAIR-RAG: Llama-3-8B-Instruct\nMax New Tokens: 1024\nMax Input Length: 8000\nEvaluation Settings\nMetrics: em, f1, acc\nRandom Sample: True\nTable 5: Hyperparameter and Model Configuration De-\ntails. Unless specified otherwise, all hyperparameters\nsuch as temperature, top_p, and sequence lengths were\nkept at the default values provided by the FlashRAG\nframework to ensure a fair and controlled comparison\nacross all tested methodologies.\n\nB Full Prompts for the FAIR-RAG\nPipeline\nThis appendix reproduces the exact prompts that\nguide the behavior of the specialized agents within\nthe FAIR-RAG pipeline.\nB.1 Query Validation and Dynamic Model\nSelection\nThe following prompt is used by the initial agent\nto validate the user\u2019s query for clarity and safety,\nand to select the most appropriate execution model\n(e.g., simple RAG vs. full agentic pipeline).\nPROMPT = \"\"\"\n**Situation:** A user has submitted a question to a Question\nAnswering System that uses different processing\nstrategies based on query complexity.\n**Intent:**\nAnalyze the user's question to determine the optimal\nprocessing strategy required to generate the most\naccurate answer. The strategies are: Factual Retrieval\n(SMALL), Information Synthesis (LARGE), or Multi-Step\nDeduction (REASONER).\n**Scaffolding:**\nYou are a highly-calibrated query analysis agent. Your task\nis to classify the user's question into one of the\nthree categories below based on the cognitive process\nrequired to answer it. After \"Selected Label:\", output\nONLY the exact label.\n- **\"SMALL\" (Factual Retrieval):**\n- **Process:** Requires finding a single, self-contained\nfact. The answer is typically explicit and doesn't need\nmuch context.\n- **Example:** \"When was the Eiffel Tower completed?\" or\n\"Who is the CEO of NVIDIA?\"\n- **\"LARGE\" (Information Synthesis):**\n- **Process:** Requires combining, summarizing, or\ncomparing information from one or more documents to\nform a coherent, explanatory answer. This is the\nappropriate choice for most standard, open-ended\nquestions.\n- **Example:** \"What is the difference between nuclear\nfission and fusion?\" or \"Explain the impact of the\nprinting press on the Renaissance.\"\n- **\"REASONER\" (Multi-Step Deduction):**\n- **Process:** The answer is not explicitly stated and\nmust be inferred by chaining multiple pieces of\ninformation together (multi-hop reasoning) or by\nperforming calculations.\n- **Example:** \"What is the hometown of the director of\nthe movie that starred Tom Hanks and was released in\n1994?\" or \"If a car travels 150 km in 2 hours, what is\nits average speed in meters per second?\"\n**Analytical Guidelines for Classification:**\n1. \"SMALL\" (Factual Extraction):\n- Cognitive Task: Find and extract a specific, named\nentity or a short, self-contained fact (e.g., a name,\ndate, location, number, or title).\n- Key Indicators: The question can be answered with a\nsingle piece of information, often a proper noun or a\nspecific value. It typically starts with \"Who,\" \"When,\"\n\"Where,\" or \"What is the name of...\"\n- Example Questions: \"Who directed the movie Inception?\"\nor \"What year did the Berlin Wall fall?\"\n- Decision Rule: Classify as SMALL if the expected\nanswer is a concise, singular fact that requires no\nfurther explanation or combination of information.\n2. \"LARGE\" (Information Synthesis & Elaboration):\n- Cognitive Task: Gather, combine, and summarize\ninformation from one or multiple sources to form a\ncohesive, descriptive answer. This involves explaining\nconcepts, comparing entities, or describing processes.\n- Key Indicators: The question asks for an explanation,\ndescription, comparison, or summary. It often contains\nkeywords like \"Explain,\" \"Describe,\" \"Compare,\" \"What\nis the difference between,\" or \"Summarize the impact\nof...\"\n- Example Questions: \"What are the main differences\nbetween crocodiles and alligators?\" or \"Explain the\nprimary causes of the Industrial Revolution.\"\n- Decision Rule: Classify as LARGE if the expected\nanswer is a paragraph or a detailed sentence that\ncombines multiple facts into a comprehensive\nexplanation.\n3. \"REASONER\" (Logical Inference & Multi-Step Deduction):\n- Cognitive Task: Connect multiple, separate pieces of\ninformation to infer a new fact that is not explicitly\nstated in any single document. This often involves a\nchain of logic or a sequence of dependent questions.\n- Key Indicators: The question requires finding an\nintermediate answer to proceed to the next step. It\noften involves relationships between different entities\nor requires a calculation.\n- Example Questions: \"What was the nationality of the\nlead actress in the movie directed by the person who\nmade Titanic?\" or \"Which team won the FIFA World Cup in\nthe year the lead singer of Queen was born?\"\n- Decision Rule: Classify as REASONER if the answer\ncannot be found directly but must be constructed by\nfirst finding Fact A, then using Fact A to find Fact B.\n**User Question:** \"{user_query}\"\n**Constraints:**\n- Respond with ONLY one of the three labels: SMALL, LARGE, or\nREASONER.\n- Do not provide any explanations or additional text.\n- The label MUST be on a new line after \"Selected Label:\".\n**Output:**\nSelected Label:\n\"\"\"\nB.2 Query Decomposition\nThis prompt instructs the decomposition agent to\nbreak down a complex, multi-faceted user query\ninto a set of simpler, semantically distinct sub-\nqueries for parallel or sequential retrieval.\nPROMPT = \"\"\"\n**Situation:** You are an expert query analyst for a general\nknowledge Question-Answering system. A user has asked a\nquestion that might be complex, comparative, or\nmulti-faceted. Your task is to decompose this question\ninto a set of precise, meaningful, and distinct\n\nsub-queries to ensure the retrieval system can find\ncomprehensive and accurate evidence from a database.\n**Intent:** Decompose the original user question into its\ncore semantic components. Transform these components\ninto short, keyword-rich, and meaningful search phrases\nin English. The goal is to generate queries that, when\nsearched, will collectively cover all aspects of the\noriginal question.\n**Scaffolding:**\nFirst, understand the principles of effective decomposition:\n1. **Identify Distinct Concepts:** Separate the main\nsubjects, actions, conditions, and comparisons in the\nquery.\n2. **Use Synonyms & Related Terms:** Think about different\nways a concept might be phrased in the database (e.g.,\n\"interaction\" can be searched as \"relationship\" or\n\"cooperation\").\n3. **Create Meaningful Phrases:** Instead of single keywords,\ngenerate short phrases that preserve the context of the\nsub-question.\n4. **Cover All Angles:** Ensure every part of the original\nquestion is represented by at least one sub-query.\nNow, study the following example carefully to understand how\nto apply these principles.\n--- EXAMPLE ---\n**Original User Query:** \"What was Albert Einstein's view on\nquantum mechanics and how did he interact with Niels\nBohr about it?\"\n**Rationale/Analysis (This is your thought process):**\nThe query has two main, distinct parts:\n1. Einstein's **opinion/view** about quantum mechanics.\n2. Einstein's **interaction** with Niels Bohr on the topic.\nA good search needs to find evidence for both aspects\nseparately. Simply searching for \"Einstein quantum\nmechanics\" might not retrieve documents that\nspecifically discuss his \"view\" or \"interaction\".\nTherefore, I should create targeted queries for each\nconcept.\n**Optimized Queries (Output):**\n- Einstein's opinion on quantum mechanics\n- Einstein Bohr debates on quantum theory\n- Collaboration between Einstein and Bohr\n- Einstein's criticism of quantum mechanics\n--- END OF EXAMPLE ---\nNow, apply this exact methodology to decompose the following\nquery.\n**User Query:** \"{user_query}\"\n**Constraints:**\n- The output must be a list of meaningful search phrases.\n- Each phrase must be on a new line, prefixed with a hyphen\n(-).\n- Queries must be in English.\n- Generate an optimized list of 1 to 4 sub-queries. Create\nONLY as many as are **truly necessary** to cover all\naspects of the original question.\n**Output:** (just write Optimized Queries and do not explain\nany more and do not say \"Here are the optimized\nqueries:\" or something like that.)\nOptimized Queries: (A list of optimized queries)\n\"\"\"\nB.3 Evidence Filtering\nThis prompt guides the filtering agent to assess\nthe relevance and quality of the retrieved evidence\nchunks against a given (sub-)query, discarding ir-\nrelevant, redundant, or low-quality information.\nPROMPT = \"\"\"\nYou are filtering retrieved documents for a\nquestion-answering system. Your goal is to KEEP all\ndocuments that could contribute to answering the query.\n**IMPORTANT PRINCIPLES:**\n1. BE INCLUSIVE: When in doubt, KEEP the document.\n2. A document is useful if it contains factual information\nabout the entities/topics in the query.\n3. Even partial information is valuable (e.g., a document\nabout Terry Richardson without birthdate is still\nuseful for a query about his age).\n4. Documents are only \"Not Useful\" if they are about\nCOMPLETELY DIFFERENT entities or topics.\n**Original User Query:** \"{original_query}\"\n**Retrieved Documents (Batch {batch_number}):**\n{numbered_candidates_text_for_prompt}\n**TASK:** Identify ONLY documents that are completely\nirrelevant. A document is irrelevant ONLY if:\n- It's about a different person/entity with a similar name\n(e.g., Tony Richardson vs Terry Richardson).\n- It's about a completely unrelated topic.\n- It contains no information about any entity mentioned in\nthe query.\n**OUTPUT FORMAT:**\n- List ONLY the document IDs that should be removed\n- If all documents are potentially useful, output: None\n- Format: [doc_X], [doc_Y] or None\n**You MUST KEEP documents that have:**\n- Documents about the correct person/entity, even without\nspecific dates/facts.\n- Biographical information (birth dates, career details,\nachievements).\n- Relationships or connections between queried entities.\n- Specific facts relevant to the query type (dates for\ntemporal queries, attributes for comparisons).\n- Contextual information that helps understand the entities.\n**Examples of what to REMOVE:**\n- Documents about different people with similar names.\n- Documents about unrelated topics.\n- Duplicate documents (keep the most informative version).\n**Output:** (A list of **irrelevant** temporary document IDs,\nor \"None\")\nUnhelpful Document IDs:\n\"\"\"\n\nB.4 Structured Evidence Assessment (SEA)\nThe Structured Evidence Assessment (SEA) agent\nuses the following prompt to analyse the evidence\nand determine if the gathered and filtered evidence\nis adequate to form a complete and faithful answer\nto the user\u2019s query.\nPROMPT = \"\"\"\n**Role:** You are a Strategic Intelligence Analyst. Your\nmission is to determine if the provided evidence is\nsufficient to accurately answer the user's question by\nfollowing a sequential analysis.\n**Core Mission:** Your entire process must be\nquestion-centric, not evidence-centric. You will\ndeconstruct the user's query into a checklist of\nrequired information, and then systematically verify\neach item against the evidence. You MUST ignore all\ninformation, however interesting, that is not on your\nchecklist.\n**You MUST follow this thinking process and output format\nexactly:**\n**1. Mission Deconstruction:**\n- **Main Goal:** [State briefly the primary objective of the\nuser's question and what the user's question requires\nyou to find]\n- **Required Findings:** [List the specific, individual\npieces of information needed to answer the question. A\n\"finding\" can be a direct fact or a logical inference\nfrom clues.]\n**2. Intelligence Synthesis & Analysis:**\n- **Confirmed Findings:** [Go through your \"Required\nFindings\" checklist. For each item, state what the\nevidence confirms. If the finding is not stated\ndirectly, explain the logical inference you made from\nthe provided clues. You MUST only mention facts that\ncan contribute to answering the question's required\ncomponents (checklist). You MUST ignore any evidence,\nentities, or facts-even if interesting-that do not help\nanswer the specific components of the user's question.\nDo not mention irrelevant people or topics in your\nanalysis. You are an expert. If the evidence provides\nstrong, logical clues (e.g., a person's birthplace in a\ncountry, a job title within an industry), you MUST make\nthe logical inference (e.g., determining nationality,\nprofession). Do not use weak phrases like \"it does not\nexplicitly state.\"]\n- **Remaining Gaps:** [If there is missing information,\nclearly state what crucial information is still\nmissing, formulating it as a requirement for the next\nphase that creates new queries to search more. else\nNone]\n**3. Final Assessment:**\n- **Conclusion:** [The final answer may not be explicitly\nstated in a single sentence. You are an expert. If the\nevidence provides strong, logical clues (e.g., a\nperson's birthplace in a country, a job title within an\nindustry), you MUST make the logical inference (e.g.,\ndetermining nationality, profession). Do not use weak\nphrases like \"it does not explicitly state.\"]\n- **Sufficient:** [A single word: \"Yes\" if the \"Remaining\nGaps\" list is empty, or \"No\" if any required finding is\nstill missing.]\n--- EXAMPLES ---\n**--- Example 1 (Insufficient Evidence - Clear Gap) ---**\n**Original Question:** \"What was the official box office\ngross for the film directed by the creator of the TV\nseries *'Seinfeld'*?\"\n**Evidence:**\n- \"Larry David, the creator of the acclaimed TV series\n*'Seinfeld'*, also wrote and directed the 2013 film\n*'Clear History'*.\"\n- \"The film *'Clear History'* starred Larry David and Jon\nHamm and was released on HBO.\"\n- \"The 2019 film *'Joker'* had a box office gross of over $1\nbillion.\"\n**Your Output for Example 1:**\n**1. Mission Deconstruction:**\n- **Main Goal:** To find the box office gross for the film\ndirected by the creator of *'Seinfeld'*.\n- **Required Findings:** A: The identity of the creator of\n*'Seinfeld'*; B: The name of the film they directed; C:\nThe official box office gross of that film.\n**2. Intelligence Synthesis & Analysis:**\n- **Confirmed Findings:** A: The evidence confirms the\ncreator is **Larry David**. B: The evidence confirms\nthe film he directed is **'Clear History'**.\n- **Remaining Gaps:** C: The official box office gross for\nthe film *'Clear History'*.\n**3. Final Assessment:**\n- **Conclusion:** We have identified the director **Larry\nDavid** and the film he directed is **'Clear\nHistory'**. but the evidence lacks The official box\noffice gross for the film *'Clear History'* to answer\nthe question.\n- **Sufficient:** No\n**--- Example 2 (Sufficient Evidence - Inference Required)\n---**\n**Original Question:** \"Who is older, the author of\n*'Dracula'* or the lead actor from the 1931 film\nadaptation?\"\n**Evidence:**\n- \"Bram Stoker, the Irish author, wrote the classic horror\nnovel *'Dracula'* in 1897.\"\n- \"Stoker was born in Dublin, Ireland, in November 1847.\"\n- \"The 1931 film adaptation of *'Dracula'* famously starred\nHungarian-American actor Bela Lugosi in the title role.\"\n- \"Bela Lugosi's date of birth is recorded as October 20,\n1882.\"\n**Your Output for Example 2:**\n**1. Mission Deconstruction:**\n- **Main Goal:** To compare the ages of the author of\n*'Dracula'* and the lead actor of the 1931 film.\n- **Required Findings:** A: The birth year of the author of\n*'Dracula'*; B: The birth year of the lead actor of the\n1931 film.\n**2. Intelligence Synthesis & Analysis:**\n- **Confirmed Findings:** A: The evidence states the author\nis **Bram Stoker**, who was born in **1847**. B: The\nevidence states the lead actor is **Bela Lugosi**, who\nwas born in **1882**.\n- **Remaining Gaps:** None.\n**3. Final Assessment:**\n- **Conclusion:** We have found the birth year of the author\nof *'Dracula'* is **Bram Stoker**, who was born in\n**1847**. the lead actor of the 1931 film is **Bela\n\nLugosi**, who was born in **1882**. We have found the\nbirth years for both required individuals and can\ntherefore perform the age comparison.\n- **Sufficient:** Yes\n--- END OF EXAMPLES ---\nNow, perform this task for the following:\n**Original Question:**\n\"{original_query}\"\n**Evidence:**\n{combined_evidence}\n\"\"\"\nB.5 Query Refinement\nIf the evidence is found to be insufficient by the\nStructured Evidence Assessment (SEA) agent, this\nprompt is used to generate a refined or a new follow-\nup query to retrieve the missing information.\nPROMPT = \"\"\"\n**Situation:** An initial analysis of the evidence has\nconfirmed some facts but also identified specific\ninformation that is still missing and required to\nanswer the user's original query.\n**Intent:** Generate a new, optimized list of search queries\nthat are laser-focused on finding ONLY the missing\npieces of information identified in the \"Analysis\nSummary\".\n**Scaffolding & Logic:**\n- **USE the known facts** from the summary to make the new\nqueries more precise (e.g., use a person's name once\nit's known).\n- **TARGET the missing information** from the summary\ndirectly. Each new query should aim to resolve one of\nthe identified gaps.\n- **AVOID repeating** or rephrasing previous queries.\n--- ADVANCED EXAMPLE ---\n**Original Question:** \"How old is the youngest child of the\ndirector of the film *Inception*?\"\n**Analysis Summary:**\nBased on the evidence, we know that the director of\n*Inception* is **Christopher Nolan**. Christopher Nolan\nis married to producer Emma Thomas. Christopher Nolan\nhas children. However, the provided documents contain\n**no specific information about his children**, such as\ntheir names and birth dates. To answer the question, we\nstill need to find: **the names and ages of Christopher\nNolan's children** to identify the youngest.\n**Previous Queries:**\n- director of Inception\n- Christopher Nolan films\n**Your Output for Example:**\nImproved Queries:\n- Christopher Nolan children names\n- Christopher Nolan children birth dates\n- Youngest child of Christopher Nolan and Emma Thomas\n--- END OF EXAMPLE ---\nNow, apply this exact logic to the following inputs:\n**Original Question:** {original_query}\n**Analysis Summary:**\n{analysis_summary}\n**Previous Queries:**\n{combined_previous_queries}\n**Constraints:**\n- Generate an optimized list of 1 to 4 sub-queries. Create\nonly as many as are truly necessary.\n- Queries must be simple, independent, meaningful, and\nkeyword-focused.\n- **Leverage the \"Known Facts\" to create highly targeted\nqueries.** For example, once the summary confirms the\ndirector is'Christopher Nolan', the next query should\nbe \"Christopher Nolan children ages\", not a generic\n\"director of Inception children ages\".\n**Output:** (A list of new, targeted queries. Do not explain\nanything and do not say \"Here are the optimized\nqueries:\" or something like that.)\nImproved Queries:\n\"\"\"\nB.6 Faithful Answer Generation\nThis is the final and most comprehensive prompt,\ninstructing the generation model to synthesize the\nfiltered evidence into a faithful, accurate, and well-\nstructured answer. It strictly constrains the model\nto use only the provided sources and to avoid any\nform of hallucination.\nPROMPT = \"\"\"Answer the question based on the given documents.\nONLY give me the answer and do not output any other words.\nThe following are given documents.\nThe retrieval documents are listed as follows:\n{combined_evidence}\nQuestion:\n{original_query}\nAnswer:\n\"\"\"\nC LLM-as-Judge Evaluation Prompts\nThis appendix contains the complete and\nunabridged prompts used in ourLLM-as-Judge\nevaluation framework. To ensure full transparency\nand enable the replication of our evaluation\nmethodology, we provide the exact instructions\ngiven to the judge model for each assessment\ncriterion. Each prompt is carefully designed to\nelicit a consistent and unbiased evaluation of a\nspecific quality aspect of the generated responses.\n\nThe evaluation is performed by providing the\njudge model with the original query, the retrieved\ncontext, and the generated answer, along with one\nof the following instructional prompts.\nC.1 Binary Semantic Correctness (ACC LLM)\nTo ensure the reproducibility of our semantic accu-\nracy evaluation, this section details the prompt used\nfor theLLM-as-Judge Accuracy (ACC LLM)met-\nric, as reported in Table 2. The prompt instructs the\nLLM to act as an impartial judge, comparing the\nmodel\u2019s generated answer against a set of ground-\ntruth answers. It provides a binary \"Yes\" or \"No\"\njudgment in a structured JSON format, enabling\nautomated and consistent evaluation of semantic\ncorrectness.\nACC_PROMPT = \"\"\"You are an impartial judge. Evaluate whether\nthe model's prediction correctly answers the given\nquestion. The prediction is correct if it implies ANY\nof the ground-truth answers provided.\n- Question:\n{question}\n- Ground-truth Answers (The prediction is correct if it\nmatches ANY of these):\n{answer}\n- Prediction:\n{model_output}\nDoes the Prediction imply any of the Ground-truth Answers?\nRespond with a JSON object containing a single key \"judgment\"\nwith a value of \"Yes\" or \"No\".\nExample: {\"judgment\": \"Yes\"}\n\"\"\"\nC.2 Component-Level Quality Scoring\nThis section provides the prompt template used for\nourcomponent-level ablation study, with results\npresented in Table 3. Unlike the binary correct-\nness evaluation in Appendix C.1, this prompt is\ndesigned for a more nuanced quality assessment.\nIt instructs the LLM-as-Judge to evaluate the out-\nput of specific generative modules (i.e., Query De-\ncomposition and Query Refinement) and assign a\nquality score on a 1-to-5 Likert scale. This fine-\ngrained analysis allows us to isolate and measure\nthe efficacy of individual components within the\nFAIR-RAG pipeline.\nPROMPTS = {\n\"query_decomposition\": \"\"\"\nYou are an expert AI evaluator specializing in search\nand query analysis. Your task is to assess the quality\nof query decomposition.\nEvaluate the generated sub-queries based on the\noriginal user question using the following criteria:\n1. **Relevance:** How directly related is each\nsub-query to the main question?\n2. **Coverage:** Do the sub-queries collectively\ncover all essential aspects of the main question?\n3. **Efficiency:** Are the sub-queries concise,\nfocused, and well-formed for a search engine?\n[User Question]:\n\"{question}\"\n[Generated Sub-Queries]:\n{sub_queries}\nProvide your assessment in the following JSON format:\n{{\n\"score\": <A numeric score from 1.0 (Very Poor) to\n5.0 (Excellent) based on the criteria above>,\n\"reasoning\": \"<A very brief explanation for your\nscore>\"\n}}\n\"\"\",\n\"filter_efficacy\": \"\"\"\nYou are an expert auditor for an AI's document\nfiltering module. Your task is to meticulously evaluate\nthe filter's decisions by strictly adhering to the\n**exact instructions and principles** it was originally\ngiven.\n[User Question]:\n\"{question}\"\n[The Filter's Original Instructions & Principles]:\nThe filter's goal was to identify and discard \"Not\nUseful\" documents. It operated under the following\nrules:\n1. **Primary Principle:** **\"BE INCLUSIVE: When in\ndoubt, KEEP the document.\"**\n2. **Definition of \"Useful\":** A document is\nconsidered useful if it contains factual information\nabout the entities or topics in the query. **Even\npartial information is valuable.**\n3. **Definition of \"Not Useful\":** A document is\nonly \"Not Useful\" if it is about **completely\ndifferent** entities/topics or contains no relevant\ninformation.\n4. **Specific \"KEEP\" Criteria:** The filter was\nexplicitly instructed to **KEEP** documents containing:\n- The correct person/entity, even without\nall specific facts.\n- Biographical information (birth dates,\ncareer details, achievements).\n- Relationships or connections between\nqueried entities.\n- Specific facts relevant to the query type\n(e.g., dates).\n- General contextual information that helps\nunderstand the entities.\n5. **Specific \"REMOVE\" Criteria:** The filter was\ngiven examples of what to **REMOVE**:\n\n- Documents about different people with\nsimilar names.\n- Documents about completely unrelated\ntopics.\n- Duplicate documents.\nYour audit must strictly follow all of the same\nrules, especially the **\"BE INCLUSIVE\"** principle.\n[Documents the Filter KEPT]:\n{kept_docs}\n[Documents the Filter DISCARDED]:\n{discarded_docs}\n**Your Audit Task:**\nReferencing the filter's original instructions above,\nidentify its errors:\n1. **Precision Errors (Incorrectly Kept):** Review\nthe KEPT list. Identify the IDs of any documents that\nare **clearly \"Not Useful\"** and should have been\ndiscarded. If a document is borderline but meets any of\nthe \"KEEP\" criteria, the filter was **correct** to keep\nit.\n2. **Recall Errors (Incorrectly Discarded):** Review\nthe DISCARDED list. Identify the IDs of any documents\nthat were **unambiguously \"Useful\"** based on the\ncriteria and should have been kept.\nProvide your audit findings in the following strict\nJSON format. If no errors are found in a category,\nprovide an empty list.\n{{\n\"incorrectly_kept_ids\": [\"<ID of any'Not Useful'\ndocument found in the KEPT list>\", ...],\n\"incorrectly_discarded_ids\": [\"<ID of any\n'Useful'document found in the DISCARDED list>\", ...]\n}}\n\"\"\",\n\"sufficiency_check\": \"\"\"\n**Role:** You are a pragmatic and efficient QA\nEvaluator. Your goal is to determine if the provided\nevidence is \"good enough\" to satisfactorily answer the\nuser's question.\n**Core Task:**\nYour task is to assess if the main goal of the user's\nquestion can be achieved with the given evidence. You\nmust distinguish between \"critical\" missing information\nand \"nice-to-have\" details.\n**Guiding Principles:**\n1. **Focus on the Primary Intent:** First, identify\nthe core question(s) the user is asking. What is the\nmost important piece of information they are looking\nfor?\n2. **Assess Evidence Against Intent:** Check if the\nevidence contains the necessary facts to fulfill this\nprimary intent.\n3. **Pragmatism Rule:**\n- The evidence is **\"Sufficient\" (Yes)** if\nthe main question can be answered, even if peripheral\ndetails or deeper context is missing.\n- The evidence is **\"Insufficient\" (No)**\nonly if a **critical piece of information**, essential\nto forming the main answer, is absent.\n--- EXAMPLE ---\n**User Question:** \"What was the main outcome of the\nBattle of Badr and which year did it take place?\"\n**Evidence:**\n- \"The Battle of Badr was a decisive victory for the\nearly Muslims.\"\n- \"Key leaders of the Quraysh were defeated in the\nengagement.\"\n- \"The victory at Badr greatly strengthened the\npolitical and military position of the Islamic\ncommunity in Medina.\"\n**Your Analysis for Example:**\nThe evidence clearly confirms the \"main outcome\" (a\ndecisive victory for Muslims). However, a critical part\nof the question, \"which year did it take place?\", is\ncompletely missing from the evidence. Therefore, a\ncomplete answer cannot be formed.\n**Your Output for Example:**\n{{\n\"reasoning\": \"The evidence confirms the outcome of\nthe battle (a decisive victory) but a critical piece of\nrequested information, the year of the battle, is\ncompletely missing.\",\n\"is_sufficient\": false\n}}\n--- END OF EXAMPLE ---\nNow, apply this pragmatic logic to the following:\n[User Question]:\n\"{question}\"\n[Collected Evidence]:\n{evidence}\nProvide your final assessment in the following strict\nJSON format:\n{{\n\"reasoning\": \"<A brief analysis of what can be\nanswered and what critical information, if any, is\nstill missing.>\",\n\"is_sufficient\": <true or false>\n}}\n\"\"\",\n\"query_refinement\": \"\"\"\nYou are an expert AI systems evaluator. A RAG system\ndetermined its initial evidence was insufficient and\ngenerated new sub-queries to find missing information.\nYour task is to evaluate the quality of these new\nqueries.\n[User Question]:\n\"{question}\"\n[Insufficient Initial Evidence]:\n{evidence}\n[Newly Generated Sub-Queries for Refinement]:\n{new_queries}\nAssess how effectively the new sub-queries target the\ninformation gaps in the initial evidence to help answer\n\nthe main question.\nProvide your assessment in the following JSON format:\n{{\n\"score\": <A numeric score from 1.0 (Poorly\ntargeted) to 5.0 (Excellent, precisely targets gaps)>,\n\"reasoning\": \"<A very brief explanation for your\nscore>\"\n}}\n\"\"\",\n\"final_context_relevance\": \"\"\"\nYou are an expert information retrieval evaluator.\nYour task is to score the relevance of each document in\nthe final context that was used to generate an answer.\n[User Question]:\n\"{question}\"\n[Final Context Used for Generation\n(final_relevant_evidence)]:\n{final_evidence}\nFor each document in the final context, provide a\nrelevance score.\nProvide your assessment in the following JSON format:\n{{\n\"relevance_scores\": [\n{{ \"doc_id\": \"<_id of doc 1>\", \"score\":\n<numeric score from 1.0 (Irrelevant) to 5.0 (Highly\nRelevant)> }},\n{{ \"doc_id\": \"<_id of doc 2>\", \"score\":\n<numeric score from 1.0 (Irrelevant) to 5.0 (Highly\nRelevant)> }}\n]\n}}\n\"\"\",\n\"faithfulness\": \"\"\"\nYou are an expert in AI safety and fact-checking,\nspecializing in the evaluation of Retrieval-Augmented\nGeneration (RAG) systems. Your task is to evaluate the\nanswer's faithfulness to the provided evidence with\nnuance.\n- A faithful answer must be fully grounded in the\nprovided context. However, this does not mean it must\nbe a simple copy-paste of the text. **Valid synthesis,\nsummarization, and logical inference based *only* on\nthe provided information are considered faithful and\ndesirable.**\n- A statement is only considered **\"Unfaithful\"** if\nit introduces new, verifiable information that is\n**absent** from the context or if it **contradicts**\nthe context.\n[User Question (for context)]:\n\"{question}\"\n[Provided Context (final_relevant_evidence)]:\n{final_evidence}\n[Generated Answer]:\n\"{final_answer}\"\n**Your Task:**\n1. Analyze each claim within the [Generated Answer].\n2. For each claim, determine if it is directly\nstated, a valid synthesis/inference from the context,\nor an unfaithful statement (introducing new facts).\n3. Based on this analysis, provide an overall verdict\naccording to the rubric below.\n**Verdict Rubric:**\n- **'Fully Faithful'**: All claims in the answer are\neither directly stated in the context or are valid\nlogical conclusions/summaries derived *only* from the\ninformation present in the context.\n- **'Partially Faithful'**: The answer is mostly\nfaithful, but contains minor, non-critical claims or\ndetails that cannot be inferred from the context.\n- **'Not Faithful'**: The answer contains significant\nor central factual claims that are not supported by, or\nactively contradict, the context.\nProvide your verdict in the following strict JSON\nformat:\n{{\n\"faithfulness_verdict\": \"<One of three strings:\n'Fully Faithful','Partially Faithful', or'Not\nFaithful'>\",\n\"reasoning\": \"<If not fully faithful, specify\nwhich claims in the answer are unsupported by the\ncontext. Explain if it's an invalid inference or a\ncompletely new fact.>\"\n}}\n\"\"\",\n\"iterative_improvement\": \"\"\"\nYou are an expert AI quality evaluator. For a single\nquestion, you are given four answers generated by the\nsame system but with different levels of iterative\nrefinement (1, 2, 3, and 4 iterations). Your task is to\nrank these answers from best to worst.\n[User Question]:\n\"{question}\"\n[Answer from 1 Iteration (iter_1)]:\n\"{answer_1}\"\n[Answer from 2 Iterations (iter_2)]:\n\"{answer_2}\"\n[Answer from 3 Iterations (iter_3)]:\n\"{answer_3}\"\n[Answer from 4 Iterations (iter_4)]:\n\"{answer_4}\"\nRank these three answers from best (Rank 1) to worst\n(Rank 4).\nProvide your ranking in the following JSON format:\n{{\n\"ranking\": [\"<ID of the best answer, e.g.,\n'iter_3'>\", \"<ID of the second-best answer, e.g.,\n'iter_4'>\", \"<ID of the third-best answer, e.g.,\n'iter_2'>\",\"<ID of the worst answer, e.g.,'iter_1'>\"],\n\"reasoning\": \"<A very brief explanation for your\nranking, noting whether more iterations led to a clear\nimprovement>\"\n\n}}\n\"\"\"\n}\nD Failure Mode Analysis Prompt\nThis appendix details the prompts engineered for\nour LLM-assisted failure mode analysis, as de-\nscribed in Section 5.2.4. To ensure a systematic\nand reproducible evaluation, we designed a two-\npart prompt structure. The PROMPT_SYSTEM\nprompt establishes the LLM\u2019s persona as an \u201cex-\npert evaluation researcher\u201d and enforces a strict\nJSON output schema based on a predefined fail-\nure taxonomy. The PROMPT_USER_TEMPLATE\nthen provides the data batch and requires the model\nto ground its diagnosis in specific evidence from\nthe logs, ensuring each classification is structured,\nexplainable, and actionable.\nFAILURE_ANALYSIS_PROMPT = \"\"\"\n**ROLE:** You are an expert RAG (Retrieval-Augmented Generation)\nsystem diagnostician. Your task is to perform a meticulous root\ncause analysis on a failed query-answer pair from an advanced,\niterative RAG system.\n**CONTEXT:** The system has already produced an answer that was\ngraded as incorrect. You have been given the complete execution\ntrace for this failed sample. Your goal is to identify the single,\nprimary point of failure within the RAG pipeline.\n**FAILURE CATEGORIES:**\nYou must classify the failure into one of the following six\ncategories. Read these definitions carefully.\n1. **Query Decomposition Error:** The initial user question was\nnot broken down into effective, specific sub-queries. The\nsub-queries were irrelevant, missed key aspects of the original\nquestion, or sent the retrieval process in the wrong direction\nfrom the very beginning.\n2. **Retrieval Failure:** The retriever, despite having\nwell-formed sub-queries, failed to find and return the relevant\ndocuments from the knowledge base. The correct information was\nsimply not present in the`[All Retrieved Documents (Unfiltered)]`\nset.\n3. **Evidence Filtering Error:** The correct information WAS\nsuccessfully retrieved by the retriever, but the subsequent\nfiltering/reranking step mistakenly discarded the crucial\ndocuments. Look for correct information in`[Discarded Documents]`\nthat should have been kept.\n4. **SEA Error (Strategic Analyst Error):** The system's\n'Strategic Intelligence Analyst'module failed in its reasoning.\nThis can manifest in several ways:\n- **A) Flawed Deconstruction:** The'Required Findings'\nchecklist in its analysis was incorrect or missed the main\npoint of the user's question.\n- **B) Faulty Analysis:** The module failed to make a correct\nlogical inference from the evidence, hallucinated a'Confirmed\nFinding'that wasn't supported, or incorrectly identified the\n'Remaining Gaps'.\n- **C) Contradictory Verdict:** The detailed analysis pointed\nto missing information, but the final verdict was mistakenly\n'Sufficient: Yes', causing the system to stop searching\nprematurely.\n5. **Query Refinement Error:** After correctly identifying that\nthe initial evidence was insufficient, the system failed to\ngenerate effective new sub-queries to target the specific\ninformation gaps. The new queries were redundant, vague, or did\nnot address the missing pieces identified by the SEA module.\n6. **Generation Failure:** All preceding steps worked correctly.\nThe final set of evidence (`[Final Relevant Evidence]`) contained\nall the necessary information to form a correct answer. However,\nthe language model failed during the final synthesis step by\nhallucinating, making incorrect logical inferences, or\nmisinterpreting the provided evidence.\n**PRIMARY FAILURE RULE:**\nIdentify the **earliest, most fundamental error** in the pipeline.\nFor example, if Retrieval failed to find good documents, the\nGeneration will also fail, but the root cause is **Retrieval\nFailure**.\n**EXECUTION TRACE FOR ANALYSIS:**\n[User Question]:\n\"{question}\"\n[Ground Truth Answer (The correct answer)]:\n\"{ground_truth_answer}\"\n[Generated (Incorrect) Answer]:\n\"{final_answer}\"\n--- RAG Pipeline Details ---\n[Initial Sub-Queries Generated]:\n{sub_queries}\n[All Retrieved Documents (Unfiltered)]:\n{all_retrieved_docs}\n[Discarded Documents (By Filter)]:\n{discarded_docs}\n[Final Relevant Evidence (Used for Generation)]:\n{final_evidence}\n--- Iteration Reports ---\n{iteration_reports_formatted}\n--- YOUR TASK ---\nBased on all the provided information and adhering strictly to\nthe definitions, provide your analysis in the following JSON format.\n{\n\"failure_category\": \"<The value for this key MUST be one of the\nfollowing exact strings:'Query Decomposition Error','Retrieval\nFailure','Evidence Filtering Error','SEA Error','Query\nRefinement Error','Generation Failure'. Do NOT add any extra\ntext or explanations.>\",\n\"reasoning\": \"<Provide a concise, step-by-step justification\nfor your choice of category. Reference specific parts of the\nexecution trace (e.g.,'The SEA module's analysis incorrectly\nstated it confirmed the actor's birth year, but the evidence\nonly mentioned their nationality').>\",\n\"root_cause_analysis\": \"<Go one level deeper. Why did this\nerror likely happen? (e.g.,'The SEA prompt might be too\ncomplex, leading to reasoning errors,'or'The filtering model\nmay be poorly calibrated for short documents').>\",\n\"suggested_improvement\": \"<Propose a concrete, actionable\nsolution to fix or mitigate this specific type of error in the\nfuture. (e.g.,'Simplify the SEA prompt by removing the persona\nand focusing on a checklist,'or'Fine-tune the reranker with\nmore examples of this type').>\"\n}\n\"\"\"",
  "full_text_length": 123882,
  "link_pdf": "https://arxiv.org/pdf/2510.22344v1",
  "paper_id": "2510.22344v1"
}