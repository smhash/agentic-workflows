{
  "source": "arxiv",
  "query": "Agentic RAG",
  "fetched_at": "2025-11-21T17:17:52.343738",
  "title": "Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation",
  "url": "http://arxiv.org/abs/2510.25518v1",
  "content": "Retrieval Augmented Generation (RAG) for\nFintech: Agentic Design and Evaluation\n1,3Thomas Cook\u2217, 2,3Richard Osuagwu\u2217, 1,3Liman Tsatiashvili\u2217, 4,3Vrynsia Vrynsia\u2217,\n3Koustav Ghosal, 3Maraim Masoud, 3Riccardo Mattivi\n1TU Dublin, Dublin, Ireland\n2Maynooth University, Ireland\n3Mastercard, Ireland\n4National College of Ireland, Ireland\n{firstname.lastname}@mastercard.com\nAbstract\u2014Retrieval-Augmented Generation (RAG) systems of-\nten face limitations in specialized domains such as fintech, where\ndomain-specific ontologies, dense terminology, and acronyms\ncomplicate effective retrieval and synthesis. This paper introduces\nan agentic RAG architecture designed to address these challenges\nthrough a modular pipeline of specialised agents. The proposed\nsystem supports intelligent query reformulation, iterative sub-\nquery decomposition guided by keyphrase extraction, contextual\nacronym resolution, and cross-encoder-based context re-ranking.\nWe evaluate our approach against a standard RAG baseline\nusing a curated dataset of 85 question\u2013answer\u2013reference triples\nderived from an enterprise fintech knowledge base. Experimental\nresults demonstrate that the agentic RAG system outperforms\nthe baseline in retrieval precision and relevance, albeit with\nincreased latency. These findings suggest that structured, multi-\nagent methodologies offer a promising direction for enhancing\nretrieval robustness in complex, domain-specific settings.\nIndex Terms\u2014Retrieval Augmented Generation, Agentic AI,\nFintech, Natural Language Processing, Knowledge Base, Domain-\nSpecific Ontology, Query Understanding\nI. INTRODUCTION\nRetrieval Augmented Generation (RAG) combines large\nlanguage models (LLMs) with external document retrieval,\nallowing models to access information beyond their training\ndata. These systems have shown impressive results in general-\npurpose applications such as technical support [1], coding\nassistants [2], and document summarization [3], especially\nwhen operating as proof-of-concepts on publicly available or\nloosely structured data. However, deploying such systems at\nscale in highly specialized and tightly regulated domains such\nas financial technology is far from straightforward [4], [5].\nFintech-specific use cases often involvestructured-\nunstructured data(or tightly regulated free text), such as\nproprietary knowledge bases, role-based access to information,\nand stringent compliance constraints [6], all of which make\nit difficult to repurpose existing tools and methodologies\ndesigned for open-domain settings or Software-as-a-Service\n(SaaS)-based environments. These complexities are both tech-\nnical and organizational. Regulatory restrictions prohibit data\n*Work done during internship at Mastercard, Ireland\nFig. 1. Word cloud illustrating the distribution of internal knowledge artifacts\nin fintech. The prominence of terms such as \u201cfeature,\u201d \u201cstatus,\u201d \u201cmodel,\u201d and\n\u201cAPI\u201d reflects the operational focus of internal documentation\u2014often centered\naround technical specifications, product state, and integration interfaces. This\nconcentration of tightly scoped, semi-structured information highlights the\nchallenge of designing RAG systems that can interpret fragmented context\nacross teams and tools, where standard SaaS-based approaches fall short due\nto regulatory and organizational constraints.\nfrom leaving organisational boundaries, making cloud-hosted\nAPIs or third-party evaluation platforms unsuitable for most\npractical deployments. In addition, often fintech firms follow\na compartmentalized structure with clear divisions of respon-\nsibility between product managers, analysts, compliance offi-\ncers, and engineers. This further complicates how knowledge\nis captured, retrieved, and applied.\nFor example, consider a product manager in Mastercard\nanalyzing feature alignment across offerings such as MDES\n(Mastercard Digital Enablement Service) 1, M4M (Mastercard\nfor Merchants) [7], and SwitchCore (Mastercard\u2019s transaction\nswitching platform) [8]. Internal knowledge sources\u2014ranging\nfrom note-taking apps, product management platforms and\narchitecture decks to regulatory compliance PDFs\u2014often use\nacronyms inconsistently or assume implicit context based on\nteam-specific language. This forces the user to manually piece\ntogether fragmented information to understand interoperabil-\n1https://www.mastercard.com/newsroom/press/news-briefs/mastercard-dig\nital-enablement-service-mdes, Accessed: 2025-06-25\narXiv:2510.25518v1  [cs.AI]  29 Oct 2025\n\nity constraints, roadmap alignment, or API-level integration\nguidelines. A standard RAG system typically underperforms\nin such scenarios, frequently misinterpreting short forms like\n\u201cCMA\u201d (which could mean \u201cConsumer Management Appli-\ncation\u201d or \u201cCardholder Management Architecture\u201d depending\non the context), or retrieving documents based on keyword\noverlap rather than intent.\nAnother challenge with such domain-specific use-cases for\nlarge enterprises is evaluation. Traditional RAG benchmarks\nassume public datasets, crowd-sourced relevance judgments,\nor static ground truth\u2014all of which are infeasible in fintech.\nFinding subject-matter experts to annotate queries at scale is\nboth costly and time-consuming. Moreover, regulations around\nconfidentiality and data residency prevent the use of crowd\nplatforms for human evaluation.\nTherefore, designing on-prem, domain-specific RAG ap-\nplications requires several considerations and not limited to\n\u2014 a careful rethinking of architecture, privacy guarantees, a\nstructured retrieval and reasoning process capable of adapting\nto ambiguity (especially when the source content is internal,\nfragmented, and often acronym-heavy). In this work, we focus\non some of these core challenges and propose an agent design\ntailored to domain-specific retrieval and reasoning. We also\npresent evaluation methodologies that are secure, reproducible,\nand feasible at scale within enterprise constraints.\nWe propose a hierarchical agent framework, where an\nOrchestrator Agentdelegates tasks to specialized sub-\nagents responsible for acronym resolution, domain-aware\nquery rewriting, and cross-encoder-based re-ranking. This\nmodular structure allows the system to encode organizational\nidiosyncrasies such as internal taxonomies, access controls,\nand documentation practices, significantly improving the\nquality of answers retrieved. In addition, we develop a\nsemi-automated evaluation strategy that combines LLM-as-a-\njudge [9] paradigms with constrained prompt templates and\nhuman-in-the-loop validation. Inspired by recent work on\nsynthetic evaluation datasets [9], [10], we leverage internal\nknowledge bases to generate realistic query-answer pairs,\nwhich are then assessed using a mix of LLM scoring and\nmanual spot-checking. This allows for a secure, efficient, and\nconsistent evaluation process that scales across teams and\ndata silos.\nTo summarize, our primary contributions include:\n\u2022A Fintech-focused Agentic Design:We develop an\nOrchestrator Agentthat coordinates specialized agents for\nacronym resolution, iterative sub-query generation using\nkey phrases, and context refinement through a cross-\nencoder re-ranking method.\n\u2022Automatic Query Enhancement with Continuous\nFeedback:Our system proactively identifies and resolves\ndomain-specific acronyms within user queries and re-\ntrieved content, enhancing retrieval accuracy and answer\ncompleteness.\n\u2022Thorough Evaluation Methodology:We construct an\nevaluation dataset from an enterprise knowledge base,\nleveraging LLM-driven synthetic data generation and\nmanual validation to ensure high-quality, contextually\nrelevant questions.\n\u2022Empirical Comparative Analysis:We quantitatively and\nqualitatively compare our agentic approach against a\nstandard RAG baseline, measuring effectiveness through\nmetrics such as retrieval accuracy and answer relevance.\nThe remainder of this paper is organized as follows. Sec-\ntion II reviews relevant work in RAG, multi-agent pipelines,\nand domain-specific retrieval systems. Section III describes\nour methodology, including the design of the baseline and\nagentic RAG architectures. Section IV details the knowledge\nbase preparation and the construction of the evaluation dataset.\nSection V presents the experimental setup and results, compar-\ning retrieval accuracy and latency across configurations. Sec-\ntion VI provides a qualitative error analysis and discussion of\nobserved performance patterns. Finally, Section VII concludes\nthe paper and outlines directions for future work.\nII. RELATEDWORK\nAgentic RAG:Recent work has increasingly explored en-\nhancing Retrieval-Augmented Generation (RAG) systems with\nstructured, multi-agent architectures for complex information\ntasks. Agentic RAG integrates autonomous AI agents into the\npipeline, enabling dynamic query decomposition and iterative\nreasoning. Singhet al.introduce the concept of Agentic RAG,\nwhere agents perform planning, reflection, and tool use to\nrefine context and retrieval [11]. Similarly, Nguyenet al.\npropose MA-RAG, a modular, training-free framework where\nagents (e.g., Planner, Extractor, QA) collaboratively process\nquestions using chain-of-thought reasoning [12]. Their results\nshow that this structure improves answer quality and resolves\nambiguities without fine-tuning. The Pathway team further\nobserves that domain-adaptive agents enhance performance by\nassigning each agent task-relevant expertise [12], [13].\nDomain-specific RAG:Deploying RAG in specialized do-\nmains such as finance, healthcare, and law requires grounding\nretrieval in domain knowledge. Recent studies emphasize\nusing structured ontologies to enhance LLM performance.\nBarronet al.introduce the SMART-SLIC framework, com-\nbining knowledge graphs (KGs) and vector stores customized\nfor a domain to support retrieval [14]. They demonstrate\nthat referencing KG ontologies improves QA by aligning\nretrieval with relevant subgraph structures. Their approach\nfuses structured (KG) and unstructured (text) sources to reduce\nhallucinations and improve citation fidelity [14]. However,\nthey also highlight the resource demands of curating such\ndomain-specific infrastructure. Other studies similarly confirm\nthat structured knowledge sources boost retrieval precision in\nspecialized settings such as legal and medical QA [11], [14].\nLLM Applications in Fintech:Large language models\nhave found wide adoption in fintech, powering analytics and\nautomation across customer support, fraud detection, risk\nmodeling, and compliance. Daiya surveys how LLMs monitor\nfinancial risks by processing vast unstructured data, detecting\n\npatterns, and forecasting threats from logs, news, and com-\nmunications [15]. Broughton highlights their role in fraud\ndetection, credit scoring, and automating regulatory tasks like\ndocument parsing and report generation [16]. These studies\nagree that LLMs extract actionable insights from diverse\nfinancial sources, whether through sentiment analysis, trading\nsignal identification, or customer feedback mining [15], [16].\nThese applications underscore the need for domain-aware\nRAG to enhance factual accuracy in high-stakes financial\ncontexts.\nRAG for Fintech:Within fintech, RAG systems are be-\ning prototyped for data-driven assistants and analytics tools.\nLumenova AI reports growing adoption of RAG-backed chat-\nbots that integrate live market and account data to improve\ncustomer interaction [17]. A notable example is Bank of\nAmerica\u2019s \u201cErica,\u201d which uses LLMs and real-time retrieval to\nsupport over a billion user interactions [17]. Hernandez Leal\ndemonstrates a RAG-powered assistant trained on SEC filings\nto answer investor questions from 10-K and 10-Q reports [18].\nThese examples show how RAG can anchor LLM outputs in\nauthoritative financial sources, enhancing both precision and\ncitation. While still in early stages, these systems suggest that\nreal-time, curated retrieval pipelines significantly boost LLM\nutility and trustworthiness in financial applications [17], [18].\nIII. AGENTDESIGN\nBuilding on the challenges introduced in Section I, this\nsection details how we first implemented a Baseline RAG\npipeline to explore retrieval limitations in fintech settings,\nand subsequently refined the design into a modular agentic\narchitecture.\nA. Baseline RAG System\nThe Baseline RAG system, which will be referred to as\n\"B-RAG\" for the remainder of this work, represents a stan-\ndard retrieval pipeline that serves as a comparison point for\nour agentic architecture. This approach follows a sequential\nprocess, as illustrated in Figure 2.\nThe B-RAG system, illustrated in Figure 2, consists of a\nsequence of specialized agents structured to enable stepwise\ninformation retrieval and synthesis. The process begins with\nthe user\u2019s initial input, which serves as the contextual founda-\ntion for subsequent components. This input is then propagated\nthrough the following agents:\n(a)Query Reformulator Agent: Takes the user\u2019s question\nand refines it into a concise, keyword-focused query\noptimized for retrieval. It uses predefined prompt tem-\nplates to detect if the query is a continuation of previous\ninteractions or a standalone new topic.\n(b)Retriever Agent: Executes a single-pass retrieval from the\nknowledge base with cosine similarity between embed-\ndings to locate relevant document chunks.\n(c)Summary Agent: Aggregates retrieved document chunks\ninto a coherent answer. It is explicitly instructed to\navoid generating information not present in the retrieved\nFig. 2. Overview of the B-RAG pipeline. The process follows a linear\nflow beginning with the user\u2019s initial query, followed by query reformulation,\nsingle-pass retrieval, summarization, and response generation.\ncontext, clearly indicating if the retrieved content lacks\nsufficient information.\nFinally, the output of the summary agent is presented as a\ncoherent response, supported by the reference links.\nThe B-RAG pipeline serves as a reference due to its simplified\narchitecture, which allows us to conduct clear and interpretable\nperformance benchmarking against the more complex agentic\nsystem. However, this simplicity imposes several notable lim-\nitations:\n\u2022Lack of multi-pass retrieval logic: The system performs a\nsingle retrieval pass without iterative refinement, thereby\nconstraining its capacity to effectively address complex\nqueries that require deeper exploration.\n\u2022Absence of sub-query decomposition: The pipeline is un-\nable to partition broad or ambiguous queries into multiple\ntargeted sub-queries. This limitation results into dimin-\nished precision when questions span multiple document\nfragments.\n\u2022No acronym resolution mechanism: The system does not\nresolve domain-specific acronyms frequently encountered\nin enterprise context, which may adversely affect the\nclarity and accuracy of retrieved information.\n\u2022Absence of document re-ranking: Retrieved results are uti-\nlized without further subsequent validation or refinement\nthrough cross-encoder-based re-ranking. This potentially\nmay compromise the relevance of the final responses.\nDespite these limitations, the simple design of the baseline\npipeline offers some advantages, including ease of deploy-\nment, lower computational overhead, and enhanced inter-\npretability. These attributes make it an effective benchmark\nfor systematically evaluating the incremental improvements\nintroduced by each advanced component of the proposed\nagentic architecture.\nB. Proposed Refined Design\nThe Agentic RAG system, which will be referred to as\n\"A-RAG\" throughout this paper, is designed to address the\n\nlimitations inherent in the B-RAG system by integrating\nmodular intelligence and task-specific specialization. Central\nto A-RAG is theOrchestrator Agent, which coordinates a suite\nof specialized agents, each tasked with a distinct retrieval or\nsynthesis function. This design is inspired by an investigative\nresearch workflow. It initiates with a direct attempt to address\nthe user query, subsequently evaluates the output, and when\nnecessary, activates more focused and iterative exploration.\nTo address challenges such as ambiguous terminology, frag-\nmented sources, and answer confidence, A-RAG incorporates\nseveral key capabilities, including acronym resolution, sub-\nquery generation, parallel retrieval, re-ranking, and quality\nassessment. The quality of generated answers is evaluated\nby a dedicated QA agent whose confidence score determines\nwhether further iterative refinement is required. A comparison\nof the hybrid pipeline architectures for B-RAG and A-RAG\nworkflows is shown in Figure 3. It highlights the differences\nin processing flow and iterative mechanisms.\nC. System Architecture and Workflow\nThe A-RAG system comprises a set of lightweight, modular\nagents orchestrated to support iterative retrieval and reasoning.\nThe following subsections describe the system\u2019s core agent\ncomponents and the operational workflow using a representa-\ntive fintech query example.\nCore Agent Components:The orchestrator coordinates eight\nspecialized agents, each responsible for a distinct stage in the\nretrieval\u2013reasoning cycle:\n1)Intent Classifier\u2013 determines whether the user input\nrequires(i) retrieval, which involves fetching new context\nfrom the knowledge base, or(ii) summary, which com-\npresses existing conversational history.\n2)Query Reformulator\u2013 transforms the raw query into a\ndense, keyword-optimized search string by removing func-\ntion words, expanding recognized acronyms, and injecting\ndomain-specific synonyms to maximize embedding-based\nrecall.\n3)Retriever Manager & Retriever Agent\u2013 launch one\nor more vector store queries\u2014executed in parallel if sub-\nqueries exist\u2014and return the top-kmost relevant chunks.\n4)Sub-Query Generator\u2013 in cases of low initial retrieval\nscores, identifies 2\u20133 key entities or phrases to construct\ntargeted follow-up queries.\n5)Re-Ranker Agent\u2013 reorders the retrieved chunks using\na cross-encoder to prioritize those with higher semantic\nalignment to the query.\n6)Summary Agent\u2013 fuses the ranked context snippets into\na concise, citation-style answer composed strictly from the\nretrieved content.\n7)QA Agent\u2013 evaluates the synthesized answer on a 0\u201310\nscale, assessing relevance and support from context; the\nscore determines whether the pipeline concludes or pro-\nceeds with refinement.\n8)Acronym Resolution Logic\u2013 manages a local glossary\nand, when invoked, appends in-line definitions to prevent\nFig. 3. Comparison of hybrid pipeline architectures for B-RAG and A-\nRAG workflows. The left panel shows B-RAG\u2019s single-pass process, including\nquery reformulation, retrieval, answer synthesis, and output generation without\niterative refinement. The right panel illustrates A-RAG\u2019s extended pipeline,\nfeaturing acronym resolution, sub-query generation, document re-ranking,\nand an answer quality assessment (QA) agent. If the QA agent assigns low\nconfidence to the synthesized answer (e.g., below a set threshold), a feedback\nloop triggers sub-query generation to iteratively expand and refine retrieval.\ndownstream agents from misinterpreting shorthand expres-\nsions.\nWorkflow Overview:The operational workflow integrates\nthe agent components into a cohesive, adaptive pipeline,\nillustrated with the example query:\u201cHow is CVaR calculated\nin the IRRBB framework?\u201dThe process begins with query\nreformulation and acronym expansion, ensuring clarity and\ndisambiguation of terms likeCVaRandIRRBB. A first-pass\nvector retrieval fetches relevant document chunks, which are\nsynthesized into an initial answer. If the QA agent assigns\na low confidence score, the system triggers refinement: sub-\nqueries such as \u201cCVaR formula\u201d or \u201cIRRBB risk quantifi-\ncation\u201d are generated, results are re-ranked, and a revised\nsynthesis is attempted. Should this prove inadequate, a broader\nretrieval sweep is conducted to increase coverage. In the\nabsence of a sufficiently confident answer, the system transpar-\nently communicates its uncertainty to the user. This adaptive\n\npipeline dynamically balances computational efficiency with\nretrieval depth based on answer confidence.\nIV. DATA ANDEVALUATIONSTRATEGY\nIn Section I, we described the challenge of designing\nretrieval systems that can operate effectively over fragmented\nand acronym-heavy enterprise knowledge bases. This section\ndetails how we constructed the knowledge corpus and gener-\nated an evaluation set tailored to this context.\nA. Knowledge Base Preparation\nThe knowledge base was derived from proprietary internal\ndocuments exported in structured markup format. A custom\npipeline was developed to preprocess the data. This process\nfocused on:\n\u2022Converting structured elements such as tables and code\nblocks into linear, plain-text representations while pre-\nserving semantic relationships.\n\u2022Removing formatting noise and markup artifacts to pro-\nduce clean, model-ready text.\nTo enable retrieval, the documents were segmented into\noverlapping chunks (to preserve contextual coherence across\nboundaries), embedded with a publicly available sentence\ntransformer, and stored in a vector index.\nThe resulting corpus consisted of over 30,000 text chunks\nrepresenting 1,624 unique documents. On average, each docu-\nment was split into approximately 19 chunks. Figure 4 shows\nthe distribution of chunk lengths measured by word count.\nFig. 4. Distribution of chunk lengths by word count. The majority of chunks\nare between 50 and 120 words, showing the trade-off between retrieval\ngranularity and contextual coherence.\nTo further illustrate the dataset composition, Figure 1 and\nFigure 5 provide complementary views of term frequency.\nBoth highlight the prevalence of domain-specific language in\nthe internal fintech corpus, particularly terms such as \u201cfea-\nture,\u201d \u201cAPI,\u201d and \u201cmodel,\u201d reflecting its emphasis on product,\ntechnical, and compliance-related content.\nB. Evaluation Set Generation\nInspired by the LLM-as-a-judge paradigm [9], we designed\na multi-phase, model-assisted pipeline to create a high-quality\nevaluation set. This approach is motivated by the constraints\noutlined in Section I, particularly the impracticality of relying\nFig. 5. Top 20 most frequent terms in the corpus, ranked by raw frequency.\nsolely on manual annotation within confidential enterprise\nenvironments.\nFirst, we randomly sampled a subset of chunks from the cu-\nrated corpus. For each chunk, a language model was prompted\nto generate a question-answer pair grounded in the provided\ncontext. Two distinct prompt templates were used; one tailored\nto narrative text and the other to structured elements such as\ntables. This distinction is made to ensure both content diversity\nand fidelity to the source material.\nSubsequently, each generated question\u2013answer pair under-\nwent a quality control phase in which a language model as-\nsessed contextual alignment, factual accuracy, and coherence.\nTo standardize this evaluation, we defined three criteria that\neach pair was required to meet:\n\u2022Specificity:Is the question clearly and narrowly scoped\nto a particular aspect of the context?\n\u2022Faithfulness:Is the answer grounded exclusively in the\nprovided source text, without introducing external infor-\nmation?\n\u2022Completeness:Does the answer fully and directly ad-\ndress the question posed?\nOnly candidate pairs that satisfied all three criteria were\nretained. These pairs then underwent an additional round\nof manual review to eliminate any remaining edge cases\nor ambiguous examples. Finally, the resulting evaluation set\nconsisted of 85 validated question-answer pairs, each traceable\nto a unique document chunk. This dataset served as the\nfoundation for the quantitative and qualitative evaluations\npresented in Sections V-B, V-C.\nC. Human-Verified Prompt\u2013Answer Benchmark\nIn addition to the automated evaluation set described above,\nwe constructed a manually curated benchmark designed to\nstress-test both retrieval fidelity and answer ranking. The\n\nbenchmark construction process involved the following key\nsteps:\nQuestion sourcing:Candidate questions were extracted\nfrom routine activities within the internalknowledge base,\nincluding onboarding runbooks, intern reports, and project or\nproduct wikis.\nHuman validation of answers:For each question, human\nannotators identified all relevant documentation pages and\nselected one representative chunk per page as a potential\nanswer. Among these, the most comprehensive chunk was\ndesignated as the ground truth. This setup reflects a \u201cone\ncorrect, many plausible\u201d retrieval scenario.\nIntent categorization:Each query was labeled according\nto an intent taxonomy\u2014Procedural,Definitional, orAcronym\nExpansion\u2014to support stratified analysis, as discussed in\nSection V-C.\nDataset composition:The final benchmark comprises\n27 question\u2013answer pairs, each corresponding to a distinct\nquery with multiple associated ground-truth answers: seven\nprocedural, fourteen definitional, and six acronym-expansion\nexamples.\nThis human-verified dataset was used to (i) compute strict\nretrieval metrics (Hit@k), and (ii) support error analysis in\nthe presence of multiple valid answers. For each item, we\nlog the following fields:Question,Category,Ground-truth\nAnswer(s),Ground-truth Source(s),Generated Answer, and\nRetrieved Source(s).\nV. EXPERIMENTS ANDRESULTS\nA. Experimental Setup\nWe compared B-RAG and A-RAG pipelines on a special-\nized internal fintech knowledge base. Both pipelines used the\nsame LLM backend,Llama-3.1-8B-Instruct, served\nvia a vLLM endpoint. Document embeddings were generated\nusing theall-MiniLM-L6-v2model and stored in Chro-\nmaDB. We evaluated both systems using two key metrics: (1)\nRetrieval Accuracy (Hit Rate @5), defined as the percentage\nof questions for which the correct document source appeared\namong the top five retrieved links; and (2) Average Latency,\ndefined as the time from user query submission to the final\nsystem response.\nB. Quantitative Results\nTable I summarizes the results. The A-RAG system\nachieved a strict retrieval accuracy of 62.35%, outperforming\nthe Baseline\u2019s 54.12%. This performance gain can be attributed\nto the system\u2019s specialized agents for acronym resolution\nand sub-query expansion, which were designed to address\nthe fragmented and semantically sparse nature of enterprise\nknowledge described in Section I. As expected, A-RAG\u2019s\naverage query latency was 5.02 seconds, significantly higher\nthan B-RAG\u2019s 0.79 seconds, due to multi-stage processing and\niterative document re-ranking.\nTABLE I\nPERFORMANCECOMPARISON OFB-RAGANDA-RAG SYSTEMS\nMetric B-RAG A-RAG\nTotal Questions Evaluated 85 85\nRetrieval Accuracy (Hit @5, %) 54.12 62.35\nAvg. Latency per Query (s) 0.79 5.02\na) Adjusted Retrieval Interpretation:Although retrieval\nwas evaluated using exact link matches, manual inspection\nidentified several queries where the system returned correct\ncontent from semantically equivalent but non-identical doc-\numents. Specifically, A-RAG retrieved valid answers from\nalternate sources in six additional cases, and B-RAG in three.\nIncorporating these, the adjusted retrieval accuracy increases\nto 69.41% for A-RAG and 58.82% for B-RAG. These cases\nreinforce the core challenge introduced in Section I; that\nenterprise knowledge in fintech domains is often fragmented,\nwith key information distributed across related documents.\nUnlike B-RAG, which often fails when the exact match\nis missing, A-RAG\u2019s sub-query generation and iterative re-\nranking modules better synthesize partial context across\nsources, producing correct responses even when the originat-\ning chunk is not directly retrieved. In the next section we\nevaluate answer quality usingsemantic accuracy, defined as\nthe mean LLM-judge score measuring semantic equivalence\nbetween a system\u2019s answer and the human ground-truth an-\nswer, independent of surface lexical overlap.\nC. Answer-Quality Evaluation (Semantic Accuracy)\nBuilding on the retrieval metrics in Table I (Section V-B),\nwe next measure how faithfully each pipeline answers user\nquestions. Rather than lexical overlap, we targetsemantic\nagreement with the ground-truth answers created in Sec-\ntion IV.\nWe define the following metric to calculate the semantic\naccuracy. LetNdenote the total number of evaluated questions\n(hereN= 85). For every questionq i, an external vLLM-\nhosted judge (Llama-3.1-8B) assigns an integer score\nsi \u2208 {1, . . . ,10}using the rubric in Table II. The semantic-\naccuracy metric is the mean judge score:\n\u00afs= 1\nN\nNX\ni=1\nsi.\nTABLE II\nLLM-JUDGE RUBRIC(SEMANTIC ACCURACY).\nScore Interpretation\n9\u201310 Exact match or perfect paraphrase\n6\u20138 Correct but missing minor detail\n3\u20135 Honest refusal / incomplete answer\n1\u20132 Incorrect or hallucinated\nApplying this metric to our evaluation set reveals that the\nA-RAG pipeline outperforms the baseline B-RAG. A-RAG\n\nachieves a mean score of\u00afs= 7.04, compared to\u00afs= 6.35\nfor B-RAG, yielding a performance gain of\u2206\u00afs\u22480.68.\nAs shown in Figure 6, A-RAG notably reduces the fre-\nquency of low-quality answers (s<5) and increases the pro-\nportion of responses in the top rubric tier (s\u22659). The per-\nquestion score deltas (Figure 7) show that A-RAG is preferred\nin 64 % of cases, ties in 25 % and is outperformed by B-RAG\nin only 11 %. Even in those rare cases, the largest drop is\njust 3 points, indicating that A-RAG rarely degrades answer\nquality.\nFig. 6. Histogram+KDE of all 85 LLM-judge scores per pipeline. A-RAG\nshifts mass into the 7\u201310 band, halves low-quality answers (s <5falls from\n18% to 8%), and increases \u201cexcellent\u201d (s\u22659) responses from 12% to 22%.\nFig. 7. Per-question score difference\u2206s=s A-RAG \u2212s B-RAG. Distribution\ncenters at+1(median) with 64% of points>0, 25% at 0, and only 11%\n<0; extreme negative swings never exceed\u22123, indicating A-RAG rarely\ndegrades answer quality.\nD. Human-Curated Benchmark\nWe further evaluate both systems using a human-curated\nbenchmark consisting of 17 questions, each designed to allow\nmultiple plausible answers. The benchmark comprises 9 defini-\ntional questions, 4 procedural questions, and 4 acronym-based\nquestions. Collectively, these questions are associated with\n33 distinct ground-truth source links. In addition to assessing\nsemantic accuracy, we introduce coverage as a complementary\nmetric:\nCoverage = |R|\n|G| ,\nwhereGis the set of 33 distinct ground-truth source links\nandRthe subset retrieved within the top\u20135 across all ques-\ntions. B-RAG achieves 66.67% (22/33), while A-RAG attains\n69.70% (23/33). This result indicates a modest advantage in\naggregating distributed evidence.\nTABLE III\nHUMAN-CURATED BENCHMARK RESULTS(COVERAGE ANDSEMANTIC\nACCURACY)\nSystem Category #Questions Coverage (%) Semantic Acc.\nB-RAG Overall 17 66.67 7.88\nDefinitional 9 73.68 7.78\nProcedural 4 57.14 7.75\nAcronym 4 57.14 8.25\nA-RAG Overall 17 69.70 8.06\nDefinitional 9 68.42 7.89\nProcedural 4 100.0 8.25\nAcronym 4 42.85 8.25\nVI. DISCUSSION\nA. Interpretation of Results\nThe results indicate that the A-RAG system achieved mea-\nsurable improvements in retrieval accuracy over the Baseline,\nconfirming our hypothesis that agentic decomposition strate-\ngies are beneficial in fragmented, domain-specific settings. The\nstrict accuracy gain (62.35% vs. 54.12%) is further supported\nby manual review, where A-RAG often retrieved semantically\ncorrect information from alternative sources not marked as\nground truth. These observations align with the core challenge\ndescribed in Section I: enterprise fintech documents are seman-\ntically sparse and often distribute relevant information across\nmultiple partially redundant pages.\nQualitative analysis revealed that the most effective agentic\ncomponent was sub-query generation, which enabled targeted\nexploration of edge cases where the initial query lacked\nspecificity. Acronym resolution proved more error-prone: in\ncases of ambiguous or undefined acronyms, the agent occa-\nsionally surfaced overly generic sources. The cross-encoder re-\nranking improved relevance when multiple near-matches were\nretrieved but introduced latency. These trade-offs confirm that\nwhile A-RAG design introduces computational overhead, its\nmodular structure is more robust to domain-specific ambiguity\nthan the static B-RAG pipeline.\nAs shown in the human-curated benchmark (Table III), A-\nRAG achieves higher overall coverage (69.70% vs. 66.67%)\nand a modest improvement in semantic accuracy (8.06 vs.\n7.88), indicating enhanced consolidation of dispersed evi-\ndence. The gains are most evident in procedural queries,\nwhere coverage reaches 100% and semantic quality rises\nto 8.25. This suggests that iterative sub-query expansion is\nparticularly effective when task steps are implicit. In contrast,\nacronym and definitional queries do not show consistent im-\nprovements\u2014acronym coverage declines (57.14% to 42.85%),\nand definitional coverage decreases slightly. These outcomes\nimply that acronym resolution and re-ranking mechanisms\nmay occasionally over-filter or misprioritize near-duplicate\nsources. Nevertheless, the identical acronym semantic scores\n(8.25) suggest that A-RAG can synthesize sufficient context\nfrom partial retrieval. Overall, the \u201cmany plausible answers\u201d\nsetting underscores both the robustness advantages of agentic\n\norchestration and specific limitations (e.g., acronym recall) that\nwarrant further investigation.\nB. Addressing Domain-Specific Ontology\nThe A-RAG pipeline explicitly handled two challenges cen-\ntral to fintech ontologies: acronym disambiguation and context\nfragmentation. By integrating an acronym helper agent and\nembedding definitions where available, the system was able\nto expand domain-specific abbreviations during both query\nreformulation and document re-ranking. However, this compo-\nnent showed limitations in edge cases where acronyms were\nneither defined nor contextually grounded in the document.\nIn contrast, B-RAG relied entirely on surface-level keyword\noverlap, often missing results that required any interpretive\ninference.\nSub-query phrasing also improved navigation through the\nontology by decomposing vague or overloaded user queries\ninto focused, domain-aligned sub-tasks. This was impactful\nfor queries that implicitly referenced process hierarchies or\ntools\u2014common in enterprise documentation. Overall, the A-\nRAG system demonstrated a stronger ability to operate within\nthe constraints of a specialized ontology, especially when\nsemantic cues in the documents were minimal or inconsistently\nstructured.\nC. Limitations of the Current Study\nThe study is constrained by several factors. Firstly, the\nevaluation dataset is relatively small (85 questions) and may\nnot reflect the full spectrum of real-world queries or document\nstructures in fintech. Secondly, all results are grounded in\nthe use of a single LLM (Llama-3.1-8B-Instruct) and\nembedding model, limiting generalization across architectures.\nThirdly, certain agents (e.g., acronym resolver) rely on heuris-\ntic definitions and simple regex-based expansion, which may\nunder perform in more complex acronym usage. Lastly, while\nadjusted accuracy was discussed, it is not backed by a formal\nmetric; a more rigorous semantic equivalence measure should\nbe incorporated in future work to better capture retrieval\nperformance in fragmented corpora.\nVII. CONCLUSION ANDFUTUREWORK\nThis study investigated whether a modular, agent-driven\nRAG pipeline could more effectively navigate fragmented en-\nterprise knowledge bases in the fintech domain. The proposed\nA-RAG system incorporated acronym resolution, subquery\ndecomposition, and document reranking agents to enable more\nadaptive retrieval strategies, yielding a measurable improve-\nment in strict retrieval accuracy (62.35% vs. 54.12%) over a\nstandard baseline, and rising to 69.41% when accounting for\nsemantically relevant but non-ground-truth sources.\nLooking ahead, future work could explore more principled\ndesign strategies for agent coordination, such as reinforce-\nment learning or meta-controller frameworks that dynami-\ncally adapt the agent composition based on query type or\nretrieval feedback. Additionally, integrating stronger context-\nawareness\u2014such as discourse-level tracking, temporal ground-\ning, or multi-turn memory could help align answers more\nclosely with user intent. Enhancing the reasoning capabilities\nof the pipeline remains an open challenge, particularly in\nproducing not just faithful but helpful responses that satisfy\nnuanced informational needs. Techniques such as agent self-\ncritique, counterfactual retrieval, or reflection-based loops may\noffer promising paths forward. We expect this work con-\ntribute a meaningful step toward more robust and interpretable\nretrieval-augmented systems in high-stakes, domain-specific\napplications.\nREFERENCES\n[1] V . Scotti and M. J. Carman, \u201cLlm support for real-time technical\nassistance,\u201d inJoint European Conference on Machine Learning and\nKnowledge Discovery in Databases. Springer, 2024, pp. 388\u2013393.\n[2] Cursor AI, \u201cCursor: The ai-first code editor,\u201d https://www.cursor.so,\n2024, accessed: 2025-06-25.\n[3] OpenAI, \u201cChatgpt: Optimizing language models for dialogue,\u201d https:\n//openai.com/blog/chatgpt, 2022, accessed: 2025-06-25.\n[4] E. Schuman, \u201c88% of AI pilots fail to reach production \u2014 but that\u2019s\nnot all on IT,\u201d https://www.cio.com/article/3850763/88-of-ai-pilots-fai\nl-to-reach-production-but-thats-not-all-on-it.html, Mar. 2025, accessed:\n2025-06-25.\n[5] A. Menshawy, Z. Nawaz, and M. Fahmy, \u201cNavigating challenges and\ntechnical debt in large language models deployment,\u201d inProceedings of\nthe 4th Workshop on Machine Learning and Systems, 2024, pp. 192\u2013199.\n[6] P. Giudici, \u201cFintech risk management: A research challenge for artificial\nintelligence in finance,\u201dFrontiers in Artificial Intelligence, vol. V olume\n1 - 2018, 2018. [Online]. Available: https://www.frontiersin.org/journa\nls/artificial-intelligence/articles/10.3389/frai.2018.00001\n[7] Mastercard, \u201cMdes for merchants (m4m): Mastercard digital enablement\nservice for merchants,\u201d https://www.mastercard.com/news/eemea/en/new\nsroom/press-releases/en/2020/august/mastercard-research-shows-surge\n-in-digital-payments-as-ecommerce-reaches-new-heights-in-the-uae/,\n2020, accessed: 2025-06-25.\n[8] \u2014\u2014, \u201cSwitchcore: Mastercard transaction switching platform,\u201d https:\n//www.mastercard.com/eea/switching-services/our-technology/network.\nhtml, 2025, accessed: 2025-06-25.\n[9] Y . Liu, D. Iter, Y . Xu, S. Wang, R. Xu, and C. Zhu, \u201cG-eval: Nlg\nevaluation using gpt-4 with better human alignment,\u201d 2023. [Online].\nAvailable: https://arxiv.org/abs/2303.16634\n[10] J. Chen, H. Lin, X. Han, and L. Sun, \u201cBenchmarking large language\nmodels in retrieval-augmented generation,\u201d inProceedings of the AAAI\nConference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 754\u2013\n17 762.\n[11] A. Singh, A. Ehtesham, S. Kumar, and T. T. Khoei, \u201cAgentic retrieval-\naugmented generation: A survey on agentic rag,\u201darXiv preprint\narXiv:2501.09136, 2025.\n[12] T. Nguyen, P. Chin, and Y .-W. Tai, \u201cMa-rag: Multi-agent retrieval-\naugmented generation via collaborative chain-of-thought reasoning,\u201d\narXiv preprint arXiv:2505.20096, 2025.\n[13] Pathway Community, \u201cAdaptive agents for real-time rag: Domain-\nspecific ai for legal, finance & healthcare,\u201d https://pathway.com/bl\nog/adaptive-agents-rag/, 2025.\n[14] R. C. Barron, V . Grantcharov, S. Wanna, M. E. Eren, M. Bhattarai,\nN. Solovyev, G. Tompkins, C. Nicholas, K. . Rasmussen, C. Matuszek,\nand B. S. Alexandrov, \u201cDomain-specific retrieval-augmented generation\nusing vector stores, knowledge graphs, and tensor factorization,\u201darXiv\npreprint arXiv:2410.02721, 2024.\n[15] H. Daiya, \u201cLeveraging large language models (llms) for enhanced risk\nmonitoring in fintech,\u201dIEEE Computer Society Tech News, 2024.\n[16] M. Broughton, \u201cLarge language models: How they help fintechs,\u201d https:\n//thepaymentsassociation.org/article/large-language-models-how-they-h\nelp-fintechs/, 2024.\n[17] Lumenova AI, \u201cAi in finance: The promise and risks of rag,\u201d https:\n//www.lumenova.ai/blog/ai-finance-retrieval-augmented-generation/,\n2024.\n[18] P. H. Leal, \u201cBuilding a financial education chatbot with retrieval-\naugmented generation (rag),\u201d https://medium.com/@hlealpablo/bui\nlding-a-financial-education-chatbot-with-retrieval-augmented-generatio\nn-rag-bf338aa2df09, 2023.",
  "authors": [
    "Thomas Cook",
    "Richard Osuagwu",
    "Liman Tsatiashvili",
    "Vrynsia Vrynsia",
    "Koustav Ghosal",
    "Maraim Masoud",
    "Riccardo Mattivi"
  ],
  "summary": "Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies, dense terminology, and acronyms complicate effective retrieval and synthesis. This paper introduces an agentic RAG architecture designed to address these challenges through a modular pipeline of specialized agents. The proposed system supports intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking. We evaluate our approach against a standard RAG baseline using a curated dataset of 85 question--answer--reference triples derived from an enterprise fintech knowledge base. Experimental results demonstrate that the agentic RAG system outperforms the baseline in retrieval precision and relevance, albeit with increased latency. These findings suggest that structured, multi-agent methodologies offer a promising direction for enhancing retrieval robustness in complex, domain-specific settings.",
  "pdf_url": "https://arxiv.org/pdf/2510.25518v1",
  "entry_id": "http://arxiv.org/abs/2510.25518v1",
  "published": "2025-10-29",
  "updated": "2025-10-29",
  "comment": "Keywords: RAG Agentic AI Fintech NLP KB Domain-Specific Ontology Query Understanding",
  "journal_ref": null,
  "doi": null,
  "primary_category": "cs.AI",
  "categories": [
    "cs.AI"
  ],
  "links": [
    {
      "href": "https://arxiv.org/abs/2510.25518v1",
      "rel": "alternate",
      "title": null
    },
    {
      "href": "https://arxiv.org/pdf/2510.25518v1",
      "rel": "related",
      "title": "pdf"
    }
  ],
  "full_text": "Retrieval Augmented Generation (RAG) for\nFintech: Agentic Design and Evaluation\n1,3Thomas Cook\u2217, 2,3Richard Osuagwu\u2217, 1,3Liman Tsatiashvili\u2217, 4,3Vrynsia Vrynsia\u2217,\n3Koustav Ghosal, 3Maraim Masoud, 3Riccardo Mattivi\n1TU Dublin, Dublin, Ireland\n2Maynooth University, Ireland\n3Mastercard, Ireland\n4National College of Ireland, Ireland\n{firstname.lastname}@mastercard.com\nAbstract\u2014Retrieval-Augmented Generation (RAG) systems of-\nten face limitations in specialized domains such as fintech, where\ndomain-specific ontologies, dense terminology, and acronyms\ncomplicate effective retrieval and synthesis. This paper introduces\nan agentic RAG architecture designed to address these challenges\nthrough a modular pipeline of specialised agents. The proposed\nsystem supports intelligent query reformulation, iterative sub-\nquery decomposition guided by keyphrase extraction, contextual\nacronym resolution, and cross-encoder-based context re-ranking.\nWe evaluate our approach against a standard RAG baseline\nusing a curated dataset of 85 question\u2013answer\u2013reference triples\nderived from an enterprise fintech knowledge base. Experimental\nresults demonstrate that the agentic RAG system outperforms\nthe baseline in retrieval precision and relevance, albeit with\nincreased latency. These findings suggest that structured, multi-\nagent methodologies offer a promising direction for enhancing\nretrieval robustness in complex, domain-specific settings.\nIndex Terms\u2014Retrieval Augmented Generation, Agentic AI,\nFintech, Natural Language Processing, Knowledge Base, Domain-\nSpecific Ontology, Query Understanding\nI. INTRODUCTION\nRetrieval Augmented Generation (RAG) combines large\nlanguage models (LLMs) with external document retrieval,\nallowing models to access information beyond their training\ndata. These systems have shown impressive results in general-\npurpose applications such as technical support [1], coding\nassistants [2], and document summarization [3], especially\nwhen operating as proof-of-concepts on publicly available or\nloosely structured data. However, deploying such systems at\nscale in highly specialized and tightly regulated domains such\nas financial technology is far from straightforward [4], [5].\nFintech-specific use cases often involvestructured-\nunstructured data(or tightly regulated free text), such as\nproprietary knowledge bases, role-based access to information,\nand stringent compliance constraints [6], all of which make\nit difficult to repurpose existing tools and methodologies\ndesigned for open-domain settings or Software-as-a-Service\n(SaaS)-based environments. These complexities are both tech-\nnical and organizational. Regulatory restrictions prohibit data\n*Work done during internship at Mastercard, Ireland\nFig. 1. Word cloud illustrating the distribution of internal knowledge artifacts\nin fintech. The prominence of terms such as \u201cfeature,\u201d \u201cstatus,\u201d \u201cmodel,\u201d and\n\u201cAPI\u201d reflects the operational focus of internal documentation\u2014often centered\naround technical specifications, product state, and integration interfaces. This\nconcentration of tightly scoped, semi-structured information highlights the\nchallenge of designing RAG systems that can interpret fragmented context\nacross teams and tools, where standard SaaS-based approaches fall short due\nto regulatory and organizational constraints.\nfrom leaving organisational boundaries, making cloud-hosted\nAPIs or third-party evaluation platforms unsuitable for most\npractical deployments. In addition, often fintech firms follow\na compartmentalized structure with clear divisions of respon-\nsibility between product managers, analysts, compliance offi-\ncers, and engineers. This further complicates how knowledge\nis captured, retrieved, and applied.\nFor example, consider a product manager in Mastercard\nanalyzing feature alignment across offerings such as MDES\n(Mastercard Digital Enablement Service) 1, M4M (Mastercard\nfor Merchants) [7], and SwitchCore (Mastercard\u2019s transaction\nswitching platform) [8]. Internal knowledge sources\u2014ranging\nfrom note-taking apps, product management platforms and\narchitecture decks to regulatory compliance PDFs\u2014often use\nacronyms inconsistently or assume implicit context based on\nteam-specific language. This forces the user to manually piece\ntogether fragmented information to understand interoperabil-\n1https://www.mastercard.com/newsroom/press/news-briefs/mastercard-dig\nital-enablement-service-mdes, Accessed: 2025-06-25\narXiv:2510.25518v1  [cs.AI]  29 Oct 2025\n\nity constraints, roadmap alignment, or API-level integration\nguidelines. A standard RAG system typically underperforms\nin such scenarios, frequently misinterpreting short forms like\n\u201cCMA\u201d (which could mean \u201cConsumer Management Appli-\ncation\u201d or \u201cCardholder Management Architecture\u201d depending\non the context), or retrieving documents based on keyword\noverlap rather than intent.\nAnother challenge with such domain-specific use-cases for\nlarge enterprises is evaluation. Traditional RAG benchmarks\nassume public datasets, crowd-sourced relevance judgments,\nor static ground truth\u2014all of which are infeasible in fintech.\nFinding subject-matter experts to annotate queries at scale is\nboth costly and time-consuming. Moreover, regulations around\nconfidentiality and data residency prevent the use of crowd\nplatforms for human evaluation.\nTherefore, designing on-prem, domain-specific RAG ap-\nplications requires several considerations and not limited to\n\u2014 a careful rethinking of architecture, privacy guarantees, a\nstructured retrieval and reasoning process capable of adapting\nto ambiguity (especially when the source content is internal,\nfragmented, and often acronym-heavy). In this work, we focus\non some of these core challenges and propose an agent design\ntailored to domain-specific retrieval and reasoning. We also\npresent evaluation methodologies that are secure, reproducible,\nand feasible at scale within enterprise constraints.\nWe propose a hierarchical agent framework, where an\nOrchestrator Agentdelegates tasks to specialized sub-\nagents responsible for acronym resolution, domain-aware\nquery rewriting, and cross-encoder-based re-ranking. This\nmodular structure allows the system to encode organizational\nidiosyncrasies such as internal taxonomies, access controls,\nand documentation practices, significantly improving the\nquality of answers retrieved. In addition, we develop a\nsemi-automated evaluation strategy that combines LLM-as-a-\njudge [9] paradigms with constrained prompt templates and\nhuman-in-the-loop validation. Inspired by recent work on\nsynthetic evaluation datasets [9], [10], we leverage internal\nknowledge bases to generate realistic query-answer pairs,\nwhich are then assessed using a mix of LLM scoring and\nmanual spot-checking. This allows for a secure, efficient, and\nconsistent evaluation process that scales across teams and\ndata silos.\nTo summarize, our primary contributions include:\n\u2022A Fintech-focused Agentic Design:We develop an\nOrchestrator Agentthat coordinates specialized agents for\nacronym resolution, iterative sub-query generation using\nkey phrases, and context refinement through a cross-\nencoder re-ranking method.\n\u2022Automatic Query Enhancement with Continuous\nFeedback:Our system proactively identifies and resolves\ndomain-specific acronyms within user queries and re-\ntrieved content, enhancing retrieval accuracy and answer\ncompleteness.\n\u2022Thorough Evaluation Methodology:We construct an\nevaluation dataset from an enterprise knowledge base,\nleveraging LLM-driven synthetic data generation and\nmanual validation to ensure high-quality, contextually\nrelevant questions.\n\u2022Empirical Comparative Analysis:We quantitatively and\nqualitatively compare our agentic approach against a\nstandard RAG baseline, measuring effectiveness through\nmetrics such as retrieval accuracy and answer relevance.\nThe remainder of this paper is organized as follows. Sec-\ntion II reviews relevant work in RAG, multi-agent pipelines,\nand domain-specific retrieval systems. Section III describes\nour methodology, including the design of the baseline and\nagentic RAG architectures. Section IV details the knowledge\nbase preparation and the construction of the evaluation dataset.\nSection V presents the experimental setup and results, compar-\ning retrieval accuracy and latency across configurations. Sec-\ntion VI provides a qualitative error analysis and discussion of\nobserved performance patterns. Finally, Section VII concludes\nthe paper and outlines directions for future work.\nII. RELATEDWORK\nAgentic RAG:Recent work has increasingly explored en-\nhancing Retrieval-Augmented Generation (RAG) systems with\nstructured, multi-agent architectures for complex information\ntasks. Agentic RAG integrates autonomous AI agents into the\npipeline, enabling dynamic query decomposition and iterative\nreasoning. Singhet al.introduce the concept of Agentic RAG,\nwhere agents perform planning, reflection, and tool use to\nrefine context and retrieval [11]. Similarly, Nguyenet al.\npropose MA-RAG, a modular, training-free framework where\nagents (e.g., Planner, Extractor, QA) collaboratively process\nquestions using chain-of-thought reasoning [12]. Their results\nshow that this structure improves answer quality and resolves\nambiguities without fine-tuning. The Pathway team further\nobserves that domain-adaptive agents enhance performance by\nassigning each agent task-relevant expertise [12], [13].\nDomain-specific RAG:Deploying RAG in specialized do-\nmains such as finance, healthcare, and law requires grounding\nretrieval in domain knowledge. Recent studies emphasize\nusing structured ontologies to enhance LLM performance.\nBarronet al.introduce the SMART-SLIC framework, com-\nbining knowledge graphs (KGs) and vector stores customized\nfor a domain to support retrieval [14]. They demonstrate\nthat referencing KG ontologies improves QA by aligning\nretrieval with relevant subgraph structures. Their approach\nfuses structured (KG) and unstructured (text) sources to reduce\nhallucinations and improve citation fidelity [14]. However,\nthey also highlight the resource demands of curating such\ndomain-specific infrastructure. Other studies similarly confirm\nthat structured knowledge sources boost retrieval precision in\nspecialized settings such as legal and medical QA [11], [14].\nLLM Applications in Fintech:Large language models\nhave found wide adoption in fintech, powering analytics and\nautomation across customer support, fraud detection, risk\nmodeling, and compliance. Daiya surveys how LLMs monitor\nfinancial risks by processing vast unstructured data, detecting\n\npatterns, and forecasting threats from logs, news, and com-\nmunications [15]. Broughton highlights their role in fraud\ndetection, credit scoring, and automating regulatory tasks like\ndocument parsing and report generation [16]. These studies\nagree that LLMs extract actionable insights from diverse\nfinancial sources, whether through sentiment analysis, trading\nsignal identification, or customer feedback mining [15], [16].\nThese applications underscore the need for domain-aware\nRAG to enhance factual accuracy in high-stakes financial\ncontexts.\nRAG for Fintech:Within fintech, RAG systems are be-\ning prototyped for data-driven assistants and analytics tools.\nLumenova AI reports growing adoption of RAG-backed chat-\nbots that integrate live market and account data to improve\ncustomer interaction [17]. A notable example is Bank of\nAmerica\u2019s \u201cErica,\u201d which uses LLMs and real-time retrieval to\nsupport over a billion user interactions [17]. Hernandez Leal\ndemonstrates a RAG-powered assistant trained on SEC filings\nto answer investor questions from 10-K and 10-Q reports [18].\nThese examples show how RAG can anchor LLM outputs in\nauthoritative financial sources, enhancing both precision and\ncitation. While still in early stages, these systems suggest that\nreal-time, curated retrieval pipelines significantly boost LLM\nutility and trustworthiness in financial applications [17], [18].\nIII. AGENTDESIGN\nBuilding on the challenges introduced in Section I, this\nsection details how we first implemented a Baseline RAG\npipeline to explore retrieval limitations in fintech settings,\nand subsequently refined the design into a modular agentic\narchitecture.\nA. Baseline RAG System\nThe Baseline RAG system, which will be referred to as\n\"B-RAG\" for the remainder of this work, represents a stan-\ndard retrieval pipeline that serves as a comparison point for\nour agentic architecture. This approach follows a sequential\nprocess, as illustrated in Figure 2.\nThe B-RAG system, illustrated in Figure 2, consists of a\nsequence of specialized agents structured to enable stepwise\ninformation retrieval and synthesis. The process begins with\nthe user\u2019s initial input, which serves as the contextual founda-\ntion for subsequent components. This input is then propagated\nthrough the following agents:\n(a)Query Reformulator Agent: Takes the user\u2019s question\nand refines it into a concise, keyword-focused query\noptimized for retrieval. It uses predefined prompt tem-\nplates to detect if the query is a continuation of previous\ninteractions or a standalone new topic.\n(b)Retriever Agent: Executes a single-pass retrieval from the\nknowledge base with cosine similarity between embed-\ndings to locate relevant document chunks.\n(c)Summary Agent: Aggregates retrieved document chunks\ninto a coherent answer. It is explicitly instructed to\navoid generating information not present in the retrieved\nFig. 2. Overview of the B-RAG pipeline. The process follows a linear\nflow beginning with the user\u2019s initial query, followed by query reformulation,\nsingle-pass retrieval, summarization, and response generation.\ncontext, clearly indicating if the retrieved content lacks\nsufficient information.\nFinally, the output of the summary agent is presented as a\ncoherent response, supported by the reference links.\nThe B-RAG pipeline serves as a reference due to its simplified\narchitecture, which allows us to conduct clear and interpretable\nperformance benchmarking against the more complex agentic\nsystem. However, this simplicity imposes several notable lim-\nitations:\n\u2022Lack of multi-pass retrieval logic: The system performs a\nsingle retrieval pass without iterative refinement, thereby\nconstraining its capacity to effectively address complex\nqueries that require deeper exploration.\n\u2022Absence of sub-query decomposition: The pipeline is un-\nable to partition broad or ambiguous queries into multiple\ntargeted sub-queries. This limitation results into dimin-\nished precision when questions span multiple document\nfragments.\n\u2022No acronym resolution mechanism: The system does not\nresolve domain-specific acronyms frequently encountered\nin enterprise context, which may adversely affect the\nclarity and accuracy of retrieved information.\n\u2022Absence of document re-ranking: Retrieved results are uti-\nlized without further subsequent validation or refinement\nthrough cross-encoder-based re-ranking. This potentially\nmay compromise the relevance of the final responses.\nDespite these limitations, the simple design of the baseline\npipeline offers some advantages, including ease of deploy-\nment, lower computational overhead, and enhanced inter-\npretability. These attributes make it an effective benchmark\nfor systematically evaluating the incremental improvements\nintroduced by each advanced component of the proposed\nagentic architecture.\nB. Proposed Refined Design\nThe Agentic RAG system, which will be referred to as\n\"A-RAG\" throughout this paper, is designed to address the\n\nlimitations inherent in the B-RAG system by integrating\nmodular intelligence and task-specific specialization. Central\nto A-RAG is theOrchestrator Agent, which coordinates a suite\nof specialized agents, each tasked with a distinct retrieval or\nsynthesis function. This design is inspired by an investigative\nresearch workflow. It initiates with a direct attempt to address\nthe user query, subsequently evaluates the output, and when\nnecessary, activates more focused and iterative exploration.\nTo address challenges such as ambiguous terminology, frag-\nmented sources, and answer confidence, A-RAG incorporates\nseveral key capabilities, including acronym resolution, sub-\nquery generation, parallel retrieval, re-ranking, and quality\nassessment. The quality of generated answers is evaluated\nby a dedicated QA agent whose confidence score determines\nwhether further iterative refinement is required. A comparison\nof the hybrid pipeline architectures for B-RAG and A-RAG\nworkflows is shown in Figure 3. It highlights the differences\nin processing flow and iterative mechanisms.\nC. System Architecture and Workflow\nThe A-RAG system comprises a set of lightweight, modular\nagents orchestrated to support iterative retrieval and reasoning.\nThe following subsections describe the system\u2019s core agent\ncomponents and the operational workflow using a representa-\ntive fintech query example.\nCore Agent Components:The orchestrator coordinates eight\nspecialized agents, each responsible for a distinct stage in the\nretrieval\u2013reasoning cycle:\n1)Intent Classifier\u2013 determines whether the user input\nrequires(i) retrieval, which involves fetching new context\nfrom the knowledge base, or(ii) summary, which com-\npresses existing conversational history.\n2)Query Reformulator\u2013 transforms the raw query into a\ndense, keyword-optimized search string by removing func-\ntion words, expanding recognized acronyms, and injecting\ndomain-specific synonyms to maximize embedding-based\nrecall.\n3)Retriever Manager & Retriever Agent\u2013 launch one\nor more vector store queries\u2014executed in parallel if sub-\nqueries exist\u2014and return the top-kmost relevant chunks.\n4)Sub-Query Generator\u2013 in cases of low initial retrieval\nscores, identifies 2\u20133 key entities or phrases to construct\ntargeted follow-up queries.\n5)Re-Ranker Agent\u2013 reorders the retrieved chunks using\na cross-encoder to prioritize those with higher semantic\nalignment to the query.\n6)Summary Agent\u2013 fuses the ranked context snippets into\na concise, citation-style answer composed strictly from the\nretrieved content.\n7)QA Agent\u2013 evaluates the synthesized answer on a 0\u201310\nscale, assessing relevance and support from context; the\nscore determines whether the pipeline concludes or pro-\nceeds with refinement.\n8)Acronym Resolution Logic\u2013 manages a local glossary\nand, when invoked, appends in-line definitions to prevent\nFig. 3. Comparison of hybrid pipeline architectures for B-RAG and A-\nRAG workflows. The left panel shows B-RAG\u2019s single-pass process, including\nquery reformulation, retrieval, answer synthesis, and output generation without\niterative refinement. The right panel illustrates A-RAG\u2019s extended pipeline,\nfeaturing acronym resolution, sub-query generation, document re-ranking,\nand an answer quality assessment (QA) agent. If the QA agent assigns low\nconfidence to the synthesized answer (e.g., below a set threshold), a feedback\nloop triggers sub-query generation to iteratively expand and refine retrieval.\ndownstream agents from misinterpreting shorthand expres-\nsions.\nWorkflow Overview:The operational workflow integrates\nthe agent components into a cohesive, adaptive pipeline,\nillustrated with the example query:\u201cHow is CVaR calculated\nin the IRRBB framework?\u201dThe process begins with query\nreformulation and acronym expansion, ensuring clarity and\ndisambiguation of terms likeCVaRandIRRBB. A first-pass\nvector retrieval fetches relevant document chunks, which are\nsynthesized into an initial answer. If the QA agent assigns\na low confidence score, the system triggers refinement: sub-\nqueries such as \u201cCVaR formula\u201d or \u201cIRRBB risk quantifi-\ncation\u201d are generated, results are re-ranked, and a revised\nsynthesis is attempted. Should this prove inadequate, a broader\nretrieval sweep is conducted to increase coverage. In the\nabsence of a sufficiently confident answer, the system transpar-\nently communicates its uncertainty to the user. This adaptive\n\npipeline dynamically balances computational efficiency with\nretrieval depth based on answer confidence.\nIV. DATA ANDEVALUATIONSTRATEGY\nIn Section I, we described the challenge of designing\nretrieval systems that can operate effectively over fragmented\nand acronym-heavy enterprise knowledge bases. This section\ndetails how we constructed the knowledge corpus and gener-\nated an evaluation set tailored to this context.\nA. Knowledge Base Preparation\nThe knowledge base was derived from proprietary internal\ndocuments exported in structured markup format. A custom\npipeline was developed to preprocess the data. This process\nfocused on:\n\u2022Converting structured elements such as tables and code\nblocks into linear, plain-text representations while pre-\nserving semantic relationships.\n\u2022Removing formatting noise and markup artifacts to pro-\nduce clean, model-ready text.\nTo enable retrieval, the documents were segmented into\noverlapping chunks (to preserve contextual coherence across\nboundaries), embedded with a publicly available sentence\ntransformer, and stored in a vector index.\nThe resulting corpus consisted of over 30,000 text chunks\nrepresenting 1,624 unique documents. On average, each docu-\nment was split into approximately 19 chunks. Figure 4 shows\nthe distribution of chunk lengths measured by word count.\nFig. 4. Distribution of chunk lengths by word count. The majority of chunks\nare between 50 and 120 words, showing the trade-off between retrieval\ngranularity and contextual coherence.\nTo further illustrate the dataset composition, Figure 1 and\nFigure 5 provide complementary views of term frequency.\nBoth highlight the prevalence of domain-specific language in\nthe internal fintech corpus, particularly terms such as \u201cfea-\nture,\u201d \u201cAPI,\u201d and \u201cmodel,\u201d reflecting its emphasis on product,\ntechnical, and compliance-related content.\nB. Evaluation Set Generation\nInspired by the LLM-as-a-judge paradigm [9], we designed\na multi-phase, model-assisted pipeline to create a high-quality\nevaluation set. This approach is motivated by the constraints\noutlined in Section I, particularly the impracticality of relying\nFig. 5. Top 20 most frequent terms in the corpus, ranked by raw frequency.\nsolely on manual annotation within confidential enterprise\nenvironments.\nFirst, we randomly sampled a subset of chunks from the cu-\nrated corpus. For each chunk, a language model was prompted\nto generate a question-answer pair grounded in the provided\ncontext. Two distinct prompt templates were used; one tailored\nto narrative text and the other to structured elements such as\ntables. This distinction is made to ensure both content diversity\nand fidelity to the source material.\nSubsequently, each generated question\u2013answer pair under-\nwent a quality control phase in which a language model as-\nsessed contextual alignment, factual accuracy, and coherence.\nTo standardize this evaluation, we defined three criteria that\neach pair was required to meet:\n\u2022Specificity:Is the question clearly and narrowly scoped\nto a particular aspect of the context?\n\u2022Faithfulness:Is the answer grounded exclusively in the\nprovided source text, without introducing external infor-\nmation?\n\u2022Completeness:Does the answer fully and directly ad-\ndress the question posed?\nOnly candidate pairs that satisfied all three criteria were\nretained. These pairs then underwent an additional round\nof manual review to eliminate any remaining edge cases\nor ambiguous examples. Finally, the resulting evaluation set\nconsisted of 85 validated question-answer pairs, each traceable\nto a unique document chunk. This dataset served as the\nfoundation for the quantitative and qualitative evaluations\npresented in Sections V-B, V-C.\nC. Human-Verified Prompt\u2013Answer Benchmark\nIn addition to the automated evaluation set described above,\nwe constructed a manually curated benchmark designed to\nstress-test both retrieval fidelity and answer ranking. The\n\nbenchmark construction process involved the following key\nsteps:\nQuestion sourcing:Candidate questions were extracted\nfrom routine activities within the internalknowledge base,\nincluding onboarding runbooks, intern reports, and project or\nproduct wikis.\nHuman validation of answers:For each question, human\nannotators identified all relevant documentation pages and\nselected one representative chunk per page as a potential\nanswer. Among these, the most comprehensive chunk was\ndesignated as the ground truth. This setup reflects a \u201cone\ncorrect, many plausible\u201d retrieval scenario.\nIntent categorization:Each query was labeled according\nto an intent taxonomy\u2014Procedural,Definitional, orAcronym\nExpansion\u2014to support stratified analysis, as discussed in\nSection V-C.\nDataset composition:The final benchmark comprises\n27 question\u2013answer pairs, each corresponding to a distinct\nquery with multiple associated ground-truth answers: seven\nprocedural, fourteen definitional, and six acronym-expansion\nexamples.\nThis human-verified dataset was used to (i) compute strict\nretrieval metrics (Hit@k), and (ii) support error analysis in\nthe presence of multiple valid answers. For each item, we\nlog the following fields:Question,Category,Ground-truth\nAnswer(s),Ground-truth Source(s),Generated Answer, and\nRetrieved Source(s).\nV. EXPERIMENTS ANDRESULTS\nA. Experimental Setup\nWe compared B-RAG and A-RAG pipelines on a special-\nized internal fintech knowledge base. Both pipelines used the\nsame LLM backend,Llama-3.1-8B-Instruct, served\nvia a vLLM endpoint. Document embeddings were generated\nusing theall-MiniLM-L6-v2model and stored in Chro-\nmaDB. We evaluated both systems using two key metrics: (1)\nRetrieval Accuracy (Hit Rate @5), defined as the percentage\nof questions for which the correct document source appeared\namong the top five retrieved links; and (2) Average Latency,\ndefined as the time from user query submission to the final\nsystem response.\nB. Quantitative Results\nTable I summarizes the results. The A-RAG system\nachieved a strict retrieval accuracy of 62.35%, outperforming\nthe Baseline\u2019s 54.12%. This performance gain can be attributed\nto the system\u2019s specialized agents for acronym resolution\nand sub-query expansion, which were designed to address\nthe fragmented and semantically sparse nature of enterprise\nknowledge described in Section I. As expected, A-RAG\u2019s\naverage query latency was 5.02 seconds, significantly higher\nthan B-RAG\u2019s 0.79 seconds, due to multi-stage processing and\niterative document re-ranking.\nTABLE I\nPERFORMANCECOMPARISON OFB-RAGANDA-RAG SYSTEMS\nMetric B-RAG A-RAG\nTotal Questions Evaluated 85 85\nRetrieval Accuracy (Hit @5, %) 54.12 62.35\nAvg. Latency per Query (s) 0.79 5.02\na) Adjusted Retrieval Interpretation:Although retrieval\nwas evaluated using exact link matches, manual inspection\nidentified several queries where the system returned correct\ncontent from semantically equivalent but non-identical doc-\numents. Specifically, A-RAG retrieved valid answers from\nalternate sources in six additional cases, and B-RAG in three.\nIncorporating these, the adjusted retrieval accuracy increases\nto 69.41% for A-RAG and 58.82% for B-RAG. These cases\nreinforce the core challenge introduced in Section I; that\nenterprise knowledge in fintech domains is often fragmented,\nwith key information distributed across related documents.\nUnlike B-RAG, which often fails when the exact match\nis missing, A-RAG\u2019s sub-query generation and iterative re-\nranking modules better synthesize partial context across\nsources, producing correct responses even when the originat-\ning chunk is not directly retrieved. In the next section we\nevaluate answer quality usingsemantic accuracy, defined as\nthe mean LLM-judge score measuring semantic equivalence\nbetween a system\u2019s answer and the human ground-truth an-\nswer, independent of surface lexical overlap.\nC. Answer-Quality Evaluation (Semantic Accuracy)\nBuilding on the retrieval metrics in Table I (Section V-B),\nwe next measure how faithfully each pipeline answers user\nquestions. Rather than lexical overlap, we targetsemantic\nagreement with the ground-truth answers created in Sec-\ntion IV.\nWe define the following metric to calculate the semantic\naccuracy. LetNdenote the total number of evaluated questions\n(hereN= 85). For every questionq i, an external vLLM-\nhosted judge (Llama-3.1-8B) assigns an integer score\nsi \u2208 {1, . . . ,10}using the rubric in Table II. The semantic-\naccuracy metric is the mean judge score:\n\u00afs= 1\nN\nNX\ni=1\nsi.\nTABLE II\nLLM-JUDGE RUBRIC(SEMANTIC ACCURACY).\nScore Interpretation\n9\u201310 Exact match or perfect paraphrase\n6\u20138 Correct but missing minor detail\n3\u20135 Honest refusal / incomplete answer\n1\u20132 Incorrect or hallucinated\nApplying this metric to our evaluation set reveals that the\nA-RAG pipeline outperforms the baseline B-RAG. A-RAG\n\nachieves a mean score of\u00afs= 7.04, compared to\u00afs= 6.35\nfor B-RAG, yielding a performance gain of\u2206\u00afs\u22480.68.\nAs shown in Figure 6, A-RAG notably reduces the fre-\nquency of low-quality answers (s<5) and increases the pro-\nportion of responses in the top rubric tier (s\u22659). The per-\nquestion score deltas (Figure 7) show that A-RAG is preferred\nin 64 % of cases, ties in 25 % and is outperformed by B-RAG\nin only 11 %. Even in those rare cases, the largest drop is\njust 3 points, indicating that A-RAG rarely degrades answer\nquality.\nFig. 6. Histogram+KDE of all 85 LLM-judge scores per pipeline. A-RAG\nshifts mass into the 7\u201310 band, halves low-quality answers (s <5falls from\n18% to 8%), and increases \u201cexcellent\u201d (s\u22659) responses from 12% to 22%.\nFig. 7. Per-question score difference\u2206s=s A-RAG \u2212s B-RAG. Distribution\ncenters at+1(median) with 64% of points>0, 25% at 0, and only 11%\n<0; extreme negative swings never exceed\u22123, indicating A-RAG rarely\ndegrades answer quality.\nD. Human-Curated Benchmark\nWe further evaluate both systems using a human-curated\nbenchmark consisting of 17 questions, each designed to allow\nmultiple plausible answers. The benchmark comprises 9 defini-\ntional questions, 4 procedural questions, and 4 acronym-based\nquestions. Collectively, these questions are associated with\n33 distinct ground-truth source links. In addition to assessing\nsemantic accuracy, we introduce coverage as a complementary\nmetric:\nCoverage = |R|\n|G| ,\nwhereGis the set of 33 distinct ground-truth source links\nandRthe subset retrieved within the top\u20135 across all ques-\ntions. B-RAG achieves 66.67% (22/33), while A-RAG attains\n69.70% (23/33). This result indicates a modest advantage in\naggregating distributed evidence.\nTABLE III\nHUMAN-CURATED BENCHMARK RESULTS(COVERAGE ANDSEMANTIC\nACCURACY)\nSystem Category #Questions Coverage (%) Semantic Acc.\nB-RAG Overall 17 66.67 7.88\nDefinitional 9 73.68 7.78\nProcedural 4 57.14 7.75\nAcronym 4 57.14 8.25\nA-RAG Overall 17 69.70 8.06\nDefinitional 9 68.42 7.89\nProcedural 4 100.0 8.25\nAcronym 4 42.85 8.25\nVI. DISCUSSION\nA. Interpretation of Results\nThe results indicate that the A-RAG system achieved mea-\nsurable improvements in retrieval accuracy over the Baseline,\nconfirming our hypothesis that agentic decomposition strate-\ngies are beneficial in fragmented, domain-specific settings. The\nstrict accuracy gain (62.35% vs. 54.12%) is further supported\nby manual review, where A-RAG often retrieved semantically\ncorrect information from alternative sources not marked as\nground truth. These observations align with the core challenge\ndescribed in Section I: enterprise fintech documents are seman-\ntically sparse and often distribute relevant information across\nmultiple partially redundant pages.\nQualitative analysis revealed that the most effective agentic\ncomponent was sub-query generation, which enabled targeted\nexploration of edge cases where the initial query lacked\nspecificity. Acronym resolution proved more error-prone: in\ncases of ambiguous or undefined acronyms, the agent occa-\nsionally surfaced overly generic sources. The cross-encoder re-\nranking improved relevance when multiple near-matches were\nretrieved but introduced latency. These trade-offs confirm that\nwhile A-RAG design introduces computational overhead, its\nmodular structure is more robust to domain-specific ambiguity\nthan the static B-RAG pipeline.\nAs shown in the human-curated benchmark (Table III), A-\nRAG achieves higher overall coverage (69.70% vs. 66.67%)\nand a modest improvement in semantic accuracy (8.06 vs.\n7.88), indicating enhanced consolidation of dispersed evi-\ndence. The gains are most evident in procedural queries,\nwhere coverage reaches 100% and semantic quality rises\nto 8.25. This suggests that iterative sub-query expansion is\nparticularly effective when task steps are implicit. In contrast,\nacronym and definitional queries do not show consistent im-\nprovements\u2014acronym coverage declines (57.14% to 42.85%),\nand definitional coverage decreases slightly. These outcomes\nimply that acronym resolution and re-ranking mechanisms\nmay occasionally over-filter or misprioritize near-duplicate\nsources. Nevertheless, the identical acronym semantic scores\n(8.25) suggest that A-RAG can synthesize sufficient context\nfrom partial retrieval. Overall, the \u201cmany plausible answers\u201d\nsetting underscores both the robustness advantages of agentic\n\norchestration and specific limitations (e.g., acronym recall) that\nwarrant further investigation.\nB. Addressing Domain-Specific Ontology\nThe A-RAG pipeline explicitly handled two challenges cen-\ntral to fintech ontologies: acronym disambiguation and context\nfragmentation. By integrating an acronym helper agent and\nembedding definitions where available, the system was able\nto expand domain-specific abbreviations during both query\nreformulation and document re-ranking. However, this compo-\nnent showed limitations in edge cases where acronyms were\nneither defined nor contextually grounded in the document.\nIn contrast, B-RAG relied entirely on surface-level keyword\noverlap, often missing results that required any interpretive\ninference.\nSub-query phrasing also improved navigation through the\nontology by decomposing vague or overloaded user queries\ninto focused, domain-aligned sub-tasks. This was impactful\nfor queries that implicitly referenced process hierarchies or\ntools\u2014common in enterprise documentation. Overall, the A-\nRAG system demonstrated a stronger ability to operate within\nthe constraints of a specialized ontology, especially when\nsemantic cues in the documents were minimal or inconsistently\nstructured.\nC. Limitations of the Current Study\nThe study is constrained by several factors. Firstly, the\nevaluation dataset is relatively small (85 questions) and may\nnot reflect the full spectrum of real-world queries or document\nstructures in fintech. Secondly, all results are grounded in\nthe use of a single LLM (Llama-3.1-8B-Instruct) and\nembedding model, limiting generalization across architectures.\nThirdly, certain agents (e.g., acronym resolver) rely on heuris-\ntic definitions and simple regex-based expansion, which may\nunder perform in more complex acronym usage. Lastly, while\nadjusted accuracy was discussed, it is not backed by a formal\nmetric; a more rigorous semantic equivalence measure should\nbe incorporated in future work to better capture retrieval\nperformance in fragmented corpora.\nVII. CONCLUSION ANDFUTUREWORK\nThis study investigated whether a modular, agent-driven\nRAG pipeline could more effectively navigate fragmented en-\nterprise knowledge bases in the fintech domain. The proposed\nA-RAG system incorporated acronym resolution, subquery\ndecomposition, and document reranking agents to enable more\nadaptive retrieval strategies, yielding a measurable improve-\nment in strict retrieval accuracy (62.35% vs. 54.12%) over a\nstandard baseline, and rising to 69.41% when accounting for\nsemantically relevant but non-ground-truth sources.\nLooking ahead, future work could explore more principled\ndesign strategies for agent coordination, such as reinforce-\nment learning or meta-controller frameworks that dynami-\ncally adapt the agent composition based on query type or\nretrieval feedback. Additionally, integrating stronger context-\nawareness\u2014such as discourse-level tracking, temporal ground-\ning, or multi-turn memory could help align answers more\nclosely with user intent. Enhancing the reasoning capabilities\nof the pipeline remains an open challenge, particularly in\nproducing not just faithful but helpful responses that satisfy\nnuanced informational needs. Techniques such as agent self-\ncritique, counterfactual retrieval, or reflection-based loops may\noffer promising paths forward. We expect this work con-\ntribute a meaningful step toward more robust and interpretable\nretrieval-augmented systems in high-stakes, domain-specific\napplications.\nREFERENCES\n[1] V . Scotti and M. J. Carman, \u201cLlm support for real-time technical\nassistance,\u201d inJoint European Conference on Machine Learning and\nKnowledge Discovery in Databases. Springer, 2024, pp. 388\u2013393.\n[2] Cursor AI, \u201cCursor: The ai-first code editor,\u201d https://www.cursor.so,\n2024, accessed: 2025-06-25.\n[3] OpenAI, \u201cChatgpt: Optimizing language models for dialogue,\u201d https:\n//openai.com/blog/chatgpt, 2022, accessed: 2025-06-25.\n[4] E. Schuman, \u201c88% of AI pilots fail to reach production \u2014 but that\u2019s\nnot all on IT,\u201d https://www.cio.com/article/3850763/88-of-ai-pilots-fai\nl-to-reach-production-but-thats-not-all-on-it.html, Mar. 2025, accessed:\n2025-06-25.\n[5] A. Menshawy, Z. Nawaz, and M. Fahmy, \u201cNavigating challenges and\ntechnical debt in large language models deployment,\u201d inProceedings of\nthe 4th Workshop on Machine Learning and Systems, 2024, pp. 192\u2013199.\n[6] P. Giudici, \u201cFintech risk management: A research challenge for artificial\nintelligence in finance,\u201dFrontiers in Artificial Intelligence, vol. V olume\n1 - 2018, 2018. [Online]. Available: https://www.frontiersin.org/journa\nls/artificial-intelligence/articles/10.3389/frai.2018.00001\n[7] Mastercard, \u201cMdes for merchants (m4m): Mastercard digital enablement\nservice for merchants,\u201d https://www.mastercard.com/news/eemea/en/new\nsroom/press-releases/en/2020/august/mastercard-research-shows-surge\n-in-digital-payments-as-ecommerce-reaches-new-heights-in-the-uae/,\n2020, accessed: 2025-06-25.\n[8] \u2014\u2014, \u201cSwitchcore: Mastercard transaction switching platform,\u201d https:\n//www.mastercard.com/eea/switching-services/our-technology/network.\nhtml, 2025, accessed: 2025-06-25.\n[9] Y . Liu, D. Iter, Y . Xu, S. Wang, R. Xu, and C. Zhu, \u201cG-eval: Nlg\nevaluation using gpt-4 with better human alignment,\u201d 2023. [Online].\nAvailable: https://arxiv.org/abs/2303.16634\n[10] J. Chen, H. Lin, X. Han, and L. Sun, \u201cBenchmarking large language\nmodels in retrieval-augmented generation,\u201d inProceedings of the AAAI\nConference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 754\u2013\n17 762.\n[11] A. Singh, A. Ehtesham, S. Kumar, and T. T. Khoei, \u201cAgentic retrieval-\naugmented generation: A survey on agentic rag,\u201darXiv preprint\narXiv:2501.09136, 2025.\n[12] T. Nguyen, P. Chin, and Y .-W. Tai, \u201cMa-rag: Multi-agent retrieval-\naugmented generation via collaborative chain-of-thought reasoning,\u201d\narXiv preprint arXiv:2505.20096, 2025.\n[13] Pathway Community, \u201cAdaptive agents for real-time rag: Domain-\nspecific ai for legal, finance & healthcare,\u201d https://pathway.com/bl\nog/adaptive-agents-rag/, 2025.\n[14] R. C. Barron, V . Grantcharov, S. Wanna, M. E. Eren, M. Bhattarai,\nN. Solovyev, G. Tompkins, C. Nicholas, K. . Rasmussen, C. Matuszek,\nand B. S. Alexandrov, \u201cDomain-specific retrieval-augmented generation\nusing vector stores, knowledge graphs, and tensor factorization,\u201darXiv\npreprint arXiv:2410.02721, 2024.\n[15] H. Daiya, \u201cLeveraging large language models (llms) for enhanced risk\nmonitoring in fintech,\u201dIEEE Computer Society Tech News, 2024.\n[16] M. Broughton, \u201cLarge language models: How they help fintechs,\u201d https:\n//thepaymentsassociation.org/article/large-language-models-how-they-h\nelp-fintechs/, 2024.\n[17] Lumenova AI, \u201cAi in finance: The promise and risks of rag,\u201d https:\n//www.lumenova.ai/blog/ai-finance-retrieval-augmented-generation/,\n2024.\n[18] P. H. Leal, \u201cBuilding a financial education chatbot with retrieval-\naugmented generation (rag),\u201d https://medium.com/@hlealpablo/bui\nlding-a-financial-education-chatbot-with-retrieval-augmented-generatio\nn-rag-bf338aa2df09, 2023.",
  "full_text_length": 40104,
  "link_pdf": "https://arxiv.org/pdf/2510.25518v1",
  "paper_id": "2510.25518v1"
}