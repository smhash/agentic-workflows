{
  "source": "arxiv",
  "query": "Cycle Generative Adversarial Networks",
  "fetched_at": "2025-11-21T18:06:47.054291",
  "title": "Line Art Colorization of Fakemon using Generative Adversarial Neural Networks",
  "url": "http://arxiv.org/abs/2307.05760v1",
  "content": "Line Art Colorization of Fakemon using Generative\nAdversarial Neural Networks\nErick Oliveira Rodrigues\nDepartment of Academic Informatics (DAINF)\nUniversidade Tecnologica Federal do Parana\nPato Branco, Brazil\nerickrodrigues@utfpr.edu.br\nEsteban Clua\nComputer Science Department\nUniversidade Federal Fluminense\nNiteroi, Brazil\nesteban@ic.uff.br\nGiovani Bernardes Vitor\nInstitute of Technological Sciences\nUniversidade Federal de Itajuba\nItabira, Brazil\ngiovanibernardes@unifei.edu.br\nAbstract\u2014This work proposes a complete methodology to\ncolorize images of Fakemon, anime-style monster-like creatures.\nIn addition, we propose algorithms to extract the line art from\ncolorized images as well as to extract color hints. Our work is\nthe first in the literature to use automatic color hint extraction,\nto train the networks specifically with anime-styled creatures and\nto combine the Pix2Pix and CycleGAN approaches, two different\ngenerative adversarial networks that create a single final result.\nVisual results of the colorizations are feasible but there is still\nroom for improvement.\nIndex Terms\u2014line art colorization, fakemon, anime coloriza-\ntion, generative adversarial neural networks\nI. I NTRODUCTION\nColorization of images is an important stage in 2D asset\nproduction for digital games, animations and digital content\nproduction. Colorization of grey-level images can be traced\nback to early 2000\u2019s [1], [2] where optimization methods [3],\n[4] were used to convert the grey-level pixels to target colored\npixels. Eventually, machine learning started to be used in the\nsame context. Lipowezky [5] used classical classification and\nfeature extraction [6], [7]. Later, approaches shifted towards\ndeep learning [8], which includes the usage of Generative\nAdversarial Networks (GANs), and is now mainstream.\nIn 2017, Zhang et al. [9] proposed a robust method for\nimage colorization, where one of their results can be seen in\nFigure 1. One major limitation is that it requires the user to\npre-define a \u201ccolor palette\u201d, which is also called color hint, as\nthey are associated to the position where the color occur in\nthe image. Their best results cannot be obtained without the\ncolor hint.\nFig. 1. Image colorization example by Zhang et al. [9].\nTo this date [10], [11], grey-level image colorization is\nstill an open problem. First, we can have different styles\nof colorization, which leaves room for different approaches.\nThe second issue is image resolution. GANs do not work\nreally well with large resolutions, and the colored images\nare usually 256x256, eventually 512x512. Large resolutions\nusually require huge networks that consume lots of memory\nand require much time to train, while still not producing high\nquality results.\nA third issue is artefact generation. In some cases its pos-\nsible to spot artefacts such as a repeated colorization pattern\nthat occurs over the entire image. A fourth issue is related to\nthe image domain. Certain types of grey-level images obtain\nbetter colorization results. Works coined towards a single type\nof image tend to excel generic approaches.\nIt is also possible to find other variations of image coloriza-\ntion in the literature, which includes image colorization from\ninfrared images [12], [13]. However, these are not the primary\nfocus of this work. We are interested in line art colorization for\nthe generation or the acceleration of asset creation for games\nand entertainment.\nAround 2016, we start to get online approaches for line\nart colorization such as the PaintsChainer [14]. Line art\ncolorization is a different problem when compared to image\ncolorization. In line art, we just have two main colors as input\n(black and white - eventually a few shades of gray), as opposed\nto the image colorization case, where we have rich gray-level\ninformation. In this sense, the line art colorization problem\nrequires you to fill in white spaces with varying colors and\nshades. In line art, the information that can be used is mostly\nedge, area and location of the pixel (x and y coordinates).\nLater in 2018, Zhang et al. [15] also proposed an approach\nfor line art colorization that is similar to their previous photo\ncolorization [9]. Figure 2 shows one particular example that\nincludes color hints (squares - before the colorization) and\nadjustment color hints (circles - after the colorization).\nMost line art works in the literature focus on \u201chu-\nmanoid\u201d characters, which is not directly applicable to crea-\ntures/monsters in the kinds of Pokemon and Digimon. In this\nwork, we use the so called \u201cFakemons\u201d, which are fan-art\nmonsters inspired by the Pokemon and Digimon franchises.\nThis is the first work to pursue this scenario. We also evaluate978-1-6654-6156-6/22/$31.00 \u00a92022 IEEE\narXiv:2307.05760v1  [cs.CV]  11 Jul 2023\n\nFig. 2. Line art colorization example by Zhang et al. [15].\nthe usage of automatic color hints, which is also used for\ntraining. Besides, we compare and combine the performance\nof two different GANs, the Pix2Pix network [16] as well as\nCycleGAN [17]. Combining both networks is also a novel\ncontribution that has never been explored.\nThe main contributions of this work are: (1) proposal of\nan automatic and adaptive extraction of line arts, (2) proposal\nof the first automatic proposal for color hint extraction, (2)\nfirst time combination of the responses of the Pix2Pix and\nCycleGAN and (3) first time line art colorization of monster-\nlike creatures. In what follows, we provide a literature review,\nproposed methodology, obtained results and conclusion.\nII. L ITERATURE REVIEW\nCi et al. [18] use a GAN and color hints for line art\ncolorization. Similar to [15], this work also focus on \u201chu-\nmanoid\u201d characters. Fang et al. [19] also use a GAN and\nconsider different styles of colorization. The obtained results\nare visually pleasing but are mainly anime faces.\nThe MANGAN approach [20] also uses a GAN, where the\nauthors came up with their own methodology to extract the\nline art from colorized images in order to train the network.\nTheir line art extraction contains a lot of noise and generated\nlines are thicker than usual. This extraction can influence other\nworks to under-perform. Hence, the comparison may not be\nthe fairest. Furthermore, the authors also work with colorized\n\u201chumanoid\u201d anime characters.\nAnother remarkable factor: the authors apply a Gaussian\nblur to the color hint, which spreads the colors throughout the\nimage. As a matter of analysis, we also applied a Gaussian\nblur in our approach but it does not heavily influence the end\nresult of Pix2Pix and CycleGAN. A line or a circular spot is\nsufficient for Pix2Pix to translate the image appropriately.\nPaintsTorch [21] uses a GAN architecture that is similar to\nCi et al. [18]. Their results are pleasing, one of the best results\nin the literature. However, again, the authors use \u201chumanoid\u201d\ncharacters and their method uses manually placed color hint.\nFigure 3 shows their result. We would like to draw attention to\nFig. 3. Line art colorization result obtained by Hati et al. [21].\nthe amount of color hints, which is fairly significant, specially\nfor the image at the bottom.\nSerpa et al. [22], on the other hand, used the Pix2Pix\nframework to create the shading of 2D sprites in order to\naccelerate their production. The authors show that they can\nrecreate shading under controlled circumstances. Strange poses\nincrease the difficulties to recreate the shading. This work,\nhowever, is not focused on line art colorization. It bears\nsimilarities to our approach as it uses Pix2Pix.\nPix2Pix is an image-to-image translation framework based\non a GAN. In contrast to Convolutional Neural Networks\n(CNNs), GANs learn a loss function that classifies whether\nthe output image is real or fake, at the same time that they\ntrain a generative model that minimize the loss [16]. Pix2Pix\nuses a \u201cU-Net\u201d based architecture for the generator and a\n\u201cPatchGAN\u201d classifier for the discriminator. The generator\ncreates an image and the discriminator is responsible for\nchecking whether the generated image is fake. These two\ndeep neural networks working together compose the GAN\narchitecture. The idea behind Pix2Pix is to provide a general\nimage-to-image framework that works in all sorts of problems\nand is not application specific.\nThe authors [16] mention that classical pixel classification\ntreats the output as \u201cunstructured\u201d data, as if the informa-\ntion of the class of a certain pixel does not influence on\nthe classification of neighbouring pixels. Conditional GANs\n(or cGANs), contrariwise, learn a \u201cstructured loss\u201d function,\nwhere this information \u201cpropagates\u201d over areas of pixels.\nPix2Pix is actually a cGAN. Recent works [23] have also ex-\nplored this approach coupled with classical machine learning\n(manually coined feature extraction + classifiers), and obtained\ngreat results, which means that classifiers other than neural\nnetworks can be used and adapted to the \u201cstructured\u201d idea.\nThis approach is also called \u201cconnectivity\u201d.\nBesides Pix2Pix, we also evaluated the CycleGAN frame-\nwork, also proposed by Isola [17], author of the Pix2Pix\nframework. CycleGAN is not the best framework of choice\nfor our problem, as it actually transfers styles of a group of\n\nimages to another group of images, it does not provide an\nimage-to-image translation. CycleGAN is more recommended\nto cases where the pair of images (line art and colorized ones)\nare not aligned. However, we combine Pix2Pix and CycleGAN\nto produce colorized and shaded images.\nIII. P ROPOSED METHODOLOGY\nWe collected a total of 880 colorized images of Fakemon\ncharacters in websites such as DeviantArt, while also including\na few examples made by us. We carefully collected pieces\nof art that use the Creative Commons license. We would\nlike to highlight that we do not hold the copyright for the\ncreatures shown in this work. The collected images were used\nto train the Pix2Pix and CycleGAN models. We separated a\nfew \u201cFakemons\u201d to be shown as result in this work that were\nnot included in the training dataset. In the end of the work we\nprovide the credits for the used creatures.\nThe line arts and color hints were automatically extracted\nfrom the collected images, which differs from works in the\nliterature. Some works include the color pallet later in the\nnetwork architecture or treat it as a separate input image.\nContrariwise, we actually paint the line art image using\ncircular spots of color hints and do not alter the standard\narchitecture of the network. Therefore, we actually work with\na single input image that contains the line art and the color\nhints.\nA. Line art extraction\nFirst, we use an automatic methodology for the extraction of\nthe line arts, later visiting each one of the characters to verify\nif the line art was extracted properly. We manually improved\nthe lines in a few cases. However, in real situations, the line\nart is produced by the artist and this line art extraction step\nis not necessary. This extraction was required just to create\ndata for training, as it is nearly impossible to find a dataset or\nan adequate number of pairs of images containing the line art\nand its colorized version.\nTo extract the line art, we use an adaptive threshold, shown\nin Algorithm 1. We write the tolerance variable according to\nthe number of pixels that compose the character. Larger and\nmore complex characters tend to have thinner lines while small\ncharacters tend to have thicker lines. Thus, we increase the\ntolerance variable if the amount of pixels in the character is\nlow (i.e., pixels that are not background info). Increasing the\ntolerance variable means that the algorithm will select darker\nshades of gray for the threshold value. More info on classical\nimage thresholds can be found at [24].\nB. Color hint extraction\nAfter the extraction of the line art, the color hint is esti-\nmated. The approach we used is shown in Algorithm 2. This is\na k-Medoid clustering algorithm [25] that skips the transparent\npixels and uses an unconventional distance that we adjusted\nempirically. The used distance is shown in Equation 1 and r,\ng and b represent the color layers. p1 and p2 represent the\npixels.\nAlgorithm 1: Adaptive threshold algorithm.\nData: Input image and the tolerance variable, which is a\nnumber chosen by the user.\nResult: Thresholded binary image containing the line art.\n1 Convert the input image to gray;\n2 Construct the histogram of the input image;\n3 m \u2190 Find the grey-level that corresponds to the average of\nthe occurrences in the histogram and set to m;\n4 g \u2190 Order the shades of gray and include in a vector g;\n5 Get the highest shade of gray v in g that satisfies\nv \u2217 tolerance < m;\n6 Apply the classical threshold operation at the value v;\nThe distance in Equation 1 does not use the x and y\ninformation, and therefore we may end up with clusters of a\nsingle color that are much wider than other colors. After that,\nwe apply the k-Medoid algorithm again. For the first time,\nshown in Algorithm 2, we use k = 35 to quantize the images\nin a total of at most 35 colors. This k = 35 is empirical, but\nit depends on the objective. Lower values for k (less colors)\nwould reduce the time spent by the artist when placing manual\ncolor hints over the line art.\nAlgorithm 2: Automatic color hint algorithm.\nData: p1 and p2 being the pixels or points, getHue function\ncalculates the usual hue (base color) and getSat\ncalculates the usual saturation (color intensity).\nResult: Image with the color hints.\n1 Initialize every k cluster, each cluster starts with a single\npixel that is selected randomly;\n2 Iterate over the non-transparent pixels of the image and\nassociate each one of them to one of the k clusters using\nthe distance in Equation 1;\n3 Recalculate the cluster centroids according to the distance in\nEquation 1 (the centroid must be an actual pixel that exists\n- that is not transparent);\n4 Return to step 2 and repeat the process until convergence;\n5 After convergence, iterate the clusters, get the final centroids\nand draw a circle with the corresponding color in this\nposition;\nd(p1, p2) = (getHue(p1.r \u2217 p1.r, p1.g \u2217 p1.g, p1.b \u2217 p1.b)\n\u2212getHue(p2.r \u2217 p2.r, p2.g \u2217 p2.g, p2.b \u2217 p2.b))2\n+(1.5 \u2217 getSat(0.8 \u2217 p1.r \u2217 p1.r,0.8 \u2217 p1.g \u2217 p1.g, p1.b \u2217 p1.b)\n\u2212getSat(0.8 \u2217 p2.r \u2217 p2.r,0.8 \u2217 p2.g \u2217 p2.g, p2.b \u2217 p2.b))2\n(1)\nLater, for the second time, the k-Medoid algorithm is used\nwith a slight different k and distance so that we can have\nuniformly spaced different clusters (or color hints) with the\nsame color. The distance used in this second time is the\nEuclidean distance, with a total of 5 dimensions r, g, b, x\nand y and equal weights for all of them. For that second time,\nk is set to 10. Therefore, we automatically generate a total of\n10 color hints, where each color hint is represented as a circle\nof radius 15 in pixels. The result of this processing can be\nseen in Figure ??. The top-left image in this figure has a total\n35 colors (due to the first k), and a total of 10 circular color\nspots (due to the second k). The top-right image represents\n\nFig. 4. The top-left image is the original art by Dragonith\n(bit.ly/3tQoNLU) quantized to 35 colors. Top-right is the same\nimage after line art and color hint extraction (10 total). Bottom-\nleft is the color result colorized by Pix2Pix and bottom-right\nby CycleGAN.\nthe color hint extraction. The bottom row shows the Pix2Pix\nand CycleGAN automatic colorizations.\nIt is possible to increase the number of color hints. However,\nif we train the algorithm using more color hints, the artist\nwould be required to insert more color hints as well, and\nwe want the colorization to be performed in the easiest way\npossible. We performed some experiments using more color\nhints and concluded that the results improve, but we still chose\nto stick to k = 10. This trade-off should be analysed carefully.\nWe adjusted the parameters of both Pix2Pix and CycleGAN\nempirically. The parameters do not influence that much on the\nfinal result. However, we increased the ngf and ndf parameters\n(ngf: number of generator filters in the last conv layer, ndf:\nnumber of discriminator filters in the first conv layer) to 150\nas they provide better results.\nIV. R ESULTS\nFigure 5 shows the colorization obtained with Pix2Pix.\nThese results were obtained directly from the automatic color\nhint extraction shown in Algorithm 2. We did not use any\nspecific metric to measure the accuracy of the colorization as\nwe adhered to the evaluation performed by other works in\nthe literature, which are based on human visual observation.\nWe can argue that the results are acceptable and that they\nare suitable for game art and entertainment as is. The style\nof result kind of reminds watercolor. The boundaries of the\nimages were well colored. No case presented colorizations that\nexceeded the line art limits.\nFigure 6 shows some results obtained with CycleGAN. We\nare not sure why the colors did not vary that much, as other\nthe colors varied in other experiments with CycleGAN. One\nclear difference is that it is capable of grasping the darker and\nlighter shades better than Pix2Pix, even providing a type of\n\u201cdither\u201d where it should be lighter such as in the heads of the\ntwo first Fakemon in Figure 6. Pix2Pix, on the other hand,\nworks better with the colors.\nConsidering that both approaches have interesting aspects\n(one the color and the other the shading), we tried to combine\nthem. To our surprise, the combination yielded interesting\nresults. If we divide (blend mode) the Pix2Pix result by the\nCycleGAN result, we get the response shown in Figure 7, with\nsoft colors and shading.\nAs a final experiment, we manually included several color\nhints and analysed how the Pix2Pix perform. The result can\nbe seen in Figure 8. Results do improve in some areas of the\nimage. Overall, the improvement is not substantial.\nAll the experiments in this work were run using an Intel i7-\n10700 CPU and a Nvidia RTX 3060 with 12GB of memory.\nThe Pix2Pix trained for 2 days, past epoch 1000, while the\nCycleGAN trained for 4 days straight and reached epoch\n300. We experimented with previous epochs to double check\nfor overfitting, but the results did not seem to improve. The\nparameters were mostly the standard ones, we increased the\nngf and ndf parameters to the maximum of memory the GPU\nwould support (150 for Pix2Pix and 128 for CycleGAN).\nV. C ONCLUSION\nThis work performs experiments towards the automatic col-\norization of Fakemons, monster-like characters. We collected\na total of 880 images that fit in the \u201cFakemon\u201d category for\ntraining. Contributions of this work include the algorithm for\nthe extraction of the line art as well as the automatic extraction\nand generation of the color hints. Besides, a major contribution\nis colorizing anime-styled creatures, which is the first occasion\nin the literature. We are also the first to experiment with a small\namount of color hints and to combine Pix2Pix and CycleGAN\nin the same result.\nThe first major conclusion is that there is still a lot of room\nfor improvement in automatic colorization. Even with a fairly\nlarge dataset we still had to combine the approaches to obtain\narguably adequate results (Figure 7). The results can still be\nimproved to look more like the original pieces. Furthermore,\nwe also have a limitation with respect to the resolution, the\nimages were 256x256.\nHowever, we also conclude that the results are feasible,\nthey remind the watercolor style and can be used as is in\ngames and entertainment. One particular advantage is that all\nthe generated colorization appear to be colorized with the\nsame art style, irrespective of the author who created it. These\napproaches can be used, for instance, to \u201cstandardize\u201d pieces\nof art from different authors.\nWe used frameworks (Pix2Pix and CycleGAN) that are\ngeneric for this type of task (image to image translation and\nstyle transfer), coined to work with all sorts of problems.\n\n(a) Automatic color hint.\n(b) Colorized by the proposed\nmethod (Pix2Pix).\n(c) Original art by Edari.\nhttps://bit.ly/3y8Vvup\n (d) Automatic color hint.\n(e) Colorized by the proposed\nmethod (Pix2Pix).\n(f) Original art by Bombeetle.\nhttps://bit.ly/3QC18sc\n (g) Automatic color hint.\n(h) Colorized by the proposed\nmethod (Pix2Pix).\n(i) Original art by Bombeetle.\nhttps://bit.ly/3QC18sc\n (j) Automatic color hint.\n(k) Colorized by the proposed\nmethod (Pix2Pix).\n(l) Original art by Bombeetle.\nhttps://bit.ly/3xGbOxq\nFig. 5. Results of the colorization obtained with Pix2Pix.\nFuture works may also include the creation of frameworks and\napproaches coined specifically for this task, aiming to improve\nthe results and similarity to the original piece.\nAlthough the colorization of Fakemon and usual anime art\nmay sound like the same thing, they are actually very different\napproaches. Color palettes are different, line art is different,\nthe background information is different, etc. This justifies the\ncreation of a framework for this type of task as future work,\nas well as the use of specific images for training.\nACKNOWLEDGMENT\nAll the art shown in this work was extracted from\nDeviantArt.com. We carefully selected arts that conform to\nthe Creative Commons license, which can be found at their\nwebsite. In this manuscript, we include pieces from the artists\ncalled Dragonith (https://www.deviantart.com/dragonith),\nEdari (https://www.deviantart.com/edari) and Bombeetle\n(https://www.deviantart.com/bombeetle), which can be found\nat their profile page.\nREFERENCES\n[1] D. Nie, L. Ma, S. Xiao, and X. Xiao. Grey-scale image colorization\nby local correlation based optimization algorithm. Lecture Notes in\nComputer Science, 3736:13\u201323, 2005.\n[2] Y . Tai, J. Jia, and C.K. Tang. Local color transfer via probabilistic\nsegmentation by expectation-maximization. IEEE Computer Society\nConference on Computer Vision and Pattern Recognition , 2005.\n[3] E. O. Rodrigues, L. O. Rodrigues, L. S. N. Oliveira, A. Conci, and\nP. Liatsis. Automated recognition of the pericardium contour on\nprocessed ct images using genetic algorithms. Computers in Biology\nand Medicine, 87:38\u201345, 2017.\n[4] E. O. Rodrigues, P. Liatsis, L. Satoru, and A. Conci. Fractal triangular\nsearch: a metaheuristic for image content search. IET Image Processing,\n12:1475\u20131484, 2018.\n[5] U. Lipowezky. Grayscale aerial and space image colorization using\ntexture classification. Pattern Recognition Letters, 27, 2006.\n[6] E. O. Rodrigues, A. Conci, F. F. C. Morais, and M. G. Perez. Towards\nthe automated segmentation of epicardial and mediastinal fats: A multi-\nmanufacturer approach using intersubject registration and random forest.\nIEEE International Conference on Industrial Technology (ICIT) , 2015.\n[7] E. O. Rodrigues, A. Conci, and P. Liatsis. Morphological classifiers.\nPattern Recognition, 84:82\u201396, 2018.\n\nFig. 6. CycleGAN result for the same input parameters as\nFigure 5.\nFig. 7. Combination of the Pix2Pix and CycleGAN responses\n(one divided by the other - blend mode in Photoshop). This\napproach is purely automatic, line art and color hints (10 total,\nk = 10) were extracted automatically.\nFig. 8. Pix2Pix result with manually positioned 30 color hints.\nResults did not improve substantially.\n[8] S. Huang, X. Jin, Q. Jiang, and L. Liu. Deep learning for image\ncolorization: Current and future prospects. Engineering Applications\nof Artificial Intelligence , 114, 2022.\n[9] R. Zhang, J. Y . Zhu, P. Isola, X. Geng, A. S. Lin, T. Yu, and A. A. Efros.\nReal-time user-guided image colorization with learned deep priors. ACM\nTransactions on Graphics, 36, 2017.\n[10] Y . Xiao, A. Jiang, C. Liu;, and M. Wang. Single image colorization via\nmodified cyclegan. IEEE International Conference on Image Processing\n(ICIP), 2019.\n[11] S. Y . Chen, J. Q. Zhang, Y . Y . Zhao, P. L. Rosin, Y . K. Lai, and L. Gao. A\nreview of image and video colorization: From analogies to deep learning.\nVisual Informatics, 2022.\n[12] H. Kim and J. Kim. Image-to-image translation for near-infrared image\ncolorization. International Conference on Electronics, Information, and\nCommunication (ICEIC), 2022.\n[13] F. Luo, Y . Li, G. Zeng, P. Peng, G. Wang, and Y . Li. Thermal infrared\nimage colorization for nighttime driving scenes with top-down guided\nattention. IEEE Transactions on Intelligent Transportation Systems ,\npages 1\u201316, 2022.\n[14] Petallica Paint. https://petalica-paint.pixiv.dev, 2022.\n[15] L. Zhang, C. Li, T. T. Wong, Y . Ji, and C. Liu. Two-stage sketch\ncolorization. ACM Transactions on Graphics , 37:1\u201314, 2018.\n[16] P. Isola, J. Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image translation\nwith conditional adversarial networks. Computer Vision and Pattern\nRecognition Conference, 2017.\n[17] J. Y . Zhu T. Park, P. Isola, and A. A. Efros. Unpaired image-to-\nimage translation using cycle-consistent adversarial networks. IEEE\nInternational Conference on Computer Vision , 2017.\n[18] Y . Ci, X. Ma, Z. Wang, H. Li, and Z. Luo. User-guided deep anime line\nart colorization with conditional adversarial networks. ACM Multimedia\nConference, 2018.\n[19] T. T. Fang, D. M. V o, A. Sugimoto, and S. H. Lai. Stylized-colorization\nfor line arts. International Conference on Pattern Recognition (ICPR) ,\n2020.\n[20] F. C. Silva, P. A. L. Castro, H. R. Junior, and E. C. Marujo. Mangan:\nAssisting colorization of manga characters concept art using conditional\ngan. IEEE International Conference on Image Processing (ICIP) , 2019.\n[21] Y . Hati, G. Jouet, F. Rousseaux, and C. Duhart. Paintstorch: a user-\nguided anime line art colorization tool with double generator conditional\nadversarial network. European Conference on Visual Media Production,\n2019.\n[22] Y . R. Serpa and M. A. F. Rodrigues. Towards machine-learning\nassisted asset generation for games: A study on pixel art sprite sheets.\nProceedings of SBGames , 2019.\n[23] E. O. Rodrigues, A. Conci, and P. Liatsis. Element: Multi-modal retinal\nvessel segmentation based on a coupled region growing and machine\nlearning approach. IEEE Journal of Biomedical and Health Informatics ,\n24, 2020.\n[24] E. O. Rodrigues, T. M. Porcino, A. Conci, and A. C. Silva. A simple\napproach for biometrics: Finger-knuckle prints recognition based on\na sobel filter and similarity measures. International Conference on\nSystems, Signals and Image Processing (IWSSIP) , 2016.\n[25] E. O. Rodrigues, L. Torok, P. Liatsis, J. Viterbo, and A. Conci. k-ms:\nA novel clustering algorithm based on morphological reconstruction.\nPattern Recognition, pages 392\u2013403, 2017.",
  "authors": [
    "Erick Oliveira Rodrigues",
    "Esteban Clua",
    "Giovani Bernardes Vitor"
  ],
  "summary": "This work proposes a complete methodology to colorize images of Fakemon, anime-style monster-like creatures. In addition, we propose algorithms to extract the line art from colorized images as well as to extract color hints. Our work is the first in the literature to use automatic color hint extraction, to train the networks specifically with anime-styled creatures and to combine the Pix2Pix and CycleGAN approaches, two different generative adversarial networks that create a single final result. Visual results of the colorizations are feasible but there is still room for improvement.",
  "pdf_url": "https://arxiv.org/pdf/2307.05760v1",
  "entry_id": "http://arxiv.org/abs/2307.05760v1",
  "published": "2023-07-11",
  "updated": "2023-07-11",
  "comment": "art generation, lineart colorization, image colorization, generative adversarial networks, stable diffusion, pokemon, fakemon, digimon",
  "journal_ref": "2022 21st Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)",
  "doi": "10.1109/sbgames56371.2022.9961078",
  "primary_category": "cs.CV",
  "categories": [
    "cs.CV"
  ],
  "links": [
    {
      "href": "https://arxiv.org/abs/2307.05760v1",
      "rel": "alternate",
      "title": null
    },
    {
      "href": "https://arxiv.org/pdf/2307.05760v1",
      "rel": "related",
      "title": "pdf"
    },
    {
      "href": "https://doi.org/10.1109/sbgames56371.2022.9961078",
      "rel": "related",
      "title": "doi"
    }
  ],
  "full_text": "Line Art Colorization of Fakemon using Generative\nAdversarial Neural Networks\nErick Oliveira Rodrigues\nDepartment of Academic Informatics (DAINF)\nUniversidade Tecnologica Federal do Parana\nPato Branco, Brazil\nerickrodrigues@utfpr.edu.br\nEsteban Clua\nComputer Science Department\nUniversidade Federal Fluminense\nNiteroi, Brazil\nesteban@ic.uff.br\nGiovani Bernardes Vitor\nInstitute of Technological Sciences\nUniversidade Federal de Itajuba\nItabira, Brazil\ngiovanibernardes@unifei.edu.br\nAbstract\u2014This work proposes a complete methodology to\ncolorize images of Fakemon, anime-style monster-like creatures.\nIn addition, we propose algorithms to extract the line art from\ncolorized images as well as to extract color hints. Our work is\nthe first in the literature to use automatic color hint extraction,\nto train the networks specifically with anime-styled creatures and\nto combine the Pix2Pix and CycleGAN approaches, two different\ngenerative adversarial networks that create a single final result.\nVisual results of the colorizations are feasible but there is still\nroom for improvement.\nIndex Terms\u2014line art colorization, fakemon, anime coloriza-\ntion, generative adversarial neural networks\nI. I NTRODUCTION\nColorization of images is an important stage in 2D asset\nproduction for digital games, animations and digital content\nproduction. Colorization of grey-level images can be traced\nback to early 2000\u2019s [1], [2] where optimization methods [3],\n[4] were used to convert the grey-level pixels to target colored\npixels. Eventually, machine learning started to be used in the\nsame context. Lipowezky [5] used classical classification and\nfeature extraction [6], [7]. Later, approaches shifted towards\ndeep learning [8], which includes the usage of Generative\nAdversarial Networks (GANs), and is now mainstream.\nIn 2017, Zhang et al. [9] proposed a robust method for\nimage colorization, where one of their results can be seen in\nFigure 1. One major limitation is that it requires the user to\npre-define a \u201ccolor palette\u201d, which is also called color hint, as\nthey are associated to the position where the color occur in\nthe image. Their best results cannot be obtained without the\ncolor hint.\nFig. 1. Image colorization example by Zhang et al. [9].\nTo this date [10], [11], grey-level image colorization is\nstill an open problem. First, we can have different styles\nof colorization, which leaves room for different approaches.\nThe second issue is image resolution. GANs do not work\nreally well with large resolutions, and the colored images\nare usually 256x256, eventually 512x512. Large resolutions\nusually require huge networks that consume lots of memory\nand require much time to train, while still not producing high\nquality results.\nA third issue is artefact generation. In some cases its pos-\nsible to spot artefacts such as a repeated colorization pattern\nthat occurs over the entire image. A fourth issue is related to\nthe image domain. Certain types of grey-level images obtain\nbetter colorization results. Works coined towards a single type\nof image tend to excel generic approaches.\nIt is also possible to find other variations of image coloriza-\ntion in the literature, which includes image colorization from\ninfrared images [12], [13]. However, these are not the primary\nfocus of this work. We are interested in line art colorization for\nthe generation or the acceleration of asset creation for games\nand entertainment.\nAround 2016, we start to get online approaches for line\nart colorization such as the PaintsChainer [14]. Line art\ncolorization is a different problem when compared to image\ncolorization. In line art, we just have two main colors as input\n(black and white - eventually a few shades of gray), as opposed\nto the image colorization case, where we have rich gray-level\ninformation. In this sense, the line art colorization problem\nrequires you to fill in white spaces with varying colors and\nshades. In line art, the information that can be used is mostly\nedge, area and location of the pixel (x and y coordinates).\nLater in 2018, Zhang et al. [15] also proposed an approach\nfor line art colorization that is similar to their previous photo\ncolorization [9]. Figure 2 shows one particular example that\nincludes color hints (squares - before the colorization) and\nadjustment color hints (circles - after the colorization).\nMost line art works in the literature focus on \u201chu-\nmanoid\u201d characters, which is not directly applicable to crea-\ntures/monsters in the kinds of Pokemon and Digimon. In this\nwork, we use the so called \u201cFakemons\u201d, which are fan-art\nmonsters inspired by the Pokemon and Digimon franchises.\nThis is the first work to pursue this scenario. We also evaluate978-1-6654-6156-6/22/$31.00 \u00a92022 IEEE\narXiv:2307.05760v1  [cs.CV]  11 Jul 2023\n\nFig. 2. Line art colorization example by Zhang et al. [15].\nthe usage of automatic color hints, which is also used for\ntraining. Besides, we compare and combine the performance\nof two different GANs, the Pix2Pix network [16] as well as\nCycleGAN [17]. Combining both networks is also a novel\ncontribution that has never been explored.\nThe main contributions of this work are: (1) proposal of\nan automatic and adaptive extraction of line arts, (2) proposal\nof the first automatic proposal for color hint extraction, (2)\nfirst time combination of the responses of the Pix2Pix and\nCycleGAN and (3) first time line art colorization of monster-\nlike creatures. In what follows, we provide a literature review,\nproposed methodology, obtained results and conclusion.\nII. L ITERATURE REVIEW\nCi et al. [18] use a GAN and color hints for line art\ncolorization. Similar to [15], this work also focus on \u201chu-\nmanoid\u201d characters. Fang et al. [19] also use a GAN and\nconsider different styles of colorization. The obtained results\nare visually pleasing but are mainly anime faces.\nThe MANGAN approach [20] also uses a GAN, where the\nauthors came up with their own methodology to extract the\nline art from colorized images in order to train the network.\nTheir line art extraction contains a lot of noise and generated\nlines are thicker than usual. This extraction can influence other\nworks to under-perform. Hence, the comparison may not be\nthe fairest. Furthermore, the authors also work with colorized\n\u201chumanoid\u201d anime characters.\nAnother remarkable factor: the authors apply a Gaussian\nblur to the color hint, which spreads the colors throughout the\nimage. As a matter of analysis, we also applied a Gaussian\nblur in our approach but it does not heavily influence the end\nresult of Pix2Pix and CycleGAN. A line or a circular spot is\nsufficient for Pix2Pix to translate the image appropriately.\nPaintsTorch [21] uses a GAN architecture that is similar to\nCi et al. [18]. Their results are pleasing, one of the best results\nin the literature. However, again, the authors use \u201chumanoid\u201d\ncharacters and their method uses manually placed color hint.\nFigure 3 shows their result. We would like to draw attention to\nFig. 3. Line art colorization result obtained by Hati et al. [21].\nthe amount of color hints, which is fairly significant, specially\nfor the image at the bottom.\nSerpa et al. [22], on the other hand, used the Pix2Pix\nframework to create the shading of 2D sprites in order to\naccelerate their production. The authors show that they can\nrecreate shading under controlled circumstances. Strange poses\nincrease the difficulties to recreate the shading. This work,\nhowever, is not focused on line art colorization. It bears\nsimilarities to our approach as it uses Pix2Pix.\nPix2Pix is an image-to-image translation framework based\non a GAN. In contrast to Convolutional Neural Networks\n(CNNs), GANs learn a loss function that classifies whether\nthe output image is real or fake, at the same time that they\ntrain a generative model that minimize the loss [16]. Pix2Pix\nuses a \u201cU-Net\u201d based architecture for the generator and a\n\u201cPatchGAN\u201d classifier for the discriminator. The generator\ncreates an image and the discriminator is responsible for\nchecking whether the generated image is fake. These two\ndeep neural networks working together compose the GAN\narchitecture. The idea behind Pix2Pix is to provide a general\nimage-to-image framework that works in all sorts of problems\nand is not application specific.\nThe authors [16] mention that classical pixel classification\ntreats the output as \u201cunstructured\u201d data, as if the informa-\ntion of the class of a certain pixel does not influence on\nthe classification of neighbouring pixels. Conditional GANs\n(or cGANs), contrariwise, learn a \u201cstructured loss\u201d function,\nwhere this information \u201cpropagates\u201d over areas of pixels.\nPix2Pix is actually a cGAN. Recent works [23] have also ex-\nplored this approach coupled with classical machine learning\n(manually coined feature extraction + classifiers), and obtained\ngreat results, which means that classifiers other than neural\nnetworks can be used and adapted to the \u201cstructured\u201d idea.\nThis approach is also called \u201cconnectivity\u201d.\nBesides Pix2Pix, we also evaluated the CycleGAN frame-\nwork, also proposed by Isola [17], author of the Pix2Pix\nframework. CycleGAN is not the best framework of choice\nfor our problem, as it actually transfers styles of a group of\n\nimages to another group of images, it does not provide an\nimage-to-image translation. CycleGAN is more recommended\nto cases where the pair of images (line art and colorized ones)\nare not aligned. However, we combine Pix2Pix and CycleGAN\nto produce colorized and shaded images.\nIII. P ROPOSED METHODOLOGY\nWe collected a total of 880 colorized images of Fakemon\ncharacters in websites such as DeviantArt, while also including\na few examples made by us. We carefully collected pieces\nof art that use the Creative Commons license. We would\nlike to highlight that we do not hold the copyright for the\ncreatures shown in this work. The collected images were used\nto train the Pix2Pix and CycleGAN models. We separated a\nfew \u201cFakemons\u201d to be shown as result in this work that were\nnot included in the training dataset. In the end of the work we\nprovide the credits for the used creatures.\nThe line arts and color hints were automatically extracted\nfrom the collected images, which differs from works in the\nliterature. Some works include the color pallet later in the\nnetwork architecture or treat it as a separate input image.\nContrariwise, we actually paint the line art image using\ncircular spots of color hints and do not alter the standard\narchitecture of the network. Therefore, we actually work with\na single input image that contains the line art and the color\nhints.\nA. Line art extraction\nFirst, we use an automatic methodology for the extraction of\nthe line arts, later visiting each one of the characters to verify\nif the line art was extracted properly. We manually improved\nthe lines in a few cases. However, in real situations, the line\nart is produced by the artist and this line art extraction step\nis not necessary. This extraction was required just to create\ndata for training, as it is nearly impossible to find a dataset or\nan adequate number of pairs of images containing the line art\nand its colorized version.\nTo extract the line art, we use an adaptive threshold, shown\nin Algorithm 1. We write the tolerance variable according to\nthe number of pixels that compose the character. Larger and\nmore complex characters tend to have thinner lines while small\ncharacters tend to have thicker lines. Thus, we increase the\ntolerance variable if the amount of pixels in the character is\nlow (i.e., pixels that are not background info). Increasing the\ntolerance variable means that the algorithm will select darker\nshades of gray for the threshold value. More info on classical\nimage thresholds can be found at [24].\nB. Color hint extraction\nAfter the extraction of the line art, the color hint is esti-\nmated. The approach we used is shown in Algorithm 2. This is\na k-Medoid clustering algorithm [25] that skips the transparent\npixels and uses an unconventional distance that we adjusted\nempirically. The used distance is shown in Equation 1 and r,\ng and b represent the color layers. p1 and p2 represent the\npixels.\nAlgorithm 1: Adaptive threshold algorithm.\nData: Input image and the tolerance variable, which is a\nnumber chosen by the user.\nResult: Thresholded binary image containing the line art.\n1 Convert the input image to gray;\n2 Construct the histogram of the input image;\n3 m \u2190 Find the grey-level that corresponds to the average of\nthe occurrences in the histogram and set to m;\n4 g \u2190 Order the shades of gray and include in a vector g;\n5 Get the highest shade of gray v in g that satisfies\nv \u2217 tolerance < m;\n6 Apply the classical threshold operation at the value v;\nThe distance in Equation 1 does not use the x and y\ninformation, and therefore we may end up with clusters of a\nsingle color that are much wider than other colors. After that,\nwe apply the k-Medoid algorithm again. For the first time,\nshown in Algorithm 2, we use k = 35 to quantize the images\nin a total of at most 35 colors. This k = 35 is empirical, but\nit depends on the objective. Lower values for k (less colors)\nwould reduce the time spent by the artist when placing manual\ncolor hints over the line art.\nAlgorithm 2: Automatic color hint algorithm.\nData: p1 and p2 being the pixels or points, getHue function\ncalculates the usual hue (base color) and getSat\ncalculates the usual saturation (color intensity).\nResult: Image with the color hints.\n1 Initialize every k cluster, each cluster starts with a single\npixel that is selected randomly;\n2 Iterate over the non-transparent pixels of the image and\nassociate each one of them to one of the k clusters using\nthe distance in Equation 1;\n3 Recalculate the cluster centroids according to the distance in\nEquation 1 (the centroid must be an actual pixel that exists\n- that is not transparent);\n4 Return to step 2 and repeat the process until convergence;\n5 After convergence, iterate the clusters, get the final centroids\nand draw a circle with the corresponding color in this\nposition;\nd(p1, p2) = (getHue(p1.r \u2217 p1.r, p1.g \u2217 p1.g, p1.b \u2217 p1.b)\n\u2212getHue(p2.r \u2217 p2.r, p2.g \u2217 p2.g, p2.b \u2217 p2.b))2\n+(1.5 \u2217 getSat(0.8 \u2217 p1.r \u2217 p1.r,0.8 \u2217 p1.g \u2217 p1.g, p1.b \u2217 p1.b)\n\u2212getSat(0.8 \u2217 p2.r \u2217 p2.r,0.8 \u2217 p2.g \u2217 p2.g, p2.b \u2217 p2.b))2\n(1)\nLater, for the second time, the k-Medoid algorithm is used\nwith a slight different k and distance so that we can have\nuniformly spaced different clusters (or color hints) with the\nsame color. The distance used in this second time is the\nEuclidean distance, with a total of 5 dimensions r, g, b, x\nand y and equal weights for all of them. For that second time,\nk is set to 10. Therefore, we automatically generate a total of\n10 color hints, where each color hint is represented as a circle\nof radius 15 in pixels. The result of this processing can be\nseen in Figure ??. The top-left image in this figure has a total\n35 colors (due to the first k), and a total of 10 circular color\nspots (due to the second k). The top-right image represents\n\nFig. 4. The top-left image is the original art by Dragonith\n(bit.ly/3tQoNLU) quantized to 35 colors. Top-right is the same\nimage after line art and color hint extraction (10 total). Bottom-\nleft is the color result colorized by Pix2Pix and bottom-right\nby CycleGAN.\nthe color hint extraction. The bottom row shows the Pix2Pix\nand CycleGAN automatic colorizations.\nIt is possible to increase the number of color hints. However,\nif we train the algorithm using more color hints, the artist\nwould be required to insert more color hints as well, and\nwe want the colorization to be performed in the easiest way\npossible. We performed some experiments using more color\nhints and concluded that the results improve, but we still chose\nto stick to k = 10. This trade-off should be analysed carefully.\nWe adjusted the parameters of both Pix2Pix and CycleGAN\nempirically. The parameters do not influence that much on the\nfinal result. However, we increased the ngf and ndf parameters\n(ngf: number of generator filters in the last conv layer, ndf:\nnumber of discriminator filters in the first conv layer) to 150\nas they provide better results.\nIV. R ESULTS\nFigure 5 shows the colorization obtained with Pix2Pix.\nThese results were obtained directly from the automatic color\nhint extraction shown in Algorithm 2. We did not use any\nspecific metric to measure the accuracy of the colorization as\nwe adhered to the evaluation performed by other works in\nthe literature, which are based on human visual observation.\nWe can argue that the results are acceptable and that they\nare suitable for game art and entertainment as is. The style\nof result kind of reminds watercolor. The boundaries of the\nimages were well colored. No case presented colorizations that\nexceeded the line art limits.\nFigure 6 shows some results obtained with CycleGAN. We\nare not sure why the colors did not vary that much, as other\nthe colors varied in other experiments with CycleGAN. One\nclear difference is that it is capable of grasping the darker and\nlighter shades better than Pix2Pix, even providing a type of\n\u201cdither\u201d where it should be lighter such as in the heads of the\ntwo first Fakemon in Figure 6. Pix2Pix, on the other hand,\nworks better with the colors.\nConsidering that both approaches have interesting aspects\n(one the color and the other the shading), we tried to combine\nthem. To our surprise, the combination yielded interesting\nresults. If we divide (blend mode) the Pix2Pix result by the\nCycleGAN result, we get the response shown in Figure 7, with\nsoft colors and shading.\nAs a final experiment, we manually included several color\nhints and analysed how the Pix2Pix perform. The result can\nbe seen in Figure 8. Results do improve in some areas of the\nimage. Overall, the improvement is not substantial.\nAll the experiments in this work were run using an Intel i7-\n10700 CPU and a Nvidia RTX 3060 with 12GB of memory.\nThe Pix2Pix trained for 2 days, past epoch 1000, while the\nCycleGAN trained for 4 days straight and reached epoch\n300. We experimented with previous epochs to double check\nfor overfitting, but the results did not seem to improve. The\nparameters were mostly the standard ones, we increased the\nngf and ndf parameters to the maximum of memory the GPU\nwould support (150 for Pix2Pix and 128 for CycleGAN).\nV. C ONCLUSION\nThis work performs experiments towards the automatic col-\norization of Fakemons, monster-like characters. We collected\na total of 880 images that fit in the \u201cFakemon\u201d category for\ntraining. Contributions of this work include the algorithm for\nthe extraction of the line art as well as the automatic extraction\nand generation of the color hints. Besides, a major contribution\nis colorizing anime-styled creatures, which is the first occasion\nin the literature. We are also the first to experiment with a small\namount of color hints and to combine Pix2Pix and CycleGAN\nin the same result.\nThe first major conclusion is that there is still a lot of room\nfor improvement in automatic colorization. Even with a fairly\nlarge dataset we still had to combine the approaches to obtain\narguably adequate results (Figure 7). The results can still be\nimproved to look more like the original pieces. Furthermore,\nwe also have a limitation with respect to the resolution, the\nimages were 256x256.\nHowever, we also conclude that the results are feasible,\nthey remind the watercolor style and can be used as is in\ngames and entertainment. One particular advantage is that all\nthe generated colorization appear to be colorized with the\nsame art style, irrespective of the author who created it. These\napproaches can be used, for instance, to \u201cstandardize\u201d pieces\nof art from different authors.\nWe used frameworks (Pix2Pix and CycleGAN) that are\ngeneric for this type of task (image to image translation and\nstyle transfer), coined to work with all sorts of problems.\n\n(a) Automatic color hint.\n(b) Colorized by the proposed\nmethod (Pix2Pix).\n(c) Original art by Edari.\nhttps://bit.ly/3y8Vvup\n (d) Automatic color hint.\n(e) Colorized by the proposed\nmethod (Pix2Pix).\n(f) Original art by Bombeetle.\nhttps://bit.ly/3QC18sc\n (g) Automatic color hint.\n(h) Colorized by the proposed\nmethod (Pix2Pix).\n(i) Original art by Bombeetle.\nhttps://bit.ly/3QC18sc\n (j) Automatic color hint.\n(k) Colorized by the proposed\nmethod (Pix2Pix).\n(l) Original art by Bombeetle.\nhttps://bit.ly/3xGbOxq\nFig. 5. Results of the colorization obtained with Pix2Pix.\nFuture works may also include the creation of frameworks and\napproaches coined specifically for this task, aiming to improve\nthe results and similarity to the original piece.\nAlthough the colorization of Fakemon and usual anime art\nmay sound like the same thing, they are actually very different\napproaches. Color palettes are different, line art is different,\nthe background information is different, etc. This justifies the\ncreation of a framework for this type of task as future work,\nas well as the use of specific images for training.\nACKNOWLEDGMENT\nAll the art shown in this work was extracted from\nDeviantArt.com. We carefully selected arts that conform to\nthe Creative Commons license, which can be found at their\nwebsite. In this manuscript, we include pieces from the artists\ncalled Dragonith (https://www.deviantart.com/dragonith),\nEdari (https://www.deviantart.com/edari) and Bombeetle\n(https://www.deviantart.com/bombeetle), which can be found\nat their profile page.\nREFERENCES\n[1] D. Nie, L. Ma, S. Xiao, and X. Xiao. Grey-scale image colorization\nby local correlation based optimization algorithm. Lecture Notes in\nComputer Science, 3736:13\u201323, 2005.\n[2] Y . Tai, J. Jia, and C.K. Tang. Local color transfer via probabilistic\nsegmentation by expectation-maximization. IEEE Computer Society\nConference on Computer Vision and Pattern Recognition , 2005.\n[3] E. O. Rodrigues, L. O. Rodrigues, L. S. N. Oliveira, A. Conci, and\nP. Liatsis. Automated recognition of the pericardium contour on\nprocessed ct images using genetic algorithms. Computers in Biology\nand Medicine, 87:38\u201345, 2017.\n[4] E. O. Rodrigues, P. Liatsis, L. Satoru, and A. Conci. Fractal triangular\nsearch: a metaheuristic for image content search. IET Image Processing,\n12:1475\u20131484, 2018.\n[5] U. Lipowezky. Grayscale aerial and space image colorization using\ntexture classification. Pattern Recognition Letters, 27, 2006.\n[6] E. O. Rodrigues, A. Conci, F. F. C. Morais, and M. G. Perez. Towards\nthe automated segmentation of epicardial and mediastinal fats: A multi-\nmanufacturer approach using intersubject registration and random forest.\nIEEE International Conference on Industrial Technology (ICIT) , 2015.\n[7] E. O. Rodrigues, A. Conci, and P. Liatsis. Morphological classifiers.\nPattern Recognition, 84:82\u201396, 2018.\n\nFig. 6. CycleGAN result for the same input parameters as\nFigure 5.\nFig. 7. Combination of the Pix2Pix and CycleGAN responses\n(one divided by the other - blend mode in Photoshop). This\napproach is purely automatic, line art and color hints (10 total,\nk = 10) were extracted automatically.\nFig. 8. Pix2Pix result with manually positioned 30 color hints.\nResults did not improve substantially.\n[8] S. Huang, X. Jin, Q. Jiang, and L. Liu. Deep learning for image\ncolorization: Current and future prospects. Engineering Applications\nof Artificial Intelligence , 114, 2022.\n[9] R. Zhang, J. Y . Zhu, P. Isola, X. Geng, A. S. Lin, T. Yu, and A. A. Efros.\nReal-time user-guided image colorization with learned deep priors. ACM\nTransactions on Graphics, 36, 2017.\n[10] Y . Xiao, A. Jiang, C. Liu;, and M. Wang. Single image colorization via\nmodified cyclegan. IEEE International Conference on Image Processing\n(ICIP), 2019.\n[11] S. Y . Chen, J. Q. Zhang, Y . Y . Zhao, P. L. Rosin, Y . K. Lai, and L. Gao. A\nreview of image and video colorization: From analogies to deep learning.\nVisual Informatics, 2022.\n[12] H. Kim and J. Kim. Image-to-image translation for near-infrared image\ncolorization. International Conference on Electronics, Information, and\nCommunication (ICEIC), 2022.\n[13] F. Luo, Y . Li, G. Zeng, P. Peng, G. Wang, and Y . Li. Thermal infrared\nimage colorization for nighttime driving scenes with top-down guided\nattention. IEEE Transactions on Intelligent Transportation Systems ,\npages 1\u201316, 2022.\n[14] Petallica Paint. https://petalica-paint.pixiv.dev, 2022.\n[15] L. Zhang, C. Li, T. T. Wong, Y . Ji, and C. Liu. Two-stage sketch\ncolorization. ACM Transactions on Graphics , 37:1\u201314, 2018.\n[16] P. Isola, J. Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image translation\nwith conditional adversarial networks. Computer Vision and Pattern\nRecognition Conference, 2017.\n[17] J. Y . Zhu T. Park, P. Isola, and A. A. Efros. Unpaired image-to-\nimage translation using cycle-consistent adversarial networks. IEEE\nInternational Conference on Computer Vision , 2017.\n[18] Y . Ci, X. Ma, Z. Wang, H. Li, and Z. Luo. User-guided deep anime line\nart colorization with conditional adversarial networks. ACM Multimedia\nConference, 2018.\n[19] T. T. Fang, D. M. V o, A. Sugimoto, and S. H. Lai. Stylized-colorization\nfor line arts. International Conference on Pattern Recognition (ICPR) ,\n2020.\n[20] F. C. Silva, P. A. L. Castro, H. R. Junior, and E. C. Marujo. Mangan:\nAssisting colorization of manga characters concept art using conditional\ngan. IEEE International Conference on Image Processing (ICIP) , 2019.\n[21] Y . Hati, G. Jouet, F. Rousseaux, and C. Duhart. Paintstorch: a user-\nguided anime line art colorization tool with double generator conditional\nadversarial network. European Conference on Visual Media Production,\n2019.\n[22] Y . R. Serpa and M. A. F. Rodrigues. Towards machine-learning\nassisted asset generation for games: A study on pixel art sprite sheets.\nProceedings of SBGames , 2019.\n[23] E. O. Rodrigues, A. Conci, and P. Liatsis. Element: Multi-modal retinal\nvessel segmentation based on a coupled region growing and machine\nlearning approach. IEEE Journal of Biomedical and Health Informatics ,\n24, 2020.\n[24] E. O. Rodrigues, T. M. Porcino, A. Conci, and A. C. Silva. A simple\napproach for biometrics: Finger-knuckle prints recognition based on\na sobel filter and similarity measures. International Conference on\nSystems, Signals and Image Processing (IWSSIP) , 2016.\n[25] E. O. Rodrigues, L. Torok, P. Liatsis, J. Viterbo, and A. Conci. k-ms:\nA novel clustering algorithm based on morphological reconstruction.\nPattern Recognition, pages 392\u2013403, 2017.",
  "full_text_length": 26445,
  "link_pdf": "https://arxiv.org/pdf/2307.05760v1",
  "paper_id": "2307.05760v1"
}