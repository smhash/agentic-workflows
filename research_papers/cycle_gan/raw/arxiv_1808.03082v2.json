{
  "source": "arxiv",
  "query": "Cycle Generative Adversarial Networks",
  "fetched_at": "2025-11-21T18:06:51.321060",
  "title": "Paired 3D Model Generation with Conditional Generative Adversarial Networks",
  "url": "http://arxiv.org/abs/1808.03082v2",
  "content": "Paired 3D Model Generation with Conditional\nGenerative Adversarial Networks\nCihan \u00a8Ong\u00a8 un and Alptekin Temizel\nMiddle East Technical University, Ankara, Turkey\n{congun,atemizel}@metu.edu.tr\nAbstract. Generative Adversarial Networks (GANs) are shown to be\nsuccessful at generating new and realistic samples including 3D object\nmodels. Conditional GAN, a variant of GANs, allows generating samples\nin given conditions. However, objects generated for each condition are\ndi\ufb00erent and it does not allow generation of the same object in di\ufb00er-\nent conditions. In this paper, we \ufb01rst adapt conditional GAN, which is\noriginally designed for 2D image generation, to the problem of generat-\ning 3D models in di\ufb00erent rotations. We then propose a new approach\nto guide the network to generate the same 3D sample in di\ufb00erent and\ncontrollable rotation angles (sample pairs). Unlike previous studies, the\nproposed method does not require modi\ufb01cation of the standard condi-\ntional GAN architecture and it can be integrated into the training step\nof any conditional GAN. Experimental results and visual comparison of\n3D models show that the proposed method is successful at generating\nmodel pairs in di\ufb00erent conditions.\nKeywords: Conditional Generative Adversarial Network (CGAN)\u00b7Pair\nGeneration \u00b7Joint Learning \u00b73D Voxel Model\n1 Introduction\nWhile 3D technology mostly focuses on providing better tools for humans to\nscan, create, modify and visualize 3D data, recently there has been an interest\nin automated generation of new 3D object models. Scanning a real object is\nthe most convenient way to generate digital 3D object models, however, this\nrequires availability of real-life objects and each of these objects needs to be\nscanned individually. More crucially, it does not allow creating a novel object\nmodel. Creating a novel object model is a time consuming task requiring human\nimagination, e\ufb00ort and specialist skills. So it is desirable to have an automated\nsystem facilitating streamlined generation of 3D object content.\nGenerative models have recently become mainstream with their applications\nin various domains. Generative Adversarial Networks (GANs) [7] have been a\nrecent breakthrough in the \ufb01eld of generative models. GANs provide a generic so-\nlution for various types of data leveraging the power of arti\ufb01cial neural networks,\nparticularly Convolutional Neural Networks (CNN). On the other hand, use of\nGANs brings out several challenges. While stability is the most fundamental\nproblem in GAN architecture, there are also domain speci\ufb01c challenges.\nPublished in ECCV 2018 Workshops\nc\u20ddSpringer, LNCS 11129, pp. 473-487, 2019\nThe \ufb01nal authenticated publication is available online at\nhttps://doi.org/10.1007/978-3-030-11009-3 29\narXiv:1808.03082v2  [cs.CV]  15 Mar 2019\n\n2 C. \u00a8Ong\u00a8 un and A. Temizel\nStandard GAN model generates novel samples from an input distribution.\nHowever, the generated samples are random and there is no control over them\nas the input noise and the desired features are entangled. While some solutions\nattack the entanglement problem [6], some propose new types of GANs for spe-\nci\ufb01c purposes. Conditional GAN [15] allows controlling the characteristics of the\ngenerated samples using a condition. While these conditions could be speci\ufb01ed,\nthe generated samples are random and it fails to generate pair samples in dif-\nferent conditions [12,14]. Keeping the input value the same while changing the\ncondition value does not generate the same output in di\ufb00erent conditions because\nof the entanglement problem. The representation between input and output sam-\nple is entangled in such a way that changing condition value changes the output\ncompletely. There are many studies for learning joint distributions to generate\nnovel pair samples. Most of them uses modi\ufb01ed GAN architectures, complex\nmodels or paired training data as described in the Related Works section.\nIn this study, we propose a new approach to generate paired 3D models with\nConditional GANs. Our method is integrated as an additional training step\nto Conditional GAN without changing the original architecture. This generic\nsolution provides \ufb02exibility such that it is applicable to any conditional GAN\narchitecture as long as there is a metric to measure the similarity of samples\nin di\ufb00erent conditions. Also the system can be trained with paired samples,\nunpaired samples and without any tuples of corresponding samples in di\ufb00erent\ndomains.\nIn section 2, we describe the GAN architecture and the related works. The\nproposed method is given in section 3 and experimental evaluation and results\nare provided in section 4. Conclusions and future work are given in section 5.\n2 Related Works\nGAN architecture (Fig. 1(a)) consists of a generator model G and a discrim-\ninator model D [7]. Generator model takes an input code and generates new\nsamples. Discriminator model takes real and generated samples and tries to dis-\ntinguish real ones from generated ones. Generator and discriminator are trained\nsimultaneously so that while generator learns to generate better samples, dis-\ncriminator becomes better at distinguishing samples resulting in an improved\nsample generation performance at the end of the training.\nIf GAN is trained with training datax for discriminator Dand sampled noise\nz for generator G, D is used to maximize the correctly labeled real samples as\nreal log( D(x)) and generated samples as fake log(1 \u2212D(G(z))). On the other\nhand, generator G tries to fool the discriminator to label the generated data as\nreal so Gis used for minimizing log(1 \u2212D(G(z))) . These two models duel each\nother in a min-max game with the value function V(D,G). The objective of the\nwhole system can be formulated as:\nminGmaxD V(D,G) = Ex\u223cpdata(x)[log D(x)]\n+ Ez\u223cpz(z)[log (1 \u2212D(G(z)))]. (1)\n\nPaired 3D Model Generation with CGAN 3\nUse of CNN based GANs [16] is popular in 2-D image domain with various\napplications. Pix2pix [8] is a general-purpose GAN based solution to image-to-\nimage translation problems and it has been shown to be e\ufb00ective at problems\nsuch as synthesizing photos from label maps, reconstructing objects from edge\nmaps, and colorizing images. It uses GANs in conditional settings for image-\nto-image translation tasks, where a condition is given on an input image to\ngenerate a corresponding output image. Another application of GAN is style\ntransfer [13]. For an input image, the system can transfer the style of the ref-\nerence image including time of the day, weather, season and artistic edit to the\ntarget. Perceptual Adversarial Network (PAN) [18] provides a generic framework\nfor image-to-image transformation tasks such as removing rain streaks from an\nimage (image de-raining), mapping object edges to the corresponding image,\nmapping semantic labels to a scene image and image inpainting.\nThe fundamental principle of GANs, i.e. using two di\ufb00erent models trained\ntogether, causes stability problems. These two models must be in equilibrium to\nwork together in harmony. Since the architecture is based on dueling networks,\nduring the training phase, one of the models could overpower the other, causing\na stability problem. Wasserstein GAN [3] proposes a new distance metric to cal-\nculate the discriminator loss where Wasserstein distance (Earth-Mover distance)\nis used to improve the stability of learning and provide useful learning curves.\nIn [4] several approaches are introduced for regularizing the system to stabilize\nthe training of GAN models.\nGenerating 3D models with GANs is a relatively new area with a limited\nnumber of studies. The \ufb01rst and the most popular study uses an all-convolutional\nneural network to generate 3D objects [19]. In this work, the discriminator mostly\nmirrors the generator and 64x64x64 voxels are used to represent 3D models.\nWasserstein distance [17] is employed by normalizing with gradient penalization\nas a training objective to improve multiclass 3D generation. In another study an\nautoencoder network is used to generate 3D representations in latent space [2].\nGAN model generates new samples in this latent space and these samples are\ndecoded using the same autoencoder network to obtain 3D point cloud samples.\n3D meshes can also be used to train a GAN [9] to produce mesh-based 3D output.\nTo overcome the di\ufb03culty of working with mesh data, input data is converted to\nsigned distance \ufb01eld, then processed with two GAN architectures: low-frequency\nand high-frequency generator. After generating high and low-frequency samples,\noutputs are combined to generate a 3D mesh object.\nGenerator model of GAN uses a simple input noise vector z and it is possible\nthat the noise will be used by the generator in a highly entangled way, causing the\ninput vector z not correspond to semantic features of the output data. InfoGAN\n[6] is a method proposed to solve entanglement problem. To make a semantic\nconnection between input noise vector and output data, a simple modi\ufb01cation\nis presented to the generative adversarial network objective that encourages\nit to learn interpretable and meaningful representations. Generator network is\nprovided with both the incompressible noise z and the latent code c, so the form\n\n4 C. \u00a8Ong\u00a8 un and A. Temizel\nof input data becomes ( z,c). After necessary optimizations for combining these\nvalues, expected outputs can be generated with given parameters.\nConditional GAN (Fig. 1 (b)) is an extended version of GAN [15] condition-\ning both generator and discriminator on some extra information. While standard\nGAN models generate samples from random classes, CGANs can generate sam-\nples with a predetermined class for any input distribution such as generating\nspeci\ufb01c digits by using class labels as condition in MNIST dataset.\nInput noise z and condition value y are concatenated to use as input to the\ngenerator G. Training data x and condition value y are concatenated to use as\ninput to the discriminator D. With this modi\ufb01cation, the objective function of\nconditional GAN can be formulated as follows:\nminGmaxD V(D,G) = Ex\u223cpdata(x)[log D(x|y)]\n+ Ez\u223cpz(z)[log (1 \u2212D(G(z|y)))]. (2)\nConditional GANs can generate samples in given conditions, however they\nare not able to generate pairs for the same input and di\ufb00erent condition values.\nCoupled GAN (CoGAN) [12] is a new network model for learning a joint distri-\nbution of multi-domain images. CoGAN consists of a pair of GANs, each having\na generative and a discriminator model for generating samples in one domain.\nBy sharing of weights, the system generates pairs of images sharing the same\nhigh-level abstraction while having di\ufb00erent low-level realizations. DiscoGAN\n[10] aims to discover cross-domain relations with GANs. A similar approach is\nused in CycleGAN [21] where an image is used as input instead of a noise vector\nand it generates a new image by translating it from one domain to another.\nSyncGAN [5] employs an additional synchronizer model for multi-modal genera-\ntion like sound-image pairs. AlignGAN [14] adopts a 2-step training algorithm to\nlearn the domain-speci\ufb01c semantics and shared label semantics via alternating\noptimization.\n3 Proposed Method\nWhile the standard GAN model can generate realistic samples, it basically gen-\nerates random samples in given input distribution and does not provide any\ncontrol over these generated samples. For example, when a chair dataset is used\nto train the network, it generates chairs without any control over its character-\nistics such as its rotation. Conditional GANs provide control over the generated\nsamples by training the system with given input conditions. For example, if ro-\ntation is used as a condition value for chair dataset, system can generate samples\nwith a given rotation.\nFor both standard GAN and conditional GAN, the representation between\nthe input and the output is highly entangled such that changing a value in the\ninput vector changes the output in an unpredictable way. For example, for chair\ndataset, each chair generated by standard GAN would be random and it would\nbe created in an unknown orientation. Conditional GAN allows speci\ufb01cation of\n\nPaired 3D Model Generation with CGAN 5\n(a)\n(b)\nFig. 1.(a) Standard GAN and (b) conditional GAN architectures.\na condition Input vector z and condition value y are concatenated and given\ntogether as input to the system so input becomes ( z|y). As the condition value\ny is also an input value, changing the condition also changes the output. Even\nif the input vector z is kept the same, the model generates di\ufb00erent independent\nsamples in given conditions and does not allow generating the same sample\nin di\ufb00erent conditions [12,14]. For example, for chair dataset, if the condition\nis rotation, system generates a chair in \ufb01rst rotation and a di\ufb00erent chair in\ndi\ufb00erent rotation. As these objects are di\ufb00erent, they cannot be merged at a\nlater processing stage to create a new sample with less artifacts.\nTo overcome this problem, we propose incorporating an additional step in\ntraining to guide the system to generate the same sample in di\ufb00erent conditions.\nThe pseudo code of the method is provided in Algorithm 1 and Fig. 2 illustrates\nthe proposed method for the 2-condition case. We use standard conditional GAN\nmodel and training procedure to generate samples by keeping the input vector\nz the same and changing the condition value. Generator function is de\ufb01ned as\nG(z|y) for input vector z and condition value y . We can de\ufb01ne the function for\nsame input vector and ndi\ufb00erent conditions as G(z|yn) and the domain speci\ufb01c\nmerging operator as M(G(z|yn)). We feed the merged result to discriminator\nto determine if it is realistic so the output of discriminator is D(M(G(z|yn))) .\nSince the proposed method is an additional step to standard conditional GAN,\nit is a new term for the min-max game between generator and discriminator.\n\n6 C. \u00a8Ong\u00a8 un and A. Temizel\nThe formulation of proposed method can be added to standard formulation to\nde\ufb01ne the system as a whole. The objective function of conditional GAN with\nproposed additional training step can be formulated as follows:\nminGmaxD V(D,G) = Ex\u223cpdata(x)[log D(x|y)]\n+ Ez\u223cpz(z)[log (1 \u2212D(G(z|y)))]\n+ Ez\u223cpz(z)[log (1 \u2212D(M(G(z|yn))))].\n(3)\nAs expected the system generates n di\ufb00erent samples at n di\ufb00erent rotations\neven though the input vector is the same. However as their rotations are speci\ufb01ed\nby the condition, they are known. We then merge these samples to create a single\nobject by \ufb01rst aligning these samples and then taking the average of the values\nfor each voxel, similar to taking the intersection of 3D models. The merged model\nis then fed into the discriminator to evaluate whether it is realistic or not:\n\u2013 If generated objects are di\ufb00erent (as expected at the beginning), the merged\nmodel will be empty or meaningless. The discriminator will label the merged\nresult as fake and the generator will get a negative feedback.\n\u2013 If generated objects are realistic and similar, the merged model will also be\nvery similar to them and to a realistic chair model. The discriminator is\nlikely to label the merged object as real and the generator gets a positive\nfeedback.\nBy this additional training step, even if the generated samples are realistic,\nsystem gets negative feedback unless the samples are similar. We enforce the\nsystem to generate similar and realistic samples in di\ufb00erent conditions for the\nsame input vector.\nNote that the merge operation is domain speci\ufb01c and could be selected ac-\ncording to the target domain.\nFig. 2.Proposed method illustrated for 2-condition case.\n\nPaired 3D Model Generation with CGAN 7\nAlgorithm 1.Conditional GAN training with proposed method for n-conditions\nInput: Real samples in n conditions: X0,X1,\u00b7\u00b7\u00b7 ,Xn input vector: Z, condition\nvalues: C0,C1,\u00b7\u00b7\u00b7 ,Cn\nInitialize network parameters for discriminator D, Generator G and merge op-\neration M\nfor number of training steps do\n// Standard conditional GAN\n\u2022 Update the discriminator using X0, X1,\u00b7\u00b7\u00b7 , Xn with C0, C1,\u00b7\u00b7\u00b7 , Cn respectively\n\u2022 Generate samples S0, S1,\u00b7\u00b7\u00b7 , Sn using vector Z with C0, C1,\u00b7\u00b7\u00b7 , Cn respectively\n\u2022 Update the discriminator using S0, S1,\u00b7\u00b7\u00b7 , Sn with C0, C1,\u00b7\u00b7\u00b7 , Cn respectively\n\u2022 Update the generator using S0, S1,\u00b7\u00b7\u00b7 , Sn with C0, C1,\u00b7\u00b7\u00b7 , Cn respectively\n// Proposed method\n\u2022 Align S1,\u00b7\u00b7\u00b7 ,Sn with S0\n\u2022 Merge S0,S1,\u00b7\u00b7\u00b7 ,Sn : M(S0,S1,\u00b7\u00b7\u00b7 ,Sn)\n\u2022 Feed merged sample to the discriminator with condition C0\n\u2022 Update the generator using the discriminator output\nend for\n4 Experiments\nTo test the system we used ModelNet [20] dataset to generate 3D models for\ndi\ufb00erent object classes (e.g. chair, bed, sofa). We adapted the conditional GAN\nfor the problem of generation of 3D objects. We then evaluated the proposed\nmethod for 2-conditional and 4-conditional cases. Visual results as well as ob-\njective comparisons are provided at the end of this section.\nModelNet dataset: This dataset contains a noise-free collection of 3D CAD\nmodels for objects. There are 2 manually aligned subsets with 10 and 40 classes\nof objects for deep networks. While the original models are in CAD format,\nthere is also voxelized version [20]. Voxels are basically binary 3D matrices, each\nmatrix element determines the existence of unit cube in the respective location.\nVoxelized models have 30\u00d730\u00d730 resolution. The resolution is set to 32\u00d732\u00d732\nby simply zero padding one unit on each side. For the experiments 3 object classes\nare used: chair, bed and sofa having 989, 615 and 780 samples respectively. Each\nsample has 12 orientations O1,O2,\u00b7\u00b7\u00b7 ,O12 with 30 degrees of rotation between\nthem. In the experiments with 2 orientations we use O1, and O7 which represent\nthe object in opposite directions (0 \u25e6and 180\u25e6). Experiments with 4 orientations\nuse O1, O4, O7 and O10 (0\u25e6, 90\u25e6, 180\u25e6and 270\u25e6).\nWhile there are more object classes in the dataset, either they do not have\nsu\ufb03cient number of training samples for the system to converge (less than 500)\n\n8 C. \u00a8Ong\u00a8 un and A. Temizel\nor objects are highly symmetric such that di\ufb00erent orientations come out as same\nmodels (round or rectangle objects). For di\ufb00erent rotations, the system has been\ntested with paired input samples, unpaired (shu\ufb04ed) samples or removing any\ncorrespondence between samples in di\ufb00erent conditions by using one half of the\ndataset for one condition and the other half for other condition. The tests with\ndi\ufb00erent variants of input dataset show no signi\ufb01cant change on the output.\nNetwork structure: We designed our architecture building on a GAN archi-\ntecture for 3D object generation [17]. In this architecture, the generator network\nuses 4 3D transposed deconvolutional layers and a sigmoid layer at the end.\nLayers use ReLU activation functions and the generator takes a 200 dimensional\nvector as input. Output of the generator network is a 32 \u00d732 \u00d732 resolution\n3D matrix. Discriminator network mostly mirrors the generator with 4 3D con-\nvolutional layers with leaky ReLU activation functions and a sigmoid layer at\nthe end. It takes a 32 \u00d732 \u00d732 voxel grid as input and generates a single value\nbetween 0 and 1 as output, representing the probability of a sample being real.\nBoth networks use batch normalization between all layers. Kernel size of convo-\nlutional \ufb01lters is 4 and stride is 2.\nAdapting conditional GAN for generation of 3D models:To generate 3D\nmodels on di\ufb00erent rotations, we modi\ufb01ed the aforementioned GAN architecture\nand converted it into a conditional GAN. Conditional value y is concatenated\ninto z for generator input. For discriminator input, y is concatenated into real\nand generated samples as an additional channel. To train the discriminator,\nwe feed objects on di\ufb00erent rotations with corresponding condition values. To\ngenerate pairs, we change only the y and keep the z the same.\nTraining: Since generating 3D models is a more di\ufb03cult task than di\ufb00erentiat-\ning between real and generated ones, discriminator learns faster than generator\nand it overpowers the generator. If the learning pace is di\ufb00erent between genera-\ntor and discriminator, it causes instability in the network and it fails to generate\nrealistic results [7]. To keep the training in pace, we used a threshold for discrim-\ninator training. Discriminator is updated only if the accuracy is less than 95%\nin the previous batch. The learning rates are 0.0025 for generator and 0.00005\nfor discriminator. ADAM [11] is used for optimization with \u03b2 = 0.5 . System is\ntrained using a batch size of 128. For 2 orientations, condition 0 and 1 are used\nfor 0\u25e6and 180\u25e6respectively. For 4 orientations, condition 0, 1, 2 and 3 are used\nfor 0\u25e6, 90\u25e6, 180\u25e6, 270\u25e6respectively.\nVisual results prove that, standard conditional GAN fails to generate 3D\nmodels with the same attributes in di\ufb00erent rotations. In 2-conditional case, it\ngenerates a chair with 0\u25e6orientation, and a completely di\ufb00erent chair with 180\u25e6\norientation for the same input value. On the other hand, the proposed system\ncan generate 3D models of the same object category with same attributes with\n0\u25e6 and 180 \u25e6 orientations. Also the result of merge operation is given to show\n\nPaired 3D Model Generation with CGAN 9\n(a) chair\n (b) chair\n(c) bed\n(d) bed\n(e) sofa\n(f) sofa\nFig. 3.Results with 3 classes (chair, bed and sofa) using 2-conditions (rota-\ntions). The \ufb01rst two samples are the generated pairs, merged results are shown\nin boxes. (a), (c) and (e) show the pairs generated with standard conditional\nGAN. It is clearly visible that the samples belong to di\ufb00erent objects. Standard\nconditional GAN fails to generate the same object in di\ufb00erent conditions (rota-\ntions) as expected and the merged results are noisy. (b), (d) and (f) show the\npairs generated with the proposed method. The samples are very similar and\nthe merged results (intersection of samples) support this observation. Merged\nresults are also mostly noise-free and have more detail compared to standard\nconditional GAN.\n\n10 C. \u00a8Ong\u00a8 un and A. Temizel\nthe intersection of models. Since intersection of noise is mostly empty, merged\nmodel is also mostly noise-free. For these 3 classes, system is proven to generate\npair models on di\ufb00erent rotations.\nFor additional training of the proposed method, samples are generated by\nkeeping the input vector the same and setting the condition value di\ufb00erently.\nThen the outputs are merged and fed into the discriminator. Only the generator\nis updated in this step. Experiments show that, also updating the discriminator\nin this step causes overtraining and makes the system unstable. Since this step is\nfor enforcing the generator to generate the same sample in di\ufb00erent conditions,\ntraining of the discriminator is not necessary.\nMerge method: Merging the generated samples is domain speci\ufb01c. For our\ncase, generated samples are 3D voxelized models with values between 0 and 1\nrepresenting the probability of the existence of the unit cube on that location.\nFirst aligning the samples generated with di\ufb00erent orientations and then simply\naveraging their 3D matrices, we get the merged result. In Figure 4, we illustrate\nthe merging procedure with a 2-conditional case with chair dataset. Generator\nwill output two chairs with 0 \u25e6 and 180\u25e6 rotations respectively. We can simply\nrotate the second model 180 \u25e6 to align both samples. Then, we average these\n3D matrices. By averaging we get the probability of the existence of unit cubes\nin each location taking both outputs into account. If chairs are similar, the\nintersection of them will also be a similar chair (Fig. 4(a)) and if the chairs\nare not similar, their intersection will be meaningless (Fig. 4(b)). By feeding\nthese merged results into the discriminator, we make the network evaluate the\nintersection model and train the generator using this information.\n(a)\n (b)\nFig. 4.Examples of merging operation. After generating pairs, one of the pairs\nis aligned with the other. Second sample is rotated to align with the \ufb01rst one\nin these examples. Then aligned samples are merged to form a new one. Simple\naveraging is applied to aligned pairs to get the intersection. (a) The result of the\nmerging operation will be similar to the generated samples if the samples are\nsimilar, (b) the result will be meaningless if the samples are di\ufb00erent.\nResults: The proposed framework has been implemented using Tensor\ufb02ow [1]\nversion 1.4 and tested with 3 classes: chair, bed and sofa. The results are observed\n\nPaired 3D Model Generation with CGAN 11\nafter training the model for 1500 epochs with the whole dataset. Dataset is\ndivided into batches of 128 samples. For comparison, we used the conditional\nGAN that we adapted for 3D model generation as the baseline method. Both\nsystems have been trained with the same parameters and same data. Results\nare generated with the same input and di\ufb00erent condition values. To visualize\nthe results, binary voxelization is used with a threshold of 0.5. Fig. 3 shows the\nvisual results. Note that the presented results are visualizations of raw output\nwithout any post processing or noise reduction.\nAs there is no established metric for the evaluation of generated samples,\nwe introduce 2 di\ufb00erent evaluation metrics: Average Absolute Di\ufb00erence (AAD)\nand Average Voxel Agreement Ratio (AVAR).\nRaw outputs are 3D matrices for each generated model and each element\nof these matrices is a probability value between 0 and 1. For the calculation of\nAAD with n-conditions, \ufb01rst, the generated models S1,...,S n aligned with S0\nto get SR\n1 ,...,S R\nn then AAD can be formulated as follows:\nAAD=\n\u2211n\u22121\ni=0\n\u2211\n\u2200x,y,z |SR\ni (x,y,z)\u2212M(x,y,z)|\ntotal # of matrix elements\nn (4)\nAs a result of AAD a single di\ufb00erence metric is obtained for that object. A\nlower AAD value indicates agreement of the generated models with the merged\nmodel and it is desired to have an AAD value closer to 0.\nFor the calculation of Average Voxel Agreement Ratio (AVAR), \ufb01rst the\naligned 3D matrices are binarized with a threshold of 0.5 to form voxelized SRB\ni\nMB and then Average Voxel Agreement Ratio (AVAR) can be formulated as:\nAVAR =\n\u2211n\u22121\ni=0\n\u2211\n\u2200x,y,z SRB\ni (x,y,z) \u22c0MB(x,y,z)\u2211\n\u2200x,y,z SRB\ni (x,y,z)\nn (5)\nwhere \u22c0 is the binary logical AND operator. AVAR value of 0 indicates\ndisagreement while a value of 1 indicates agreement of the models with the\nmerged model and it is desired to have an AVAR value closer to 1.\nResults for 2-conditions and a batch of 128 pairs are given in Table 1. AAD\nand AVAR results are calculated separately for each pair in the batch and then\naveraged to get a single result for the batch. The results show that the proposed\nmethod reduces the average di\ufb00erence signi\ufb01cantly; 3, 2.4 and 4.5 times for\nchair, bed and sofa respectively. Here the results are highly dependent to object\nclass. Di\ufb00erent beds and sofas are naturally more similar than di\ufb00erent chairs.\nWhile di\ufb00erent bed shapes are mostly same except headboards, chairs can be\nvery di\ufb00erent considering stools, seats etc. Also we can see it in the results, the\nproposed method improved the similarity of generated chair pairs from 0.32 to\n0.79. While the generated chair pairs are very di\ufb00erent with the baseline method,\nthe proposed method generated very similar pairs. For bed and sofa the baseline\nsimilarities are 0.69 and 0.74, relatively more similar as expected. The proposed\nmethod improved the results to 0.89 and 0.95 for bed and sofa respectively by\nconverging to the same model.\n\n12 C. \u00a8Ong\u00a8 un and A. Temizel\nTable 1. Comparison of the proposed method with baseline using di\ufb00erent\nobject classes for 2-conditions and a batch (128) of pairs. AAD: Average Absolute\nDi\ufb00erence between generated matrices, AVAR: Average Voxel Agreement Ratio.\nChair Bed Sofa\nAAD AVAR AAD AVAR AAD AVAR\nBaseline 0.027 0.32 0.029 0.69 0.018 0.74\nProposed 0.009 0.79 0.012 0.89 0.004 0.95\nTable 2. Comparison of the proposed method with baseline using di\ufb00erent\nobject classes for 4-conditions. The same metrics are used as in the 2-condition\ncase.\nChair Bed Sofa\nAAD AVAR AAD AVAR AAD AVAR\nBaseline 0.034 0.36 0.043 0.65 0.034 0.62\nProposed 0.024 0.61 0.021 0.82 0.013 0.90\nThe proposed system has also been tested with 4-conditions. For 4 orienta-\ntions, condition 0, 1, 2 and 3 are used for 0 \u25e6, 90\u25e6, 180\u25e6 and 270\u25e6 respectively.\nAlso for merging operation, all generated samples are aligned with the \ufb01rst sam-\nple with 0 \u25e6 rotation. For that purpose 2 nd, 3rd and 4th samples are rotated by\n270\u25e6, 180\u25e6 and 90\u25e6 respectively. After aligning all 4 samples, they are merged\ninto a single model by averaging.\nFig. 5 shows the visual results for 4-conditional case with the same experi-\nmental setup. Experimental results in terms of the same metrics are presented in\nTable 2. Standard conditional GAN generates 4 di\ufb00erent chairs on 4 rotations.\nOn the other hand the proposed method enforces the network to generate the\nsame chair on 4 di\ufb00erent rotations. Since the problem is more complex for 4\nrotations, individual generated samples are noisier and have lower resolution.\nThe improvement rates compared to the baseline are relatively lower than 2-\ncondition case because of the increased complexity of the problem. To account\nfor the increasing complexity of the model with higher number of conditions,\nmore training data and/or higher number of epochs need to be used.While gen-\nerating better samples with more training may seem crucial, it doesnt change the\nbehavior of the networks. Conditional GAN keeps generating di\ufb00erent samples\nand proposed model generates paired samples with each training iteration.\n5 Conclusions and Future Work\nIn this paper, we presented a new approach to generate paired 3D models with\nconditional GAN. First, we adapted the conditional GAN to generate 3D models\non di\ufb00erent rotations. Then, we integrated an additional training step to solve\nproblem of generation of pair samples, which is a shortcoming of standard con-\n\nPaired 3D Model Generation with CGAN 13\nditional GAN. The proposed method is generic and it can be integrated into any\nconditional GAN. The results show the potential of the proposed method for the\npopular problem of joint distribution learning in GANs.\nWe demonstrated that proposed method works successfully for 3D voxel mod-\nels on 2 and 4 orientations. Visual results and the objective evaluation metrics\ncon\ufb01rm the success of the proposed method. The di\ufb00erence between generated\nmodels are reduced signi\ufb01cantly in terms of the average di\ufb00erence. The merged\nsamples create noise-free high-resolution instances of the objects. This approach\ncan also be used for generating better samples compared to traditional GAN for\na particular object class.\nThe extension of the method to work with higher number of conditions is\ntrivial. However, as the training of the system takes a long time, we leave the\nexperiments with higher number of conditions and classes as a future work. The\nproposed solution is generic and could be applied to other types of data. As\na next step, we are aiming to test the method on generation of 2D images to\ninvestigate the validity of the method for di\ufb00erent data types.\n\n14 C. \u00a8Ong\u00a8 un and A. Temizel\n(a) chair\n (b) chair\n(c) bed\n (d) bed\n(e) sofa\n (f) sofa\nFig. 5.Visual results with 4 conditions. The \ufb01rst four samples are the generated\nobjects, merged results are shown in boxes. (a), (c) and (e) show the objects and\nthe merged result obtained with standard conditional GAN. (b), (d) and (f)\nshow the objects and the merged result obtained with the proposed method\nThe samples are very similar and the merged results (intersection of samples)\nsupport this claim. Merged results are also mostly noise-free and have more\ndetail compared to standard conditional GAN.\n\nPaired 3D Model Generation with CGAN 15\nReferences\n1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\nG.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,\nIrving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\nJ., Man\u00b4 e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J.,\nSteiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\nVi\u00b4 egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,\nX.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\nhttps://www.tensorflow.org/, software available from tensor\ufb02ow.org\n2. Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Representation learning\nand adversarial generation of 3d point clouds. arXiv preprint arXiv:1707.02392\n(2017)\n3. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. arXiv preprint\narXiv:1701.07875 (2017)\n4. Che, T., Li, Y., Jacob, A.P., Bengio, Y., Li, W.: Mode regularized generative\nadversarial networks. arXiv preprint arXiv:1612.02136 (2016)\n5. Chen, W.C., Chen, C.W., Hu, M.C.: Syncgan: Synchronize the latent space of cross-\nmodal generative adversarial networks. arXiv preprint arXiv:1804.00410 (2018)\n6. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Infogan:\nInterpretable representation learning by information maximizing generative adver-\nsarial nets. In: Advances in neural information processing systems. pp. 2172\u20132180\n(2016)\n7. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural\ninformation processing systems. pp. 2672\u20132680 (2014)\n8. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-\ntional adversarial networks. arXiv preprint arXiv:1611.07004 (2017)\n9. Jiang, C., Marcus, P., et al.: Hierarchical detail enhancing mesh-based shape gen-\neration with 3d generative adversarial network. arXiv preprint arXiv:1709.07581\n(2017)\n10. Kim, T., Cha, M., Kim, H., Lee, J.K., Kim, J.: Learning to discover cross-domain\nrelations with generative adversarial networks. arXiv preprint arXiv:1703.05192\n(2017)\n11. Kingma, D.P., Ba, J.L.: Adam: Amethod for stochastic optimization. In: Pro-\nceedings of the 3rd International Conference on Learning Representations (ICLR)\n(2015)\n12. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: Advances in\nneural information processing systems. pp. 469\u2013477 (2016)\n13. Luan, F., Paris, S., Shechtman, E., Bala, K.: Deep photo style transfer. CoRR,\nabs/1703.07511 2 (2017)\n14. Mao, X., Li, Q., Xie, H.: Aligngan: Learning to align cross-domain images with con-\nditional generative adversarial networks. arXiv preprint arXiv:1707.01400 (2017)\n15. Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint\narXiv:1411.1784 (2014)\n16. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434\n(2015)\n17. Smith, E., Meger, D.: Improved adversarial systems for 3d object generation and\nreconstruction. arXiv preprint arXiv:1707.09557 (2017)\n\n16 C. \u00a8Ong\u00a8 un and A. Temizel\n18. Wang, C., Xu, C., Wang, C., Tao, D.: Perceptual adversarial networks for image-to-\nimage transformation. IEEE Transactions on Image Processing 27(8), 4066\u20134079\n(2018)\n19. Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: Learning a probabilistic\nlatent space of object shapes via 3d generative-adversarial modeling. In: Advances\nin Neural Information Processing Systems. pp. 82\u201390 (2016)\n20. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets: A\ndeep representation for volumetric shapes. In: Proceedings of the IEEE conference\non computer vision and pattern recognition. pp. 1912\u20131920 (2015)\n21. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation us-\ning cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593 (2017)",
  "authors": [
    "Cihan \u00d6ng\u00fcn",
    "Alptekin Temizel"
  ],
  "summary": "Generative Adversarial Networks (GANs) are shown to be successful at generating new and realistic samples including 3D object models. Conditional GAN, a variant of GANs, allows generating samples in given conditions. However, objects generated for each condition are different and it does not allow generation of the same object in different conditions. In this paper, we first adapt conditional GAN, which is originally designed for 2D image generation, to the problem of generating 3D models in different rotations. We then propose a new approach to guide the network to generate the same 3D sample in different and controllable rotation angles (sample pairs). Unlike previous studies, the proposed method does not require modification of the standard conditional GAN architecture and it can be integrated into the training step of any conditional GAN. Experimental results and visual comparison of 3D models show that the proposed method is successful at generating model pairs in different conditions.",
  "pdf_url": "https://arxiv.org/pdf/1808.03082v2",
  "entry_id": "http://arxiv.org/abs/1808.03082v2",
  "published": "2018-08-09",
  "updated": "2019-03-15",
  "comment": "Published in ECCV 2018 Workshops, Springer, LNCS. Cite this paper as: Ongun C., Temizel A. (2019) Paired 3D Model Generation with Conditional Generative Adversarial Networks. In: Leal-Taixe L., Roth S. (eds) Computer Vision-ECCV 2018 Workshops. ECCV 2018. Lecture Notes in Computer Science, vol 11129. Springer, Cham",
  "journal_ref": null,
  "doi": null,
  "primary_category": "cs.CV",
  "categories": [
    "cs.CV"
  ],
  "links": [
    {
      "href": "https://arxiv.org/abs/1808.03082v2",
      "rel": "alternate",
      "title": null
    },
    {
      "href": "https://arxiv.org/pdf/1808.03082v2",
      "rel": "related",
      "title": "pdf"
    }
  ],
  "full_text": "Paired 3D Model Generation with Conditional\nGenerative Adversarial Networks\nCihan \u00a8Ong\u00a8 un and Alptekin Temizel\nMiddle East Technical University, Ankara, Turkey\n{congun,atemizel}@metu.edu.tr\nAbstract. Generative Adversarial Networks (GANs) are shown to be\nsuccessful at generating new and realistic samples including 3D object\nmodels. Conditional GAN, a variant of GANs, allows generating samples\nin given conditions. However, objects generated for each condition are\ndi\ufb00erent and it does not allow generation of the same object in di\ufb00er-\nent conditions. In this paper, we \ufb01rst adapt conditional GAN, which is\noriginally designed for 2D image generation, to the problem of generat-\ning 3D models in di\ufb00erent rotations. We then propose a new approach\nto guide the network to generate the same 3D sample in di\ufb00erent and\ncontrollable rotation angles (sample pairs). Unlike previous studies, the\nproposed method does not require modi\ufb01cation of the standard condi-\ntional GAN architecture and it can be integrated into the training step\nof any conditional GAN. Experimental results and visual comparison of\n3D models show that the proposed method is successful at generating\nmodel pairs in di\ufb00erent conditions.\nKeywords: Conditional Generative Adversarial Network (CGAN)\u00b7Pair\nGeneration \u00b7Joint Learning \u00b73D Voxel Model\n1 Introduction\nWhile 3D technology mostly focuses on providing better tools for humans to\nscan, create, modify and visualize 3D data, recently there has been an interest\nin automated generation of new 3D object models. Scanning a real object is\nthe most convenient way to generate digital 3D object models, however, this\nrequires availability of real-life objects and each of these objects needs to be\nscanned individually. More crucially, it does not allow creating a novel object\nmodel. Creating a novel object model is a time consuming task requiring human\nimagination, e\ufb00ort and specialist skills. So it is desirable to have an automated\nsystem facilitating streamlined generation of 3D object content.\nGenerative models have recently become mainstream with their applications\nin various domains. Generative Adversarial Networks (GANs) [7] have been a\nrecent breakthrough in the \ufb01eld of generative models. GANs provide a generic so-\nlution for various types of data leveraging the power of arti\ufb01cial neural networks,\nparticularly Convolutional Neural Networks (CNN). On the other hand, use of\nGANs brings out several challenges. While stability is the most fundamental\nproblem in GAN architecture, there are also domain speci\ufb01c challenges.\nPublished in ECCV 2018 Workshops\nc\u20ddSpringer, LNCS 11129, pp. 473-487, 2019\nThe \ufb01nal authenticated publication is available online at\nhttps://doi.org/10.1007/978-3-030-11009-3 29\narXiv:1808.03082v2  [cs.CV]  15 Mar 2019\n\n2 C. \u00a8Ong\u00a8 un and A. Temizel\nStandard GAN model generates novel samples from an input distribution.\nHowever, the generated samples are random and there is no control over them\nas the input noise and the desired features are entangled. While some solutions\nattack the entanglement problem [6], some propose new types of GANs for spe-\nci\ufb01c purposes. Conditional GAN [15] allows controlling the characteristics of the\ngenerated samples using a condition. While these conditions could be speci\ufb01ed,\nthe generated samples are random and it fails to generate pair samples in dif-\nferent conditions [12,14]. Keeping the input value the same while changing the\ncondition value does not generate the same output in di\ufb00erent conditions because\nof the entanglement problem. The representation between input and output sam-\nple is entangled in such a way that changing condition value changes the output\ncompletely. There are many studies for learning joint distributions to generate\nnovel pair samples. Most of them uses modi\ufb01ed GAN architectures, complex\nmodels or paired training data as described in the Related Works section.\nIn this study, we propose a new approach to generate paired 3D models with\nConditional GANs. Our method is integrated as an additional training step\nto Conditional GAN without changing the original architecture. This generic\nsolution provides \ufb02exibility such that it is applicable to any conditional GAN\narchitecture as long as there is a metric to measure the similarity of samples\nin di\ufb00erent conditions. Also the system can be trained with paired samples,\nunpaired samples and without any tuples of corresponding samples in di\ufb00erent\ndomains.\nIn section 2, we describe the GAN architecture and the related works. The\nproposed method is given in section 3 and experimental evaluation and results\nare provided in section 4. Conclusions and future work are given in section 5.\n2 Related Works\nGAN architecture (Fig. 1(a)) consists of a generator model G and a discrim-\ninator model D [7]. Generator model takes an input code and generates new\nsamples. Discriminator model takes real and generated samples and tries to dis-\ntinguish real ones from generated ones. Generator and discriminator are trained\nsimultaneously so that while generator learns to generate better samples, dis-\ncriminator becomes better at distinguishing samples resulting in an improved\nsample generation performance at the end of the training.\nIf GAN is trained with training datax for discriminator Dand sampled noise\nz for generator G, D is used to maximize the correctly labeled real samples as\nreal log( D(x)) and generated samples as fake log(1 \u2212D(G(z))). On the other\nhand, generator G tries to fool the discriminator to label the generated data as\nreal so Gis used for minimizing log(1 \u2212D(G(z))) . These two models duel each\nother in a min-max game with the value function V(D,G). The objective of the\nwhole system can be formulated as:\nminGmaxD V(D,G) = Ex\u223cpdata(x)[log D(x)]\n+ Ez\u223cpz(z)[log (1 \u2212D(G(z)))]. (1)\n\nPaired 3D Model Generation with CGAN 3\nUse of CNN based GANs [16] is popular in 2-D image domain with various\napplications. Pix2pix [8] is a general-purpose GAN based solution to image-to-\nimage translation problems and it has been shown to be e\ufb00ective at problems\nsuch as synthesizing photos from label maps, reconstructing objects from edge\nmaps, and colorizing images. It uses GANs in conditional settings for image-\nto-image translation tasks, where a condition is given on an input image to\ngenerate a corresponding output image. Another application of GAN is style\ntransfer [13]. For an input image, the system can transfer the style of the ref-\nerence image including time of the day, weather, season and artistic edit to the\ntarget. Perceptual Adversarial Network (PAN) [18] provides a generic framework\nfor image-to-image transformation tasks such as removing rain streaks from an\nimage (image de-raining), mapping object edges to the corresponding image,\nmapping semantic labels to a scene image and image inpainting.\nThe fundamental principle of GANs, i.e. using two di\ufb00erent models trained\ntogether, causes stability problems. These two models must be in equilibrium to\nwork together in harmony. Since the architecture is based on dueling networks,\nduring the training phase, one of the models could overpower the other, causing\na stability problem. Wasserstein GAN [3] proposes a new distance metric to cal-\nculate the discriminator loss where Wasserstein distance (Earth-Mover distance)\nis used to improve the stability of learning and provide useful learning curves.\nIn [4] several approaches are introduced for regularizing the system to stabilize\nthe training of GAN models.\nGenerating 3D models with GANs is a relatively new area with a limited\nnumber of studies. The \ufb01rst and the most popular study uses an all-convolutional\nneural network to generate 3D objects [19]. In this work, the discriminator mostly\nmirrors the generator and 64x64x64 voxels are used to represent 3D models.\nWasserstein distance [17] is employed by normalizing with gradient penalization\nas a training objective to improve multiclass 3D generation. In another study an\nautoencoder network is used to generate 3D representations in latent space [2].\nGAN model generates new samples in this latent space and these samples are\ndecoded using the same autoencoder network to obtain 3D point cloud samples.\n3D meshes can also be used to train a GAN [9] to produce mesh-based 3D output.\nTo overcome the di\ufb03culty of working with mesh data, input data is converted to\nsigned distance \ufb01eld, then processed with two GAN architectures: low-frequency\nand high-frequency generator. After generating high and low-frequency samples,\noutputs are combined to generate a 3D mesh object.\nGenerator model of GAN uses a simple input noise vector z and it is possible\nthat the noise will be used by the generator in a highly entangled way, causing the\ninput vector z not correspond to semantic features of the output data. InfoGAN\n[6] is a method proposed to solve entanglement problem. To make a semantic\nconnection between input noise vector and output data, a simple modi\ufb01cation\nis presented to the generative adversarial network objective that encourages\nit to learn interpretable and meaningful representations. Generator network is\nprovided with both the incompressible noise z and the latent code c, so the form\n\n4 C. \u00a8Ong\u00a8 un and A. Temizel\nof input data becomes ( z,c). After necessary optimizations for combining these\nvalues, expected outputs can be generated with given parameters.\nConditional GAN (Fig. 1 (b)) is an extended version of GAN [15] condition-\ning both generator and discriminator on some extra information. While standard\nGAN models generate samples from random classes, CGANs can generate sam-\nples with a predetermined class for any input distribution such as generating\nspeci\ufb01c digits by using class labels as condition in MNIST dataset.\nInput noise z and condition value y are concatenated to use as input to the\ngenerator G. Training data x and condition value y are concatenated to use as\ninput to the discriminator D. With this modi\ufb01cation, the objective function of\nconditional GAN can be formulated as follows:\nminGmaxD V(D,G) = Ex\u223cpdata(x)[log D(x|y)]\n+ Ez\u223cpz(z)[log (1 \u2212D(G(z|y)))]. (2)\nConditional GANs can generate samples in given conditions, however they\nare not able to generate pairs for the same input and di\ufb00erent condition values.\nCoupled GAN (CoGAN) [12] is a new network model for learning a joint distri-\nbution of multi-domain images. CoGAN consists of a pair of GANs, each having\na generative and a discriminator model for generating samples in one domain.\nBy sharing of weights, the system generates pairs of images sharing the same\nhigh-level abstraction while having di\ufb00erent low-level realizations. DiscoGAN\n[10] aims to discover cross-domain relations with GANs. A similar approach is\nused in CycleGAN [21] where an image is used as input instead of a noise vector\nand it generates a new image by translating it from one domain to another.\nSyncGAN [5] employs an additional synchronizer model for multi-modal genera-\ntion like sound-image pairs. AlignGAN [14] adopts a 2-step training algorithm to\nlearn the domain-speci\ufb01c semantics and shared label semantics via alternating\noptimization.\n3 Proposed Method\nWhile the standard GAN model can generate realistic samples, it basically gen-\nerates random samples in given input distribution and does not provide any\ncontrol over these generated samples. For example, when a chair dataset is used\nto train the network, it generates chairs without any control over its character-\nistics such as its rotation. Conditional GANs provide control over the generated\nsamples by training the system with given input conditions. For example, if ro-\ntation is used as a condition value for chair dataset, system can generate samples\nwith a given rotation.\nFor both standard GAN and conditional GAN, the representation between\nthe input and the output is highly entangled such that changing a value in the\ninput vector changes the output in an unpredictable way. For example, for chair\ndataset, each chair generated by standard GAN would be random and it would\nbe created in an unknown orientation. Conditional GAN allows speci\ufb01cation of\n\nPaired 3D Model Generation with CGAN 5\n(a)\n(b)\nFig. 1.(a) Standard GAN and (b) conditional GAN architectures.\na condition Input vector z and condition value y are concatenated and given\ntogether as input to the system so input becomes ( z|y). As the condition value\ny is also an input value, changing the condition also changes the output. Even\nif the input vector z is kept the same, the model generates di\ufb00erent independent\nsamples in given conditions and does not allow generating the same sample\nin di\ufb00erent conditions [12,14]. For example, for chair dataset, if the condition\nis rotation, system generates a chair in \ufb01rst rotation and a di\ufb00erent chair in\ndi\ufb00erent rotation. As these objects are di\ufb00erent, they cannot be merged at a\nlater processing stage to create a new sample with less artifacts.\nTo overcome this problem, we propose incorporating an additional step in\ntraining to guide the system to generate the same sample in di\ufb00erent conditions.\nThe pseudo code of the method is provided in Algorithm 1 and Fig. 2 illustrates\nthe proposed method for the 2-condition case. We use standard conditional GAN\nmodel and training procedure to generate samples by keeping the input vector\nz the same and changing the condition value. Generator function is de\ufb01ned as\nG(z|y) for input vector z and condition value y . We can de\ufb01ne the function for\nsame input vector and ndi\ufb00erent conditions as G(z|yn) and the domain speci\ufb01c\nmerging operator as M(G(z|yn)). We feed the merged result to discriminator\nto determine if it is realistic so the output of discriminator is D(M(G(z|yn))) .\nSince the proposed method is an additional step to standard conditional GAN,\nit is a new term for the min-max game between generator and discriminator.\n\n6 C. \u00a8Ong\u00a8 un and A. Temizel\nThe formulation of proposed method can be added to standard formulation to\nde\ufb01ne the system as a whole. The objective function of conditional GAN with\nproposed additional training step can be formulated as follows:\nminGmaxD V(D,G) = Ex\u223cpdata(x)[log D(x|y)]\n+ Ez\u223cpz(z)[log (1 \u2212D(G(z|y)))]\n+ Ez\u223cpz(z)[log (1 \u2212D(M(G(z|yn))))].\n(3)\nAs expected the system generates n di\ufb00erent samples at n di\ufb00erent rotations\neven though the input vector is the same. However as their rotations are speci\ufb01ed\nby the condition, they are known. We then merge these samples to create a single\nobject by \ufb01rst aligning these samples and then taking the average of the values\nfor each voxel, similar to taking the intersection of 3D models. The merged model\nis then fed into the discriminator to evaluate whether it is realistic or not:\n\u2013 If generated objects are di\ufb00erent (as expected at the beginning), the merged\nmodel will be empty or meaningless. The discriminator will label the merged\nresult as fake and the generator will get a negative feedback.\n\u2013 If generated objects are realistic and similar, the merged model will also be\nvery similar to them and to a realistic chair model. The discriminator is\nlikely to label the merged object as real and the generator gets a positive\nfeedback.\nBy this additional training step, even if the generated samples are realistic,\nsystem gets negative feedback unless the samples are similar. We enforce the\nsystem to generate similar and realistic samples in di\ufb00erent conditions for the\nsame input vector.\nNote that the merge operation is domain speci\ufb01c and could be selected ac-\ncording to the target domain.\nFig. 2.Proposed method illustrated for 2-condition case.\n\nPaired 3D Model Generation with CGAN 7\nAlgorithm 1.Conditional GAN training with proposed method for n-conditions\nInput: Real samples in n conditions: X0,X1,\u00b7\u00b7\u00b7 ,Xn input vector: Z, condition\nvalues: C0,C1,\u00b7\u00b7\u00b7 ,Cn\nInitialize network parameters for discriminator D, Generator G and merge op-\neration M\nfor number of training steps do\n// Standard conditional GAN\n\u2022 Update the discriminator using X0, X1,\u00b7\u00b7\u00b7 , Xn with C0, C1,\u00b7\u00b7\u00b7 , Cn respectively\n\u2022 Generate samples S0, S1,\u00b7\u00b7\u00b7 , Sn using vector Z with C0, C1,\u00b7\u00b7\u00b7 , Cn respectively\n\u2022 Update the discriminator using S0, S1,\u00b7\u00b7\u00b7 , Sn with C0, C1,\u00b7\u00b7\u00b7 , Cn respectively\n\u2022 Update the generator using S0, S1,\u00b7\u00b7\u00b7 , Sn with C0, C1,\u00b7\u00b7\u00b7 , Cn respectively\n// Proposed method\n\u2022 Align S1,\u00b7\u00b7\u00b7 ,Sn with S0\n\u2022 Merge S0,S1,\u00b7\u00b7\u00b7 ,Sn : M(S0,S1,\u00b7\u00b7\u00b7 ,Sn)\n\u2022 Feed merged sample to the discriminator with condition C0\n\u2022 Update the generator using the discriminator output\nend for\n4 Experiments\nTo test the system we used ModelNet [20] dataset to generate 3D models for\ndi\ufb00erent object classes (e.g. chair, bed, sofa). We adapted the conditional GAN\nfor the problem of generation of 3D objects. We then evaluated the proposed\nmethod for 2-conditional and 4-conditional cases. Visual results as well as ob-\njective comparisons are provided at the end of this section.\nModelNet dataset: This dataset contains a noise-free collection of 3D CAD\nmodels for objects. There are 2 manually aligned subsets with 10 and 40 classes\nof objects for deep networks. While the original models are in CAD format,\nthere is also voxelized version [20]. Voxels are basically binary 3D matrices, each\nmatrix element determines the existence of unit cube in the respective location.\nVoxelized models have 30\u00d730\u00d730 resolution. The resolution is set to 32\u00d732\u00d732\nby simply zero padding one unit on each side. For the experiments 3 object classes\nare used: chair, bed and sofa having 989, 615 and 780 samples respectively. Each\nsample has 12 orientations O1,O2,\u00b7\u00b7\u00b7 ,O12 with 30 degrees of rotation between\nthem. In the experiments with 2 orientations we use O1, and O7 which represent\nthe object in opposite directions (0 \u25e6and 180\u25e6). Experiments with 4 orientations\nuse O1, O4, O7 and O10 (0\u25e6, 90\u25e6, 180\u25e6and 270\u25e6).\nWhile there are more object classes in the dataset, either they do not have\nsu\ufb03cient number of training samples for the system to converge (less than 500)\n\n8 C. \u00a8Ong\u00a8 un and A. Temizel\nor objects are highly symmetric such that di\ufb00erent orientations come out as same\nmodels (round or rectangle objects). For di\ufb00erent rotations, the system has been\ntested with paired input samples, unpaired (shu\ufb04ed) samples or removing any\ncorrespondence between samples in di\ufb00erent conditions by using one half of the\ndataset for one condition and the other half for other condition. The tests with\ndi\ufb00erent variants of input dataset show no signi\ufb01cant change on the output.\nNetwork structure: We designed our architecture building on a GAN archi-\ntecture for 3D object generation [17]. In this architecture, the generator network\nuses 4 3D transposed deconvolutional layers and a sigmoid layer at the end.\nLayers use ReLU activation functions and the generator takes a 200 dimensional\nvector as input. Output of the generator network is a 32 \u00d732 \u00d732 resolution\n3D matrix. Discriminator network mostly mirrors the generator with 4 3D con-\nvolutional layers with leaky ReLU activation functions and a sigmoid layer at\nthe end. It takes a 32 \u00d732 \u00d732 voxel grid as input and generates a single value\nbetween 0 and 1 as output, representing the probability of a sample being real.\nBoth networks use batch normalization between all layers. Kernel size of convo-\nlutional \ufb01lters is 4 and stride is 2.\nAdapting conditional GAN for generation of 3D models:To generate 3D\nmodels on di\ufb00erent rotations, we modi\ufb01ed the aforementioned GAN architecture\nand converted it into a conditional GAN. Conditional value y is concatenated\ninto z for generator input. For discriminator input, y is concatenated into real\nand generated samples as an additional channel. To train the discriminator,\nwe feed objects on di\ufb00erent rotations with corresponding condition values. To\ngenerate pairs, we change only the y and keep the z the same.\nTraining: Since generating 3D models is a more di\ufb03cult task than di\ufb00erentiat-\ning between real and generated ones, discriminator learns faster than generator\nand it overpowers the generator. If the learning pace is di\ufb00erent between genera-\ntor and discriminator, it causes instability in the network and it fails to generate\nrealistic results [7]. To keep the training in pace, we used a threshold for discrim-\ninator training. Discriminator is updated only if the accuracy is less than 95%\nin the previous batch. The learning rates are 0.0025 for generator and 0.00005\nfor discriminator. ADAM [11] is used for optimization with \u03b2 = 0.5 . System is\ntrained using a batch size of 128. For 2 orientations, condition 0 and 1 are used\nfor 0\u25e6and 180\u25e6respectively. For 4 orientations, condition 0, 1, 2 and 3 are used\nfor 0\u25e6, 90\u25e6, 180\u25e6, 270\u25e6respectively.\nVisual results prove that, standard conditional GAN fails to generate 3D\nmodels with the same attributes in di\ufb00erent rotations. In 2-conditional case, it\ngenerates a chair with 0\u25e6orientation, and a completely di\ufb00erent chair with 180\u25e6\norientation for the same input value. On the other hand, the proposed system\ncan generate 3D models of the same object category with same attributes with\n0\u25e6 and 180 \u25e6 orientations. Also the result of merge operation is given to show\n\nPaired 3D Model Generation with CGAN 9\n(a) chair\n (b) chair\n(c) bed\n(d) bed\n(e) sofa\n(f) sofa\nFig. 3.Results with 3 classes (chair, bed and sofa) using 2-conditions (rota-\ntions). The \ufb01rst two samples are the generated pairs, merged results are shown\nin boxes. (a), (c) and (e) show the pairs generated with standard conditional\nGAN. It is clearly visible that the samples belong to di\ufb00erent objects. Standard\nconditional GAN fails to generate the same object in di\ufb00erent conditions (rota-\ntions) as expected and the merged results are noisy. (b), (d) and (f) show the\npairs generated with the proposed method. The samples are very similar and\nthe merged results (intersection of samples) support this observation. Merged\nresults are also mostly noise-free and have more detail compared to standard\nconditional GAN.\n\n10 C. \u00a8Ong\u00a8 un and A. Temizel\nthe intersection of models. Since intersection of noise is mostly empty, merged\nmodel is also mostly noise-free. For these 3 classes, system is proven to generate\npair models on di\ufb00erent rotations.\nFor additional training of the proposed method, samples are generated by\nkeeping the input vector the same and setting the condition value di\ufb00erently.\nThen the outputs are merged and fed into the discriminator. Only the generator\nis updated in this step. Experiments show that, also updating the discriminator\nin this step causes overtraining and makes the system unstable. Since this step is\nfor enforcing the generator to generate the same sample in di\ufb00erent conditions,\ntraining of the discriminator is not necessary.\nMerge method: Merging the generated samples is domain speci\ufb01c. For our\ncase, generated samples are 3D voxelized models with values between 0 and 1\nrepresenting the probability of the existence of the unit cube on that location.\nFirst aligning the samples generated with di\ufb00erent orientations and then simply\naveraging their 3D matrices, we get the merged result. In Figure 4, we illustrate\nthe merging procedure with a 2-conditional case with chair dataset. Generator\nwill output two chairs with 0 \u25e6 and 180\u25e6 rotations respectively. We can simply\nrotate the second model 180 \u25e6 to align both samples. Then, we average these\n3D matrices. By averaging we get the probability of the existence of unit cubes\nin each location taking both outputs into account. If chairs are similar, the\nintersection of them will also be a similar chair (Fig. 4(a)) and if the chairs\nare not similar, their intersection will be meaningless (Fig. 4(b)). By feeding\nthese merged results into the discriminator, we make the network evaluate the\nintersection model and train the generator using this information.\n(a)\n (b)\nFig. 4.Examples of merging operation. After generating pairs, one of the pairs\nis aligned with the other. Second sample is rotated to align with the \ufb01rst one\nin these examples. Then aligned samples are merged to form a new one. Simple\naveraging is applied to aligned pairs to get the intersection. (a) The result of the\nmerging operation will be similar to the generated samples if the samples are\nsimilar, (b) the result will be meaningless if the samples are di\ufb00erent.\nResults: The proposed framework has been implemented using Tensor\ufb02ow [1]\nversion 1.4 and tested with 3 classes: chair, bed and sofa. The results are observed\n\nPaired 3D Model Generation with CGAN 11\nafter training the model for 1500 epochs with the whole dataset. Dataset is\ndivided into batches of 128 samples. For comparison, we used the conditional\nGAN that we adapted for 3D model generation as the baseline method. Both\nsystems have been trained with the same parameters and same data. Results\nare generated with the same input and di\ufb00erent condition values. To visualize\nthe results, binary voxelization is used with a threshold of 0.5. Fig. 3 shows the\nvisual results. Note that the presented results are visualizations of raw output\nwithout any post processing or noise reduction.\nAs there is no established metric for the evaluation of generated samples,\nwe introduce 2 di\ufb00erent evaluation metrics: Average Absolute Di\ufb00erence (AAD)\nand Average Voxel Agreement Ratio (AVAR).\nRaw outputs are 3D matrices for each generated model and each element\nof these matrices is a probability value between 0 and 1. For the calculation of\nAAD with n-conditions, \ufb01rst, the generated models S1,...,S n aligned with S0\nto get SR\n1 ,...,S R\nn then AAD can be formulated as follows:\nAAD=\n\u2211n\u22121\ni=0\n\u2211\n\u2200x,y,z |SR\ni (x,y,z)\u2212M(x,y,z)|\ntotal # of matrix elements\nn (4)\nAs a result of AAD a single di\ufb00erence metric is obtained for that object. A\nlower AAD value indicates agreement of the generated models with the merged\nmodel and it is desired to have an AAD value closer to 0.\nFor the calculation of Average Voxel Agreement Ratio (AVAR), \ufb01rst the\naligned 3D matrices are binarized with a threshold of 0.5 to form voxelized SRB\ni\nMB and then Average Voxel Agreement Ratio (AVAR) can be formulated as:\nAVAR =\n\u2211n\u22121\ni=0\n\u2211\n\u2200x,y,z SRB\ni (x,y,z) \u22c0MB(x,y,z)\u2211\n\u2200x,y,z SRB\ni (x,y,z)\nn (5)\nwhere \u22c0 is the binary logical AND operator. AVAR value of 0 indicates\ndisagreement while a value of 1 indicates agreement of the models with the\nmerged model and it is desired to have an AVAR value closer to 1.\nResults for 2-conditions and a batch of 128 pairs are given in Table 1. AAD\nand AVAR results are calculated separately for each pair in the batch and then\naveraged to get a single result for the batch. The results show that the proposed\nmethod reduces the average di\ufb00erence signi\ufb01cantly; 3, 2.4 and 4.5 times for\nchair, bed and sofa respectively. Here the results are highly dependent to object\nclass. Di\ufb00erent beds and sofas are naturally more similar than di\ufb00erent chairs.\nWhile di\ufb00erent bed shapes are mostly same except headboards, chairs can be\nvery di\ufb00erent considering stools, seats etc. Also we can see it in the results, the\nproposed method improved the similarity of generated chair pairs from 0.32 to\n0.79. While the generated chair pairs are very di\ufb00erent with the baseline method,\nthe proposed method generated very similar pairs. For bed and sofa the baseline\nsimilarities are 0.69 and 0.74, relatively more similar as expected. The proposed\nmethod improved the results to 0.89 and 0.95 for bed and sofa respectively by\nconverging to the same model.\n\n12 C. \u00a8Ong\u00a8 un and A. Temizel\nTable 1. Comparison of the proposed method with baseline using di\ufb00erent\nobject classes for 2-conditions and a batch (128) of pairs. AAD: Average Absolute\nDi\ufb00erence between generated matrices, AVAR: Average Voxel Agreement Ratio.\nChair Bed Sofa\nAAD AVAR AAD AVAR AAD AVAR\nBaseline 0.027 0.32 0.029 0.69 0.018 0.74\nProposed 0.009 0.79 0.012 0.89 0.004 0.95\nTable 2. Comparison of the proposed method with baseline using di\ufb00erent\nobject classes for 4-conditions. The same metrics are used as in the 2-condition\ncase.\nChair Bed Sofa\nAAD AVAR AAD AVAR AAD AVAR\nBaseline 0.034 0.36 0.043 0.65 0.034 0.62\nProposed 0.024 0.61 0.021 0.82 0.013 0.90\nThe proposed system has also been tested with 4-conditions. For 4 orienta-\ntions, condition 0, 1, 2 and 3 are used for 0 \u25e6, 90\u25e6, 180\u25e6 and 270\u25e6 respectively.\nAlso for merging operation, all generated samples are aligned with the \ufb01rst sam-\nple with 0 \u25e6 rotation. For that purpose 2 nd, 3rd and 4th samples are rotated by\n270\u25e6, 180\u25e6 and 90\u25e6 respectively. After aligning all 4 samples, they are merged\ninto a single model by averaging.\nFig. 5 shows the visual results for 4-conditional case with the same experi-\nmental setup. Experimental results in terms of the same metrics are presented in\nTable 2. Standard conditional GAN generates 4 di\ufb00erent chairs on 4 rotations.\nOn the other hand the proposed method enforces the network to generate the\nsame chair on 4 di\ufb00erent rotations. Since the problem is more complex for 4\nrotations, individual generated samples are noisier and have lower resolution.\nThe improvement rates compared to the baseline are relatively lower than 2-\ncondition case because of the increased complexity of the problem. To account\nfor the increasing complexity of the model with higher number of conditions,\nmore training data and/or higher number of epochs need to be used.While gen-\nerating better samples with more training may seem crucial, it doesnt change the\nbehavior of the networks. Conditional GAN keeps generating di\ufb00erent samples\nand proposed model generates paired samples with each training iteration.\n5 Conclusions and Future Work\nIn this paper, we presented a new approach to generate paired 3D models with\nconditional GAN. First, we adapted the conditional GAN to generate 3D models\non di\ufb00erent rotations. Then, we integrated an additional training step to solve\nproblem of generation of pair samples, which is a shortcoming of standard con-\n\nPaired 3D Model Generation with CGAN 13\nditional GAN. The proposed method is generic and it can be integrated into any\nconditional GAN. The results show the potential of the proposed method for the\npopular problem of joint distribution learning in GANs.\nWe demonstrated that proposed method works successfully for 3D voxel mod-\nels on 2 and 4 orientations. Visual results and the objective evaluation metrics\ncon\ufb01rm the success of the proposed method. The di\ufb00erence between generated\nmodels are reduced signi\ufb01cantly in terms of the average di\ufb00erence. The merged\nsamples create noise-free high-resolution instances of the objects. This approach\ncan also be used for generating better samples compared to traditional GAN for\na particular object class.\nThe extension of the method to work with higher number of conditions is\ntrivial. However, as the training of the system takes a long time, we leave the\nexperiments with higher number of conditions and classes as a future work. The\nproposed solution is generic and could be applied to other types of data. As\na next step, we are aiming to test the method on generation of 2D images to\ninvestigate the validity of the method for di\ufb00erent data types.\n\n14 C. \u00a8Ong\u00a8 un and A. Temizel\n(a) chair\n (b) chair\n(c) bed\n (d) bed\n(e) sofa\n (f) sofa\nFig. 5.Visual results with 4 conditions. The \ufb01rst four samples are the generated\nobjects, merged results are shown in boxes. (a), (c) and (e) show the objects and\nthe merged result obtained with standard conditional GAN. (b), (d) and (f)\nshow the objects and the merged result obtained with the proposed method\nThe samples are very similar and the merged results (intersection of samples)\nsupport this claim. Merged results are also mostly noise-free and have more\ndetail compared to standard conditional GAN.\n\nPaired 3D Model Generation with CGAN 15\nReferences\n1. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\nG.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,\nIrving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\nJ., Man\u00b4 e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J.,\nSteiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\nVi\u00b4 egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,\nX.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\nhttps://www.tensorflow.org/, software available from tensor\ufb02ow.org\n2. Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Representation learning\nand adversarial generation of 3d point clouds. arXiv preprint arXiv:1707.02392\n(2017)\n3. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein gan. arXiv preprint\narXiv:1701.07875 (2017)\n4. Che, T., Li, Y., Jacob, A.P., Bengio, Y., Li, W.: Mode regularized generative\nadversarial networks. arXiv preprint arXiv:1612.02136 (2016)\n5. Chen, W.C., Chen, C.W., Hu, M.C.: Syncgan: Synchronize the latent space of cross-\nmodal generative adversarial networks. arXiv preprint arXiv:1804.00410 (2018)\n6. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Infogan:\nInterpretable representation learning by information maximizing generative adver-\nsarial nets. In: Advances in neural information processing systems. pp. 2172\u20132180\n(2016)\n7. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural\ninformation processing systems. pp. 2672\u20132680 (2014)\n8. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-\ntional adversarial networks. arXiv preprint arXiv:1611.07004 (2017)\n9. Jiang, C., Marcus, P., et al.: Hierarchical detail enhancing mesh-based shape gen-\neration with 3d generative adversarial network. arXiv preprint arXiv:1709.07581\n(2017)\n10. Kim, T., Cha, M., Kim, H., Lee, J.K., Kim, J.: Learning to discover cross-domain\nrelations with generative adversarial networks. arXiv preprint arXiv:1703.05192\n(2017)\n11. Kingma, D.P., Ba, J.L.: Adam: Amethod for stochastic optimization. In: Pro-\nceedings of the 3rd International Conference on Learning Representations (ICLR)\n(2015)\n12. Liu, M.Y., Tuzel, O.: Coupled generative adversarial networks. In: Advances in\nneural information processing systems. pp. 469\u2013477 (2016)\n13. Luan, F., Paris, S., Shechtman, E., Bala, K.: Deep photo style transfer. CoRR,\nabs/1703.07511 2 (2017)\n14. Mao, X., Li, Q., Xie, H.: Aligngan: Learning to align cross-domain images with con-\nditional generative adversarial networks. arXiv preprint arXiv:1707.01400 (2017)\n15. Mirza, M., Osindero, S.: Conditional generative adversarial nets. arXiv preprint\narXiv:1411.1784 (2014)\n16. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434\n(2015)\n17. Smith, E., Meger, D.: Improved adversarial systems for 3d object generation and\nreconstruction. arXiv preprint arXiv:1707.09557 (2017)\n\n16 C. \u00a8Ong\u00a8 un and A. Temizel\n18. Wang, C., Xu, C., Wang, C., Tao, D.: Perceptual adversarial networks for image-to-\nimage transformation. IEEE Transactions on Image Processing 27(8), 4066\u20134079\n(2018)\n19. Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: Learning a probabilistic\nlatent space of object shapes via 3d generative-adversarial modeling. In: Advances\nin Neural Information Processing Systems. pp. 82\u201390 (2016)\n20. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets: A\ndeep representation for volumetric shapes. In: Proceedings of the IEEE conference\non computer vision and pattern recognition. pp. 1912\u20131920 (2015)\n21. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation us-\ning cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593 (2017)",
  "full_text_length": 35656,
  "link_pdf": "https://arxiv.org/pdf/1808.03082v2",
  "paper_id": "1808.03082v2"
}