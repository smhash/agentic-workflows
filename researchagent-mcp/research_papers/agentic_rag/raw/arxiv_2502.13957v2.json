{
  "source": "arxiv",
  "query": "Agentic RAG",
  "fetched_at": "2025-11-21T17:17:53.767342",
  "title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation",
  "url": "http://arxiv.org/abs/2502.13957v2",
  "content": "arXiv:2502.13957v2  [cs.CL]  31 May 2025\nRAG-Gym: Systematic Optimization of Language\nAgents for Retrieval-Augmented Generation\nGuangzhi Xiong\u22171, Qiao Jin\u22172, Xiao Wang3, Yin Fang2, Haolin Liu1, Yifan Yang2, Fangyuan\nChen4, Zhixing Song5, Dengyu Wang6, Minjia Zhang3, Zhiyong Lu\u20202, and Aidong Zhang\u20201\n1University of Virginia, 2National Institutes of Health, 3University of Illinois at Urbana Champaign,\n4Dana-Farber Cancer Institute, 5University of Alabama at Birmingham, 6Yale School of Medicine\nAbstract\nRetrieval-augmented generation (RAG) has shown great promise for knowledge-\nintensive tasks and recently advanced with agentic RAG, where language agents\nengage in multi-round interactions with external knowledge sources for adaptive\ninformation retrieval. However, existing agentic RAG methods often depend on\nad-hoc prompt engineering and lack a unified optimization framework. We in-\ntroduce RAG-Gym, a comprehensive platform that systematically explores three\noptimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic\ntraining. For prompt engineering, we propose Re2Search, a novel agent incorporat-\ning reasoning reflection that significantly outperforms standard prompts. In actor\ntuning, we evaluate three popular post-training algorithms with fine-grained process\nsupervision and identify direct preference optimization as the most effective. We\nfurther demonstrate that a trained critic can enhance inference by selecting higher-\nquality intermediate reasoning steps. Together, these findings lead to the optimized\nRe2Search++ agent, which surpasses most recent methods like Search-R1 by a\nrelative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact\nof different reward sources and analyze scaling properties in training and inference,\noffering practical insights for agentic RAG optimization. The project homepage is\navailable at https://rag-gym.github.io/.\n1 Introduction\nLarge language models (LLMs) often struggle with knowledge-intensive questions when lacking\nsufficient or up-to-date domain knowledge, leading to inaccurate responses or hallucinations [ 97,\n59, 28]. Retrieval-augmented generation (RAG) addresses this by grounding outputs in relevant\ninformation from information retrieval (IR) systems, improving both accuracy and verifiability of\nanswers [42, 18]. Agentic pipelines such as ReAct [ 91] enhances conventional RAG by allowing\nLLMs to actively generate search queries and interact with IR systems in multiple rounds, which has\nbeen shown to be more effective in solving complex tasks that need multi-hop reasoning [91, 4, 65].\nHowever, most existing agentic RAG methods focus on prompt engineering [73, 4, 31, 54], which\ndemands substantial manual effort and often fails to generalize across tasks [40, 70, 2].\nMeanwhile, although various LLM post-training algorithms have been developed to enhance down-\nstream performance, they are not directly suited for agentic RAG, where the model must dynamically\nadjust its token-generation strategy in response to newly retrieved context during the reasoning\nprocess. Recent works have adapted reinforcement learning with outcome-based rewards for agentic\nRAG [69, 33, 8]. However, by overlooking process-level supervision, these approaches risk generat-\ning suboptimal intermediate search actions and exhibit limited generalization on unseen data. Given\n\u2217Equal contribution. \u2020Co-correspondence.\nPreprint. Under review.\n\nthat the retrieval steps fundamentally shape the reasoning trajectory and ultimately influence the final\nanswer, providing fine-grained supervision over these intermediate steps is essential for optimizing\nagentic RAG. Nevertheless, systematic analyses on how to optimize the language agent and identify\nbest practices for enhancing overall agentic RAG performance are still lacking.\nIn this work, we present RAG-Gym, a systematic framework that enhances agentic RAG along three\ndimensions: prompt engineering, actor tuning, and critic training. We review and compare the func-\ntional components of existing agentic RAG pipelines (see Table 1) and introduce a novel agent design\nRe2Search that leverages reasoning reflection to improve performance. Our comprehensive experi-\nments across three widely used LLM post-training algorithms reveal that fine-grained, process-level\nsupervision substantially boosts performance, particularly when both positive and negative feedback\nare integrated. Furthermore, we show that training a critic to evaluate intermediate steps yields\nadditional gains across diverse LLMs. By integrating these insights, our optimized Re2Search++\nagent achieves superior performance than existing methods on challenging knowledge-intensive tasks\n(+ 3.2%\u223c11.6% in average F1), especially on unseen datasets (+ 8.5%\u223c24.7%). We also discuss\nreward sources as well as the training and inference scaling properties of agentic RAG, providing\npractical guidelines for future optimization. Our key contributions are summarized as follows:\n\u2022 We introduceRAG-Gym, a comprehensive framework that integrates advanced prompt engineering,\nactor tuning, and critic training to enhance agentic RAG.\n\u2022 Our extensive experiments uncover best practices across these dimensions and lead to the develop-\nment of the optimized agent Re2Search++, which consistently outperforms existing methods on\nchallenging knowledge-intensive tasks.\n\u2022 We provide a detailed analysis of reward sources as well as training and inference scaling properties,\noffering actionable insights for future advancements in agentic RAG.\n2 RAG-Gym Framework\nTo facilitate fine-grained process-level supervision and systematic evaluation of optimization methods\nfor agentic RAG, we introduce the RAG-Gym framework. RAG-Gym formulates knowledge-intensive\nquestion answering as a high-level MDP with well-defined intermediate actions, and provides a\nmodular approach for optimizing language agents across three key components. An overview of\nRAG-Gym is presented in Figure 1.\n2.1 Knowledge-intensive Question Answering as Markov Decision Process\nWhile sequential token generation in LLMs can be modeled as an MDP [43, 49, 93], the integration\nof interactions with the IR environment introduces complex and inconsistent state transitions across\nagent architectures. To address this, we propose a hierarchical MDP formulation in RAG-Gym that\nunifies diverse agentic RAG designs. At the high level, agentic RAG is represented as a sequence of\nreasoning steps that interact with an IR system, while at the low level, each action involves sequential\ntoken generation by LLMs. Below, we formally define the components of the high-level MDP.\nStates. For the agentic RAG process of a given question Q, we define the state st at time step\nt to be a set consisting of the original question Q and the information-seeking history Ht. The\ninformation-seeking history is a sequence of search queries q1, \u00b7\u00b7\u00b7 , qt\u22121 and their corresponding sets\nof retrieved documents D1, \u00b7\u00b7\u00b7 , Dt\u22121, and is used to augment the agent\u2019s knowledge for answering\nthe original question. The initial state is defined as s1 = (Q, H1), where H1 is an empty set.\nActions. Although agents may employ various strategies to reason about the current state and generate\ndifferent token sequences, RAG-Gym standardizes these outputs by defining a common macro-action\nspace. At each time step t, the action at is either a search query or a predicted answer to the original\nquestion. While the detailed generated token sequences may differ among agent designs, they must\nalways be semantically equivalent to a designated macro-action within the context of agentic RAG.\nEnvironment. The high-level MDP environment in RAG-Gym is powered by an IR system, which\nis central to the agentic RAG approach. At each time step t, if the agent\u2019s action at is a search\nquery qt, the IR system returns a corresponding set of documents Dt. The state is then updated\nfrom st = (Q, Ht) to st+1 = (Q, Ht \u222a {(qt, Dt)}). Conversely, if at predicts an answer to Q, the\n2\n\nProcess \nReward\nProcess \nReward\nProcess \nReward\nAnswer \nGeneration\nQuestion\nReasoning\nRetrieval \nAugmentation\nDocument \nSummarization\nReasoning \nReflection\nQuery \nGeneration\nWhat is the date of death of\nAlberto De Martino?\n\u2026The date of death of Alberto\nDe Martino is 2 June 2015\u2026\n \nWhat is the date of death of the\ndirector of film Holocaust 2000?\n \nWho is the director of the film\n\u2018Holocaust 2000\u2019?\n \n\u2026 director of the film \u2018Holocaust\n2000\u2019 is Alberto De Martino\u2026\nQuery:\nDocuments: Agent\nAction(s)\nWhat is the date of birth\nof Alberto De Martino?\nWhat is the date of death\nof Alberto De Martino?\nAlberto De Martino\u2019s date\nof death is 1990. \nState\nQuestion: ...\nQuery: ...\nDocuments: ...\nQuery:\nDocuments:\nQuery:\nQuery:\nAnswer:\nState\nEnvironment\n Environment\nAction\nAction\nAction\nAction\nAction\nStateAction\nCritic\nProcess \nReward\nProcess \nReward\nState\nActorState Action\nPrompt Engineering Actor Tuning Critic Training\nWhat is the date of death\nof Alberto De Martino?\nQuery:\nAction\nQuestion:\nState Action\nSupervised Fine-tuning\nState\nAction\nDirect Preference Optimization\nAction\nState Action\nProximal Policy Optimization\nProcess \nReward\nModel\nInput Question: What is the date of death of the director of film Holocaust 2000?\nHotpotQA F144.91% 55.22% 60.19%41.09%\nReSearch Direct Preference Optimization Critic\nFigure 1: Overview of the RAG-Gym framework. RAG-Gym employs a modular design, comprising\nprompt engineering, actor tuning, and critic training, to systematically optimize agentic RAG perfor-\nmance. By leveraging all three components, RAG-Gym improves the F1 score of the ReAct agent on\nHotpotQA from 41.09% to 60.19%.\nepisode terminates. To maintain stable and reproducible state transitions, the configuration of the IR\nsystem (e.g., the number of returned documents) remains constant throughout.\nRewards. For the high-level MDP, the immediate reward for a state-action pair(st, at) is defined as\nzero when at is a search query, and as the correctness of the predicted answer when at is an answer.\nMoreover, by formulating knowledge-intensive QA as a high-level MDP, we can directly assess the\nquality of intermediate actions, with process-level rewards derived from various sources (e.g., human\nannotations, LLM evaluations, or rollouts). This enables both the evaluation of intermediate actions\nand the fine-grained supervision of language agents through process-level feedback.\n2.2 Systematic Optimization of Agentic Retrieval-augmented Generation\nWith the high-level MDP formulation, RAG-Gym optimizes the agentic RAG system through three\nkey components: (1) prompt engineering, which refines the language agent\u2019s structure and operational\ndesign; (2) actor tuning, which adjusts the LLM parameters to improve decision-making; and (3)\ncritic training, which develops an external verifier to assess the quality of generated macro-actions.\n2.2.1 Prompt Engineering\nThe first aspect of optimizing agentic RAG is crafting effective prompts that guide the language\nmodel in generating the appropriate actions. The system prompt defines the agent\u2019s functional\ncapabilities when processing a given state. RAG-Gym summarizes the essential functions into six\ndistinct categories:\n\u2022 Answer generation: The agent produces a final answer to the question.\n\u2022 Question reasoning: The agent outlines reasoning steps before providing the answer.\n\u2022 Retrieval augmentation: The agent incorporates retrieved content to enhance its answer.\n\u2022 Query generation: The agent formulates queries to search for relevant documents.\n\u2022 Document summarization: The agent condenses retrieved content to extract key information.\n\u2022 Reasoning reflection: The agent reviews its reasoning to identify any unverified claims.\n3\n\nWhile the first five components have already been employed in existing agent architectures, the final\ncomponent reasoning reflection is a novel addition by RAG-Gym. Inspired by recent advancements in\nreasoning models in which the models can reflect on their own reasoning process for self-correction\n[19], the newly introduced reasoning reflection directs the agent to scrutinize its reasoning process\nand identify claims that are unsupported by the information seeking history, thereby linking search\nquery generation to answer reasoning to produce more precise and relevant queries.\nCombining reasoning reflection with other existing components, we propose a new agent architecture\ncalled Re2Search, which stands for Reasoning, Reflection, and Search. A Re 2Search agent first\nreasons about all available information to construct an answer to the original question. It then\nreflects on its reasoning process to identify unverified claims that lack sufficient justification based\non available evidence. These unverified claims form the basis for generating the next search query\nthat is designed to retrieve the missing information required for constructing the answer. Table 1\nsummarizes the presence or absence of these components in several existing agent architectures,\nincluding Direct, CoT [81], RAG [42], ReAct [91], Search-o1 [44], and our proposed Re2Search,\neach enabling different LLM capabilities through prompting.\nTable 1: A comparative overview of agent architectures based on their functional components.\nComponent Direct CoT [81] RAG [42] ReAct [91] Search-o1 [44] Re 2Search\nAnswer Generation \u2714 \u2714 \u2714 \u2714 \u2714 \u2714\nQuestion Reasoning \u2717 \u2714 \u2714 \u2714 \u2714 \u2714\nRetrieval Augmentation \u2717 \u2717 \u2714 \u2714 \u2714 \u2714\nQuery Generation \u2717 \u2717 \u2717 \u2714 \u2714 \u2714\nDocument Summarization \u2717 \u2717 \u2717 \u2717 \u2714 \u2714\nReasoning Reflection \u2717 \u2717 \u2717 \u2717 \u2717 \u2714\n2.2.2 Actor Tuning\nThe second aspect of optimizing agentic RAG is tuning LLM parameters to directly enhance reasoning\ncapability. Decomposing knowledge-intensive QA into intermediate steps, the high-level MDP in\nRAG-Gym enables the targeted optimization of language agents by focusing on the generated action\nat each step, reducing the task to standard text generation. This streamlines the training process and\nfacilitates the application of various LLM post-training algorithms to enhance agent performance.\nProcess Reward Data Collection. As discussed in our high-level MDP definition, the process reward\nfor intermediate actions can be derived from multiple sources, including human annotations, LLM\nevaluations, or rollouts. In our implementation, we focus on collecting process reward data using\nadvanced LLMs such as GPT-4o [1]. Specifically, we sample trajectories from an untuned agent and\nobtain process reward annotations from GPT-4o, while filtering out trajectories that do not result\nin a correct final answer using the outcome reward. This strategy enables us to efficiently gather\nhigh-quality process reward data, which is subsequently used to optimize the LLMs for agentic RAG.\nFurther details on alternative process reward sources can be found in Section 4.1, with additional\ninformation about the data collection pipeline provided in Appendix E.\nProcess-based Training Algorithms.\nLet D denote the process reward dataset, which consists of tuples (s, a+, a\u2212), where s is a state, a+\nis a preferred (high-quality) action, and a\u2212 is a less-preferred (lower-quality) action. Each action is\nannotated based on the quality of the generated query or predicted answer. We assign the preference\nlabel to the entire token sequence produced when reasoning about the state, thereby reducing process-\nbased actor tuning to a standard text generation problem. RAG-Gym implements and compares three\nwidely used LLM post-training algorithms:\n\u2022 Supervised fine-tuning (SFT) [52]: This method uses high-quality intermediate actions to train\nlanguage agents by maximizing the log-likelihood of preferred actions (a+) conditioned on their\nrespective states s.\n\u2022 Direct preference optimization (DPO) [56]: This approach employs a contrastive learning\nframework that utilizes both preferred ( a+) and unpreferred ( a\u2212) actions. The DPO objective\nencourages the agent to increase the likelihood of preferred actions while decreasing that of\nunpreferred actions.\n4\n\n\u2022 Proximal policy optimization (PPO) [60]: This is an online reinforcement learning algorithm for\npolicy optimization. The collected data D is first used to train a process reward model r\u03d5(s, a).\nPPO then optimizes the agent to maximize the process reward of newly generated actions, while\nconstraining policy updates to ensure stability.\n2.2.3 Critic Training\nThe third aspect of optimizing agentic RAG involves training a critic, denoted as r\u03d5, to act as an\nexternal evaluator of generated actions. The critic is designed to predict process rewards for a given\nstate-action pair (s, a). Its training objective employs a contrastive loss that distinguishes preferred\nactions from less-preferred ones, following the preference modeling approach widely used in LLM\nalignment and reward modeling [47, 52]:\nLcritic(\u03d5) =\u2212E(s,a+,a\u2212)\u223cD\nh\nlog \u03c3\n\u0000\nr\u03d5(s, a+) \u2212 r\u03d5(s, a\u2212)\n\u0001i\n, (1)\nwhere \u03c3 is the sigmoid function and D denotes the process reward dataset containing both preferred\n(a+) and less-preferred (a\u2212) actions.\nWhile process reward modeling has been studied in the context of math reasoning [62, 46], its appli-\ncation to agentic RAG for knowledge-intensive question answering remains largely underexplored.\nIn RAG-Gym, our process-level critic is tailored to evaluate intermediate actions such as search\nqueries, rather than only final answers. This approach enables more fine-grained and actionable\nfeedback, facilitating the optimization of agentic RAG systems through process-level supervision.\nOnce trained, the critic provides targeted feedback on generated actions, guiding the language agent\nto make decisions that are more likely to lead to successful outcomes.\n3 Main Results\n3.1 Experimental Settings\nTo assess the performance of various agents on knowledge-intensive QA tasks and evaluate the\nbenefits of different optimization methods in RAG-Gym, we consider four datasets that are both\nknowledge- and reasoning-intensive, spanning general and medical domains. Specifically, we use\nHotpotQA [90], 2WikiMultihopQA [21], and Bamboogle [ 54], which are popular multi-hop QA\ndatasets constructed from Wikipedia, as well as the MedQA dataset [34], which consists of medical\nexam questions that require specialized domain knowledge and complex reasoning. Following prior\nwork [61], HotpotQA, 2WikiMultihopQA, and Bamboogle are evaluated using Exact Match (EM)\nand F1 scores, while the multi-choice MedQA dataset is assessed with accuracy (Acc). We also\ncompute the average EM and F1 scores across different tasks, treating accuracy as equivalent to both\nmetrics in the multi-choice evaluation setting. For actor and critic training in RAG-Gym, 1k questions\nwere sampled from the HotpotQA and MedQA training sets for process reward data collection. To\ntest the generalizability of the tuned agents, 2WikiMultihopQA and Bamboogle were evaluated using\nLLMs trained on HotpotQA. More implementation details can be found in Appendices C, E, H.\n3.2 Performance Improvements by Prompt Engineering and Actor Tuning\nTable 2 presents a performance comparison of various agents and their tuned versions using different\nactor tuning algorithms in RAG-Gym. The results indicate that the Re 2Search agent consistently\noutperforms other agents in both zero-shot and actor-tuned settings. Furthermore, when comparing\nTable 2 with Table 1, which details the functional components of each agent, it can be observed\nthat more components generally leads to improved performance. This observation validates the\neffectiveness of the summarized functions in RAG-Gym, as well as the design of the Re2Search agent,\nwhich incorporates all identified components, including our newly proposed reasoning reflection.\nAdditional case studies of our proposed Re2Search agent are provided in Appendices G.1 and G.2.\nBy comparing different process supervision approaches for actor tuning, we observe that process\nsupervision consistently enhances agent performance relative to the zero-shot learning (ZSL) baseline.\nThis improvement underscores the critical role of process supervision in refining agentic RAG.\nNotably, for Direct, CoT, and RAG agents, where tuning focuses solely on answer generation, SFT\nslightly outperforms both DPO and PPO. In contrast, for ReAct, Search-o1, and Re2Search agents,\n5\n\nTable 2: Agent performance with Llama-3.1-8B backbone. Highest scores are bolded.\nMethod Agent HotpotQA 2Wiki Bamboogle MedQA Average\nEM F1 EM F1 EM F1 Acc EM F1\nZero-shot\nLearning\nDirect 21.10 27.93 24.10 27.68 9.60 14.89 61.82 29.16 33.08\nCoT 27.10 35.17 25.70 30.08 37.60 49.50 69.60 40.00 46.09\nRAG 38.30 48.57 32.00 36.91 22.40 33.73 66.85 39.89 46.51\nReAct 30.70 41.09 28.90 35.03 32.00 41.35 62.37 38.49 44.96\nSearch-o1 35.30 47.33 34.00 41.29 44.80 52.50 66.14 45.06 51.82\nRe2Search 34.00 44.91 41.50 49.06 44.80 55.33 70.31 47.65 54.90\nRAG-Gym\nSupervised\nFine-tuning\nDirect 22.80 31.67 28.00 33.17 20.00 27.21 63.63 33.61 38.92\nCoT 26.50 35.60 27.30 32.10 42.40 53.89 69.68 41.47 47.82\nRAG 41.50 52.26 38.00 42.74 28.80 40.76 67.79 44.02 50.89\nReAct 35.50 46.06 31.00 36.79 34.40 44.17 66.69 41.90 48.43\nSearch-o1 38.20 50.02 39.00 45.91 46.40 57.18 67.64 47.81 55.19\nRe2Search 37.60 49.16 44.00 50.54 44.80 56.78 69.52 48.98 56.50\nRAG-Gym\nDirect\nPreference\nOptimization\nDirect 20.80 28.79 25.20 29.45 12.00 20.67 62.37 30.09 35.32\nCoT 26.30 35.06 28.20 32.84 40.80 51.67 71.33 41.66 47.73\nRAG 38.00 49.38 37.60 42.88 28.80 39.57 67.79 43.05 49.91\nReAct 33.00 43.96 32.20 39.24 44.80 54.35 68.89 44.72 51.61\nSearch-o1 42.20 54.34 44.10 52.66 42.40 55.59 70.23 49.73 58.21\nRe2Search 42.20 55.22 44.30 51.36 48.00 56.57 72.11 51.65 58.82\nRAG-Gym\nProximal\nPolicy\nOptimization\nDirect 19.20 26.17 25.60 28.84 7.20 12.17 61.12 28.28 32.08\nCoT 25.50 33.68 24.20 29.02 43.20 52.54 68.50 40.35 45.94\nRAG 37.70 47.60 32.00 36.29 28.80 40.24 68.03 41.63 41.44\nReAct 35.80 47.56 33.20 40.06 36.80 46.79 67.32 43.28 50.43\nSearch-o1 38.30 50.24 32.60 39.34 50.40 59.92 70.15 47.86 54.91\nRe2Search 38.40 50.30 41.40 48.06 49.60 62.06 71.72 50.28 58.04\nwhere the tuning process also involves generating high-quality queries, DPO and PPO surpass\nSFT, with DPO demonstrating a slight edge over PPO on most tasks. These findings highlight the\nimportance of utilizing both positive and negative samples during training, especially for agents that\nrequire complex, multi-step reasoning with environmental feedback. Furthermore, the tuned agents\ntend to generate more search queries during inference, as elaborated in Appendix F.\n3.3 Performance Improvements by Critic Training\nFigure 2 illustrates the performance improvements achieved through critic training. The label \u201cWith\nCritic\u201d indicates that an external critic evaluates 10 sampled actions at each step to select the best one.\nIn our experiments, all agents except for \u201cDirect\u201d consistently benefit from critic training. Moreover,\nthese performance gains transfer to actors using different LLMs. As shown in the figure, not only does\nthe original Llama-3.1-8B benefit from the trained critic, but both the DPO-tuned Llama-3.1-8B and\nGPT-4o-mini also experience significant improvements across all datasets using the same critic. This\nhighlights the potential of employing trained critics as a plug-and-play module to enhance agentic\nRAG performance, particularly for proprietary LLMs where direct fine-tuning is not feasible. A case\nstudy of using trained critics during inference is provided in Appendix G.3.\n3.4 Comparisons with Outcome Supervision Methods\nCombining the findings from previous sections, we introduce Re2Search++, an optimized agent that\nintegrates the best choices from each optimization direction. Built on Re2Search and tuned with DPO\nwhile utilizing a trained critic for action selection, Re2Search++ is evaluated against recent methods\nsuch as Search-R1 [33] and R1-Searcher [69], which rely on outcome supervision via reinforcement\nlearning (RL) with over 8k training questions. As these methods primarily focus on general-domain\nquestions, we exclude MedQA from this evaluation for a fair comparison. Table 3 shows that\nRe2Search++ achieves performance comparable to that of the RL-tuned agents on the datasets used\nfor their training (HotpotQA for Search-R1; HotpotQA and 2WikiMultihopQA for R1-Searcher),\nwhile significantly outperforming them on unseen datasets and achieving the best performance on\naverage. This result underscores the overfitting issues of RL-based outcome supervision methods\nand highlights the robustness and generalizability of Re2Search++ through its fine-grained process\nsupervision on intermediate steps.\n6\n\nWithout Critic With Critic\n20\n40\n60\n80F1 / Accuracy\n33.08 32.45\nDirect\nWithout Critic With Critic\n40\n60\n80\n46.09 49.02\nCoT\nWithout Critic With Critic\n40\n60\n80\n46.51\n55.64\nRAG\nWithout Critic With Critic\n40\n60\n80\n44.96\n56.47\nReAct\nWithout Critic With Critic\n40\n60\n80F1 / Accuracy\n51.81\n61.04\nSearch-o1\nWithout Critic With Critic\n40\n60\n80\n54.73\n62.41\nRe\u00b2Search\nWithout Critic With Critic\n40\n60\n80\n58.81\n64.12\nRe\u00b2Search (Llama-3.1-8B-DPO)\nWithout Critic With Critic\n40\n60\n80\n61.06\n65.30\nRe\u00b2Search (GPT-4o-mini)\nAvg. Without Critic Avg. With Critic HotpotQA F1 2WikiMultihopQA F1 Bamboogle F1 MedQA Accuracy\nFigure 2: Performance improvements across various agents with critics.\nTable 3: Comparison of Re 2Search++ and other methods. Shading indicates in-domain model\nperformance. CEM represents the \u201cCover Exact Match\u201d used in [69].\nLLM Method HotpotQA 2WikiMultihopQA Bamboogle Average\nEM CEM F1 EM CEM F1 EM CEM F1 EM CEM F1\nLlama-3.1-8B\nReAct 30.70 38.40 41.09 28.90 38.00 35.03 32.00 36.80 41.35 30.57 37.73 39.16Search-o1 35.30 43.80 47.33 34.00 45.80 41.29 44.80 48.80 52.50 38.03 46.13 47.04R1-Searcher 44.90 50.40 56.88 48.70 51.30 54.24 38.40 40.80 53.21 44.00 47.50 54.78\nRe2Search++46.50 57.80 60.19 48.90 60.50 56.85 55.20 63.20 66.37 50.20 60.50 61.14\nQwen-2.5-7B\nReAct 36.00 40.10 45.84 38.60 44.50 45.02 35.20 38.40 44.94 36.60 41.00 45.27Search-o1 40.70 46.60 52.15 38.90 46.20 45.79 40.80 44.80 52.91 40.17 45.87 50.28Search-R1 44.90 49.40 57.30 43.90 47.80 50.07 40.80 41.60 51.69 43.20 46.27 53.02R1-Searcher 46.80 53.70 59.61 48.80 55.00 55.36 44.80 48.00 54.01 46.80 52.23 56.33\nRe2Search++44.40 50.30 56.47 47.00 56.50 54.35 52.94 56.30 63.51 48.11 54.37 58.11\n4 Analysis and Discussion\n4.1 Comparison of Different Reward Sources\nAs discussed in Section 2, the process reward can be collected from different sources. This section\nfocuses on the evaluation of the effectiveness of these sources in guiding the agent\u2019s action selection\ntoward correct answers, as well as their alignment with human preferences, which are often considered\nto have the highest quality for process annotation [ 98]. Specifically, we compare the GPT-4o\nannotations with Llama-3.1-8B, as well as the rollout-based annotations using Math-Shepherd [77].\nWe collect process annotations from human experts on MedQA to examine the alignment between\nthe trained reward models and human preferences.\nTable 4: Comparison of various reward sources. ORM/PRM denotes the outcome/process reward\nmodel. Outcome sources are labeled for PRMs due to the trajectory filtering in RAG-Gym.\nType Outcome\nSource\nProcess\nSource\nHotpotQA\n(EM / F1)\n2Wiki\n(EM / F1)\nBamboogle\n(EM / F1)\nMedQA\n(Acc / Agree)\nORM Truth \u2013 41.10 / 53.35 47.70 / 55.59 43.20 / 57.46 66.77 / \u2013\nPRM (Random) \u2013 \u2013 32.20 / 42.83 35.70 / 42.00 38.40 / 47.86 68.26 / 50.00\nPRM (Rollout) Truth Rollout 39.60 / 51.85 42.94 / 49.57 48.80 / 56.05 68.34 / 71.03\nPRM (Llama) Truth Llama-3.1-8B 40.30 / 51.74 40.70 / 48.22 44.80 / 54.36 68.50 / 65.99\nPRM (GPT) Truth GPT-4o 44.10 / 56.84 50.20 / 57.94 51.20 / 63.15 71.96 / 85.85\n7\n\nThe results are shown in Table 4. The reward model trained with GPT-4o annotations delivers the\nhighest performance across all datasets, effectively providing accurate, fine-grained process rewards\nfor agent optimization. Moreover, it exhibits the best alignment with human preferences, achieving\nan agreement rate of 85.85% with human annotators. In contrast, although rollouts and Llama-3.1-8B\nannotations improve action selection relative to a process reward model with random selections, they\nare generally less effective than GPT-4o annotations and sometimes even bring inferior outcomes on\ngeneral-domain questions. This result underscores the limitations of current rollout-based methods,\noriginally designed for math reasoning, in the context of complex reasoning and search tasks, and\nhighlights the need for tailored approaches in agentic RAG.\n4.2 Training Time Scaling\nFor the evaluation of training sample size and its impacts on the performance of Re2Search agents,\nwe conducted experiments using critics trained on varying numbers of instances, ranging from 250 to\n1000 questions. The results, presented in Figure 3, show how the agent\u2019s performance scales with\nthe availability of more training data across four datasets. In general, the performance of Re2Search\nimproves with an increasing number of training samples, but the gains tend to converge as the sample\nsize grows. Notably, there is a sharp improvement in F1 scores on HotpotQA, 2WikiMultihopQA,\nand Bamboogle when comparing the ZSL baseline to process reward models trained on 250 samples,\nshowing that even a small amount of process reward data can yield significant performance gains.\nHowever, the improvements become less pronounced on HotpotQA and 2WikiMultihopQA when\nincreasing the training samples from 500 to 1000, indicating diminishing returns as the model\napproaches a saturation point in its learning from additional data.\n250 500 750 1000\n#Sample\n43\n50\n57F1 (%)\nZSL\nHotpotQA\n250 500 750 1000\n#Sample\n47\n53\n59F1 (%)\nZSL\n2WikiMultihopQA\n250 500 750 1000\n#Sample\n57\n61\n65F1 (%)\nZSL\nBamboogle\n250 500 750 1000\n#Sample\n68\n70\n72Acc (%)\nZSL\nMedQA\nFigure 3: Performance of Re2Search agents with critics trained on different numbers of samples.\nFor MedQA, which involves complex reasoning and information-seeking tasks requiring domain-\nspecific knowledge, a different trend is observed. With only 250 training samples, the performance\nslightly drops below the ZSL baseline, highlighting the challenges of capturing intricate domain-\nspecific processes with limited training data. As the sample size increases, however, the performance\ngradually recovers and eventually surpasses the ZSL baseline, achieving the highest accuracy of\n71.72% with 1000 samples. This underscores the importance of sufficient training data in capturing\nthe nuanced reasoning and query-generation processes required for specialized tasks.\n4.3 Inference Time Scaling\nSince trained critics optimize action-taking by identifying high-quality actions from the generated\ncandidates during inference, we explored how the agent performance changes with the increasing\nnumber of sampled actions at each time step. Figure 4 displays the results of our inference time\nscaling study, with Re2Search as the tested agent. We observe a consistent trend across multiple\nbenchmarks, where increasing the number of sampled actions generally improves performance.\nSpecifically, for HotpotQA and Bamboogle, the F1 score continues to rise as more actions are\nsampled, demonstrating the benefits of expanding the candidate set to enable better action selection at\neach step. However, performance gains gradually diminish, indicating that the agent reaches a point\nwhere additional sampled actions contribute less to improvement. This suggests that while action\nsampling is beneficial, there is a limit to how much additional sampling enhances decision-making.\n8\n\n5 10 15 20\n#Action\n43\n53\n63F1 (%)\nZSL\nHotpotQA\n5 10 15 20\n#Action\n47\n53\n59F1 (%)\nZSL\n2WikiMultihopQA\n5 10 15 20\n#Action\n56\n61\n66F1 (%)\nZSL\nBamboogle\n5 10 15 20\n#Action\n70\n72\n74Acc (%)\nZSL\nMedQA\nFigure 4: Performance of Re2Search agents with different numbers of actions sampled per step.\n5 Related Work\n5.1 Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large\nlanguage models (LLMs) on knowledge-intensive tasks. A typical RAG framework comprises two\ncore components: a retriever, which selects relevant documents from a large corpus, and a generator,\nwhich synthesizes information from these documents to produce coherent and contextually appropriate\nresponses [42, 18, 9]. RAG has demonstrated strong performance across diverse domains, including\nopen-domain question answering [ 37, 25, 7, 88, 63], fact-checking [ 78, 66], and summarization\n[3]. Subsequent research has focused on improving both the retriever\u2019s ability to select relevant\ndocuments [95, 51, 89, 30, 32] and the generator\u2019s capacity to effectively utilize retrieved information\n[15, 82, 80], thereby boosting overall system performance [26, 96, 36, 94]. Nevertheless, most RAG\npipelines still rely on a single retrieval step, which can be inadequate for complex queries that require\nsynthesizing information from multiple sources.\n5.2 Multi-hop Question Answering\nMulti-hop question answering (QA) tasks require systems to synthesize information from multiple,\ndiverse sources to produce accurate answers [ 90, 21]. These tasks highlight the limitations of\nconventional RAG architectures, where a single retrieval step often fails to capture the comprehensive\ncontext needed for complex queries. To address this, language agents such as ReAct [91, 73, 4, 31, 54]\nhave been proposed, interleaving reasoning and retrieval to dynamically accumulate relevant evidence\n[57, 79, 38, 61], which has shown promise in improving LLM performance [ 76, 64, 92, 27, 29].\nHowever, most of these methods still rely heavily on prompt engineering, which can be fragile and\nmay not effectively optimize language agents for knowledge-intensive tasks [ 40, 70, 2]. Recent\nstudies have explored reinforcement learning (RL) to optimize language agents for multi-hop QA\n[69, 33, 8, 17, 55], but these approaches risk generating suboptimal intermediate search actions and\nshow limited generalization to unseen data, as demonstrated in our experiments. Other concurrent\nwork investigates process-level supervision [22, 12, 45, 71], but typically focuses on specific agent\narchitectures and a narrow set of supervision methods, offering limited insight into the systematic\noptimization of language agents.\n5.3 Post-training of Large Language Models\nBeyond the foundational knowledge acquired during pre-training, post-training methods are essential\nfor refining Large Language Models (LLMs) and aligning them with specific downstream tasks and\ndesired behaviors. Supervised Fine-Tuning (SFT) adapts models using curated instruction-response\npairs to promote task-specific capabilities [ 52, 10]. While SFT enhances instruction-following,\nfurther alignment with nuanced human preferences is often achieved through Reinforcement Learning\nfrom Human Feedback (RLHF) [52, 6, 5], typically implemented via Proximal Policy Optimization\n(PPO) [60]. More recently, critic-free approaches such as Direct Preference Optimization (DPO)\nhave emerged as streamlined alternatives [56, 50, 14, 87], directly optimizing the LLM policy based\non reward annotations and bypassing the need for a separately trained reward model. Although\nthese techniques bring strong gains on text generation benchmarks, their integration into agentic\nRAG pipelines, where models must dynamically interact with retrieval systems and adapt reasoning\nstrategies to evolving contexts, remains underexplored.\n9\n\n6 Conclusion\nThis work presents RAG-Gym as a unified and extensible framework for systematically optimizing\nagentic RAG along the axes of prompt engineering, actor tuning, and critic training. Through\nextensive empirical analysis, we demonstrate that integrating reasoning reflection, process-level direct\npreference optimization, and critic-guided inference yields substantial improvements over existing\napproaches. We hope RAG-Gym will serve as a foundation for further advances in robust, adaptive,\nand interpretable retrieval-augmented language agents.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[2] Victor Akinwande, Yiding Jiang, Dylan Sam, and J Zico Kolter. Understanding prompt\nengineering may not require rethinking generalization. arXiv preprint arXiv:2310.03957, 2023.\n[3] Chenxin An, Ming Zhong, Zhichao Geng, Jianqiang Yang, and Xipeng Qiu. Retrievalsum: A\nretrieval enhanced framework for abstractive summarization. arXiv preprint arXiv:2109.07943,\n2021.\n[4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learn-\ning to retrieve, generate, and critique through self-reflection. In The Twelfth International\nConference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\nReview.net, 2024. URL https://openreview.net/forum?id=hSyW5go0v8.\n[5] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a\nlaboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n[6] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022.\n[7] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\nMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\net al. Improving language models by retrieving from trillions of tokens. In International\nconference on machine learning, pages 2206\u20132240. PMLR, 2022.\n[8] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z\nPan, Wen Zhang, Huajun Chen, Fan Yang, et al. Research: Learning to reason with search for\nllms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025.\n[9] Mingyue Cheng, Yucong Luo, Jie Ouyang, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang,\nJiawei Cao, Jie Ma, et al. A survey on knowledge-oriented retrieval-augmented generation.\narXiv preprint arXiv:2503.10677, 2025.\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. Journal of Machine Learning Research, 25(70):1\u201353, 2024.\n[11] Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. Reciprocal rank fusion outper-\nforms condorcet and individual rank learning methods. In Proceedings of the 32nd international\nACM SIGIR conference on Research and development in information retrieval, pages 758\u2013759,\n2009.\n[12] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen.\nProgressive multimodal reasoning via active retrieval. arXiv preprint arXiv:2412.14835, 2024.\n10\n\n[13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024.\n[14] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:\nModel alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.\n[15] Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. Enhancing\nnoise robustness of retrieval-augmented language models with adaptive adversarial training.\narXiv preprint arXiv:2405.20978, 2024.\n[16] Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, and Yanghua Xiao. Reward\nshaping to mitigate reward hacking in rlhf. arXiv preprint arXiv:2502.18770, 2025.\n[17] Jingsheng Gao, Linxu Li, Weiyuan Li, Yuzhuo Fu, and Bin Dai. Smartrag: Jointly learn\nrag-related tasks from the environment feedback. arXiv preprint arXiv:2410.18141, 2024.\n[18] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\nand Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv\npreprint arXiv:2312.10997, 2023.\n[19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n[20] Binglan Han, Teo Susnjak, and Anuradha Mathrani. Automating systematic literature reviews\nwith retrieval-augmented generation: A comprehensive overview. Applied Sciences, 14(19):\n9103, 2024.\n[21] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the\n28th International Conference on Computational Linguistics, pages 6609\u20136625, 2020.\n[22] Sheryl Hsu, Omar Khattab, Chelsea Finn, and Archit Sharma. Grounding by trying: Llms with\nreinforcement learning-enhanced retrieval. arXiv preprint arXiv:2410.23214, 2024.\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[24] Jian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. Openrlhf: An\neasy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143,\n2024.\n[25] Gautier Izacard and \u00c9douard Grave. Leveraging passage retrieval with generative models for\nopen domain question answering. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume, pages 874\u2013880, 2021.\n[26] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning\nwith retrieval augmented language models. Journal of Machine Learning Research, 24(251):\n1\u201343, 2023.\n[27] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong-Cheol Park. Adaptive-\nrag: Learning to adapt retrieval-augmented large language models through question complexity.\nIn 2024 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 7036\u20137050. Association for Computational\nLinguistics, 2024.\n[28] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.\nACM Computing Surveys, 55(12):1\u201338, 2023.\n11\n\n[29] Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, and Jiawei\nHan. Ras: Retrieval-and-structuring for knowledge-intensive llm generation. arXiv preprint\narXiv:2502.10996, 2025.\n[30] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng\nSun, and Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large\nlanguage models via reinforcement learning. arXiv preprint arXiv:2503.00223, 2025.\n[31] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969\u20137992,\n2023.\n[32] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented\ngeneration with long-context llms. arXiv preprint arXiv:2406.15319, 2024.\n[33] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Za-\nmani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with\nreinforcement learning. arXiv preprint arXiv:2503.09516, 2025.\n[34] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\ndisease does this patient have? a large-scale open domain question answering dataset from\nmedical exams. Applied Sciences, 11(14):6421, 2021.\n[35] Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John Wilbur, and\nZhiyong Lu. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs\nfor zero-shot biomedical information retrieval. Bioinformatics, 39(11):btad651, 2023.\n[36] Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, and Jun\nZhao. Rag-rewardbench: Benchmarking reward models in retrieval augmented generation for\npreference alignment. arXiv preprint arXiv:2412.13746, 2024.\n[37] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 6769\u20136781, 2020.\n[38] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and\nAshish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks.\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?\nid=_nGgzQjzaRy.\n[39] Jakub L\u00e1la, Odhran O\u2019Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G Rodriques, and\nAndrew D White. Paperqa: Retrieval-augmented generative agent for scientific research. arXiv\npreprint arXiv:2312.07559, 2023.\n[40] Divya Lamba. The role of prompt engineering in improving language understanding and\ngeneration. International Journal For Multidisciplinary Research, 2024. URL https://api.\nsemanticscholar.org/CorpusID:274939741.\n[41] Guido Lang and Tan G\u00fcrpinar. Ai-powered learning support: A study of retrieval-augmented\ngeneration (rag) chatbot effectiveness in an online course. Information Systems Education\nJournal, 23(2), 2025.\n[42] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. Advances in Neural Information Processing\nSystems, 33:9459\u20139474, 2020.\n[43] Dongheng Li, Yongchang Hao, and Lili Mou. Llmr: Knowledge distillation with a large\nlanguage model-induced reward. In Proceedings of the 2024 Joint International Conference on\nComputational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages\n10657\u201310664, 2024.\n12\n\n[44] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and\nZhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint\narXiv:2501.05366, 2025.\n[45] Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. Can we\nfurther elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving\nchallenging tasks. arXiv preprint arXiv:2410.01428, 2024.\n[46] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy\nLee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step.\nIn The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?\nid=v8L0pN6EOi.\n[47] Fei Liu et al. Learning to summarize from human feedback. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 583\u2013592, 2020.\n[48] Siru Liu, Allison B McCoy, and Adam Wright. Improving large language model applications\nin biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and\nclinical development guidelines. Journal of the American Medical Informatics Association ,\npage ocaf008, 2025.\n[49] Hao Ma, Tianyi Hu, Zhiqiang Pu, Liu Boyin, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolv-\ning with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement\nlearning. Advances in Neural Information Processing Systems, 37:15497\u201315525, 2024.\n[50] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a\nreference-free reward. Advances in Neural Information Processing Systems, 37:124198\u2013124235,\n2024.\n[51] Thang Nguyen, Peter Chin, and Yu-Wing Tai. Reward-rag: Enhancing rag with reward driven\nsupervision. arXiv preprint arXiv:2410.03780, 2024.\n[52] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in neural information processing systems,\n35:27730\u201327744, 2022.\n[53] Nicholas Pipitone and Ghita Houir Alami. Legalbench-rag: A benchmark for retrieval-\naugmented generation in the legal domain. arXiv preprint arXiv:2408.10343, 2024.\n[54] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\nMeasuring and narrowing the compositionality gap in language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pages 5687\u20135711, 2023.\n[55] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T\u00fcr, Gokhan\nTur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958,\n2025.\n[56] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\nAdvances in Neural Information Processing Systems, 36:53728\u201353741, 2023.\n[57] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown,\nand Yoav Shoham. In-context retrieval-augmented language models. Transactions of the\nAssociation for Computational Linguistics, 11:1316\u20131331, 2023.\n[58] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and\nbeyond. Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\n[59] Satya S Sahoo, Joseph M Plasek, Hua Xu, \u00d6zlem Uzuner, Trevor Cohen, Meliha Yetisgen,\nHongfang Liu, St\u00e9phane Meystre, and Yanshan Wang. Large language models for biomedicine:\nfoundations, opportunities, challenges, and best practices. Journal of the American Medical\nInformatics Association, page ocae074, 2024.\n13\n\n[60] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[61] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.\nEnhancing retrieval-augmented large language models with iterative retrieval-generation synergy.\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248\u20139274,\n2023.\n[62] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n[63] Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, and Ninghao\nLiu. Searchrag: Can search engines be helpful for llm-based medical question answering?\narXiv preprint arXiv:2502.13233, 2025.\n[64] Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun\nRen. Generate-then-ground in retrieval-augmented generation for multi-hop question answering.\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 7339\u20137353, 2024.\n[65] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\nReflexion: Language agents with verbal reinforcement learning.Advances in Neural Information\nProcessing Systems, 36, 2024.\n[66] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmenta-\ntion reduces hallucination in conversation. In Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 3784\u20133803, 2021.\n[67] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and\ncharacterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460\u2013\n9471, 2022.\n[68] Michael D Skarlinski, Sam Cox, Jon M Laurent, James D Braza, Michaela Hinks, Michael J\nHammerling, Manvitha Ponnapati, Samuel G Rodriques, and Andrew D White. Language\nagents achieve superhuman synthesis of scientific knowledge. arXiv preprint arXiv:2409.13740,\n2024.\n[69] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang,\nand Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2503.05592, 2025.\n[70] Hari Subramonyam, Divy Thakkar, Andrew Ku, Juergen Dieber, and Anoop K Sinha. Prototyp-\ning with prompts: Emerging approaches and challenges in generative ai design for collaborative\nsoftware teams. In Proceedings of the 2025 CHI Conference on Human Factors in Computing\nSystems, pages 1\u201322, 2025.\n[71] Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang,\nSong Yang, and Han Li. Rearter: Retrieval-augmented reasoning with trustworthy process\nrewarding. arXiv preprint arXiv:2501.07861, 2025.\n[72] Jakub Swacha and Micha\u0142 Gracel. Retrieval-augmented generation (rag) chatbots for education:\nA survey of applications. Applied Sciences, 15(8):4234, 2025.\n[73] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving\nretrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 10014\u201310037, 2023.\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n14\n\n[75] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan\nLambert, Shengyi Huang, Kashif Rasul, and Quentin Gallou\u00e9dec. Trl: Transformer reinforce-\nment learning. https://github.com/huggingface/trl, 2020.\n[76] Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge\nRong, and Zhang Xiong. Knowledge-driven cot: Exploring faithful reasoning in llms for\nknowledge-intensive question answering. arXiv preprint arXiv:2308.13259, 2023.\n[77] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and\nZhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations.\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 9426\u20139439, 2024.\n[78] Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim\nDas, and Preslav Nakov. Factuality of large language models: A survey. In Proceedings of the\n2024 Conference on Empirical Methods in Natural Language Processing, pages 19519\u201319529,\n2024.\n[79] Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. Rat: Retrieval\naugmented thoughts elicit context-aware reasoning in long-horizon generation. arXiv preprint\narXiv:2403.05313, 2024.\n[80] Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot,\nYuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing\nretrieval augmented generation through drafting. arXiv preprint arXiv:2407.08223, 2024.\n[81] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems, 35:24824\u201324837, 2022.\n[82] Zhepei Wei, Wei-Lin Chen, and Yu Meng. InstructRAG: Instructing retrieval-augmented\ngeneration via self-synthesized rationales. In The Thirteenth International Conference on\nLearning Representations, 2025. URL https://openreview.net/forum?id=P1qhkp8gQT.\n[83] Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie,\nIkechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, and Bruno Fleisch. Cbr-rag: case-\nbased reasoning for retrieval augmented generation in llms for legal question answering. In\nInternational Conference on Case-Based Reasoning, pages 445\u2013460. Springer, 2024.\n[84] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources\nto advance general chinese embedding, 2023.\n[85] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented\ngeneration for medicine. In Findings of the Association for Computational Linguistics ACL\n2024, pages 6233\u20136251, 2024.\n[86] Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, and Aidong Zhang. Im-\nproving retrieval-augmented generation in medicine with iterative follow-up questions. In\nBiocomputing 2025: Proceedings of the Pacific Symposium, pages 199\u2013214. World Scientific,\n2024.\n[87] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Ken-\nton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries\nof llm performance in machine translation. In International Conference on Machine Learning,\npages 55204\u201355224. PMLR, 2024.\n[88] Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang\nLi, Joyce C Ho, Carl Yang, et al. Simrag: Self-improving retrieval-augmented generation for\nadapting large language models to specialized domains. arXiv preprint arXiv:2410.17952,\n2024.\n15\n\n[89] Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May Dongmei Wang, Joyce C.\nHo, Chao Zhang, and Carl Yang. BMRetriever: Tuning large language models as better\nbiomedical text retrievers. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors,\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,\npages 22234\u201322254, Miami, Florida, USA, November 2024. Association for Computational\nLinguistics. doi: 10.18653/v1/2024.emnlp-main.1241. URL https://aclanthology.org/\n2024.emnlp-main.1241/.\n[90] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors,\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npages 2369\u20132380, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\n[91] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. In International Conference\non Learning Representations (ICLR), 2023.\n[92] Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation\nfor large language models. 2024. URL https://arxiv.org/abs/2411.19443.\n[93] Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boull\u00e9,\nand Ievgen Redko. Large language models as markov chains. arXiv preprint arXiv:2410.02724,\n2024.\n[94] Hanning Zhang, Juntong Song, Juno Zhu, Yuanhao Wu, Tong Zhang, and Cheng Niu. Rag-\nreward: Optimizing rag with reward modeling and rlhf. arXiv preprint arXiv:2501.13264,\n2025.\n[95] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to\naugment large language models. arXiv preprint arXiv:2310.07554, 2023.\n[96] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and\nJoseph E Gonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint\narXiv:2403.10131, 2024.\n[97] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo\nZhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: a survey on hallucination in\nlarge language models. arXiv preprint arXiv:2309.01219, 2023.\n[98] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng\nLiu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in\nmathematical reasoning. arXiv preprint arXiv:2501.07301, 2025.\n16\n\nA Limitations and Future Work\nDespite the strengths of RAG-Gym, several limitations remain. First, our framework relies on\nhigh-quality process reward judgments to supervise intermediate agent actions. Obtaining such\nfine-grained annotations for complex reasoning or domain-specific scenarios can be challenging.\nSecond, as with other reward modeling approaches, there is an inherent risk of reward hacking: agents\nmay learn to exploit imperfections or biases in the reward model, optimizing for the reward signal\nrather than genuine task performance [67, 16]. Third, while our experiments focus on knowledge-\nintensive question answering, the generalizability of RAG-Gym to other task types (e.g., dialogue,\nsummarization, or planning) remains to be systematically evaluated.\nWhile RAG-Gym serves as a pilot study of when and how process supervision works for agentic\nRAG, several promising directions remain for future work. First, developing more scalable and\ncost-effective annotation strategies for process reward modeling is essential, especially for complex or\nspecialized domains. Since existing rollout-based methods such as Math-Shepherd [77] did not yield\nsignificant gains in our experiments (Table 4), new approaches are needed to facilitate efficient and\nhigh-quality process reward collection. Second, the design and training of process reward judges can\nbe further refined to improve robustness and reduce susceptibility to reward hacking. Third, extending\nRAG-Gym to a broader range of agentic tasks beyond knowledge-intensive question answering such\nas dialogue will help assess its generalizability and reveal new challenges, particularly in settings\nwhere outcome rewards are ambiguous and process supervision is even more critical.\nB Broader Impacts\nRAG-Gym systematically evalutes different optimization approaches for retrieval-augmented lan-\nguage agents, which has the potential for wide-ranging societal benefits and risks. By enabling\nhigh-quality intermediate steps with process-level supervsion, our framework can improve the relia-\nbility of AI assistants in knowledge-intensive domains such as education [72, 41], healthcare [48, 86],\nscientific research [20, 39, 68], and legal analysis [ 83, 53]. Also, process-level actor tuning and\ncritic-guided inference may help reduce hallucinations and increase transparency, supporting more\ntrustworthy AI deployments.\nHowever, these advances also raise important considerations. The reliance on high-quality process\nreward annotations may introduce biases if the annotation sources are not representative or contain\nsystematic errors. Reward hacking remains a risk, as agents may learn to exploit weaknesses in the\nreward model, potentially leading to unintended behaviors or misinformation.\nC Dataset Descriptions\nIn this section, we provide detailed descriptions of the datasets used in our experiments, including\nHotpotQA [90], 2WikiMultihopQA [21], Bamboogle [54], and MedQA [34].\nHotpotQA. HotpotQA is a large-scale, multi-hop question-answering dataset that requires rea-\nsoning across multiple documents. It consists of questions that explicitly demand retrieving and\nsynthesizing information from different sources. The dataset provides both distractor and supporting\ndocuments, allowing evaluation of models\u2019 ability to filter relevant information effectively. As the\nanswers to the test questions in HotpotQA are not publicly available we took a subsample from its\nvalidation set (7,405 instances) as previous research did [91, 44]. The last 1,000 validation questions\nwere selected for the agent evaluation on HotpotQA. The first 1,000 questions were used as the\ntraining data for process supervision.\n2WikiMultihopQA. 2WikiMultihopQA is another multi-hop question-answering dataset con-\nstructed from Wikipedia. 2WikiMultihopQA focuses on high-quality reasoning paths by selecting\nsupporting documents more systematically. The dataset contains questions that require reasoning\nacross different Wikipedia pages, ensuring a diverse range of factual and inferential challenges. The\nlast 1000 questions in the development set (12,576 question in total) were used for agent evaluation.\nBamboogle. Bamboogle is a manually constructed dataset designed to evaluate compositional\nreasoning and adversarial robustness. It consists of 2-hop questions written by researchers, where\n17\n\nboth supporting facts exist in Wikipedia but are structured to be challenging for retrieval-based\nsystems. Unlike automatically generated datasets like 2WikiMultihopQA and Musique, Bamboogle\nquestions do not follow fixed templates, increasing their variability. We used the whole test set with\n125 questions for the evaluation of agents on Bamboogle.\nMedQA. MedQA is a medical question-answering dataset sourced from professional medical\nexams such as the USMLE (United States Medical Licensing Examination). It requires domain-\nspecific knowledge and reasoning to answer multiple-choice medical questions. We focused on the\nEnglish split of MedQA with 1,273 USMLE-style test questions. A subset of 1,000 questions was\nsampled from the training set (10,178 questions) for the optimization of various agents.\nD Baseline Descriptions\nHere are the detailed descriptions of various baseline agents that we implemented in the experiments.\nDirect. The Direct agent represents the simplest baseline, where the language model is prompted\nto output the predicted answer immediately, without any explicit intermediate reasoning or search\nsteps. This approach tests the model\u2019s ability to answer questions in a single step, relying solely on\nits internal knowledge and without leveraging external retrieval or multi-step reasoning.\nCoT [81]. The Chain-of-Thought (CoT) agent encourages the model to generate a step-by-step\nreasoning process before producing the final answer, but still does so in a single iteration. The agent\nis prompted to articulate its reasoning explicitly, which can help with complex questions by making\nthe model\u2019s thought process transparent and potentially improving answer accuracy. However, CoT\ndoes not incorporate external retrieval or iterative search.\nRAG [42]. The Retrieval-Augmented Generation (RAG) agent augments the language model with a\nretrieval step. At the first iteration, the agent issues the original question as a search query to retrieve\nrelevant documents. In the subsequent step, it reasons about the updated state, which includes the\nretrieved information, and generates a predicted answer. This approach leverages external knowledge\nbut does not perform multi-hop or iterative search.\nReAct [91]. The ReAct agent combines reasoning and acting by allowing the model to interleave\nnatural language reasoning with actions, such as issuing search queries or providing answers. At each\nstep, the agent reasons about the current state and decides whether to search for more information or\nto answer the question. This enables multi-step, interactive information-seeking and supports more\ncomplex reasoning chains.\nSearch-o1 [44]. The Search-o1 agent extends the ReAct framework by introducing a knowledge\nsummarization step before reasoning. For each search query, the agent reasons about the retrieved\ndocuments and briefly summarize the useful information as the direct answer to the search query,\nforming query-answer pairs that are used as input for subsequent reasoning steps. This approach\nreplaces the use of raw documents with structured summaries, potentially improving reasoning\nefficiency. Search-o1 can be viewed as a special case of ReAct where retrieval is performed via RAG\nand the agent operates on summarized knowledge rather than full documents.\nE Implementation Details\nIn our experiments, we selected Llama-3.1-8B-Instruct [13] as the base LLM for the implementations\nof various language agents, due to its context length of 128k tokens and its availability of open-source\nparameters. The critic is also trained based on the same Llama-3.1-8B-Instruct, same as the actor.\nWe involved GPT-4o-mini and Qwen-2.5-7B-Instruct to show the transferability of the trained critic\n(Figure 2) and the generalizability of RAG-Gym (Table 3) to other LLMs.\nE.1 Details of Process Data Collection\nTo evaluate intermediate reasoning and search steps in RAG-Gym, we design a process reward\nfunction that assesses queries based on three key criteria:\n18\n\n\u2022 If the retrieval history already contains sufficient information, answering should be the preferred\naction instead of searching further.\n\u2022 Queries should also be precise, actionable, and foundational to solving the question while avoiding\nunnecessary details.\n\u2022 Queries should introduce new, useful information rather than repeating past searches.\nThese criteria ensure that queries are efficient, targeted, and contribute meaningfully to constructing\nthe final answer.\nThe data collection pipeline begins with trajectory sampling, where the language agent generates a se-\nquence of actions based on its current policy. At each step in the trajectory, multiple candidate actions\nare proposed, and the best action is selected according to predefined evaluation criteria. To streamline\nthe annotation process and ensure consistency, we employ a ranking-based evaluation framework\nrather than assigning numerical scores. The selected action is then executed, and the trajectory\ntransitions to the next state. This process is repeated iteratively until the trajectory terminates.\nTo ensure quality, only sampled trajectories that result in a correct final answer are retained, as\ndetermined by the outcome reward. This filtering guarantees that the selected actions not only align\nwith the process reward criteria but also contribute to successful task completion. To address the\nchallenges of slow and costly human annotation, we leverage LLMs such as GPT-4o to annotate\nthe sampled trajectories. As demonstrated in our experiments (Table 4), annotations generated by\nGPT-4o exhibit high reliability, closely aligning with domain expert judgments. This approach\nenables scalable and efficient data collection, making it feasible to gather high-quality process reward\ndata at scale.\nFigure 5: Pipeline of the process data collection in RAG-Gym. Process reward data is collected by\nrandomly sampling action candidates at each time step and using an external annotator (e.g., GPT-4o)\nto select the best one. The episode is terminated when the agent generates a final answer.\nFor the implementation of the IR environment, we select Wikipedia as the supporting corpus for the\nretrieval of relevant information for questions from HotpotQA, 2WikiMultihopQA, and Bamboogle.\nFor the environment of solving MedQA questions, we use a combination of medical textbooks and\nStatPearls which were pre-processed in MedRAG [ 85]. For all tasks, we used both lexical and\nsemantic retrievers whose results were merged with Reciprocal Rank Fusion [11]. BM25 [58] and\nBGE-Base [84] were used for HotpotQA, 2WikiMultihopQA, and Bamboogle, while in MedQA, we\nselected BM25 and MedCPT [35]. A set of 32 documents will be retrieved for each search query.\nE.2 Details of LLM Post-training\nFor the actor tuning, we employed Low-Rank Adaptation (LoRA) [ 23] in the implementaion of\nsupervised fine-tuning (SFT) [52] and direct preference optimization (DPO) [56] [60] with r = 256\nand alpha = 512on all attention components in the transformers architecture [74]. SFT and DPO\n19\n\nwere implemented using the TRL package [75]. For proximal policy optimization (PPO), we used\nthe OpenRLHF package [24] with full-parameter tuning. Detailed hyperparameter settings for SFT,\nDPO, and PPO can be found in our source code. For the tuning of Search-o1 and Re2Search agents,\nonly the LLM for action reasoning is trained while the one for history knowledge summarization\nremains untuned.\nE.3 Details of LLM Inference\nAll results of zero-shot learning (ZSL), supervised fine-tuning (SFT), direct preference optimization\n(DPO), and proximal policy optimization (PPO) are generated with a temperature of 0.0. For the\nevaluation of agents with a critic, we employed a temperature of 1.0 with 10 different actions sampled\nfor each step in the information-seeking trajectory. Algorithm 1 presents our algorithm of using the\ntrained process reward model to guide the action selection during inference. All experiments were\nconducted on NVIDIA A100 and A6000 GPUs.\nAlgorithm 1 PRM-Guided Inference with Best-of-N Selection\n1. Input: Original question Q, actor \u03c0\u03b8, critic r\u03d5, number of candidate actions N, maximum steps\nT, information retrieval function IR.\n2. Initialize state S \u2190 (Q, H1 = \u2205).\n3. For t = 1to T:\n(a) Generate N candidate actions: aq, \u00b7\u00b7\u00b7 , aN \u223c \u03c0f(\u03b8)(\u00b7|S).\n(b) Compute process rewards and select the best action: a\u2217 \u2190 arg maxa\u2208{a1,\u00b7\u00b7\u00b7,aN} r\u03d5(S, a).\n(c) If a\u2217 is a search query:\ni. Retrieve documents: D \u2190 IR(a\u2217).\nii. Update state: S \u2190 (Q, Ht+1 = Ht \u222a {(a\u2217, D)}).\n(d) If a\u2217 is a final answer:\ni. Return a\u2217 and terminate the process.\n4. End For\nF Study on the Number of Search Queries\nIn addition to the results presented in Table 2, we further analyzed the number of search queries\ngenerated by Re2Search agents across different datasets. Table 5 reports the minimum, maximum,\nand mean number of search queries issued. The maximum value is capped at 10, reflecting the upper\nlimit of iterations allowed per question in our experiments. The results show that tuned agents (SFT,\nDPO, and PPO) consistently generate more search queries than the zero-shot agent (ZSL), indicating\nthat fine-tuning encourages more extensive information-seeking behavior, which aligns with their\nimproved performance.\nTable 5: Minimum, maximum, and mean number of search queries generated by Re2Search agents\nfor each dataset.\nHotpotQA 2Wiki Bamboogle MedQA\nMin Max Mean Min Max Mean Min Max Mean Min Max Mean\nZSL 0.0 9.0 1.5 0.0 9.0 3.4 0.0 9.0 1.0 0.0 9.0 0.4\nSFT 0.0 9.0 2.1 0.0 9.0 3.8 0.0 9.0 1.9 0.0 9.0 0.6\nDPO 0.0 9.0 3.2 0.0 9.0 4.5 0.0 9.0 3.4 0.0 9.0 2.2\nPPO 0.0 9.0 4.6 0.0 9.0 5.6 0.0 9.0 2.7 0.0 9.0 5.6\n20\n\nG Case Studies\nG.1 Comparison of Agent Designs on Bamboogle\nWe analyze the reasoning and search behaviors of RAG, ReAct, Search-o1, and Re2Search using an\nexample from the Bamboogle dataset. As shown in Figure 7, given the question \u201cWhat was the father\nof the last surviving Canadian father of Confederation?\", the three agents show distinct behaviors\nwhen generating the first action.\nThe RAG agent directly passes the question as a search query without decomposition, relying entirely\non retrieval to infer the answer. This often leads to ineffective searches that fail to retrieve necessary\nintermediate facts. ReAct and Search-o1 improve upon this by engaging in stepwise query reasoning,\nfirst identifying the need to determine the last surviving Canadian father of Confederation before\nissuing a search query. However, the generated query, \u201cList of Canadian fathers of Confederation\u201d,\nretrieves broad information rather than directly resolving the missing knowledge.\nIn contrast, Re2Search explicitly integrates answer reasoning with search. It first constructs a potential\nanswer, identifying an unverified claim that William Lyon Mackenzie King is among the last surviving\nCanadian fathers of Confederation. Recognizing the missing evidence, it formulates a targeted query,\n\u201cWho is the last surviving Canadian father of Confederation?\u201d, to resolve the uncertainty. This\napproach ensures that retrieval is aligned with answer construction, minimizing unnecessary queries\nand improving information efficiency. The case study illustrates how Re2Search effectively refines\nthe search process by linking query generation to specific knowledge gaps.\nanswer reasoning  & reflection\n(a) RAG (b) ReAct / Search-o1 (c) ReSearch\n1.  Need to identify the\nlast surviving Canadian\nfather of Confederation.\n2.  Start by searching for\nthe list of Canadian\nfathers of Confederation.\nQuery: List of Canadian\nfathers of Confederation\n1. William Lyon\nMackenzie King is among\nthe last Canadian father\nof Confederation\n2. Mackenzie King's\nfather was James\nMackenzie\nQuery: Who is the last\nsurviving Canadian father\nof Confederation?\nQuestion: What was the father of\nthe last surviving Canadian\nfather of Confederation?\nQuestion: What was the father of\nthe last surviving Canadian\nfather of Confederation?\nQuestion: What was the father of\nthe last surviving Canadian\nfather of Confederation?\nQuery: What was the\nfather of the last surviving\nCanadian father of\nConfederation?\nquery reasoning\nstep 1\nstep 2\nstep 1\nstep 2\nquery generation query generation\ndirect pass\nFigure 6: Comparison of different agent architectures in handling a multi-hop question from Bam-\nboogle.\nG.2 Comparison of Agent Designs on MedQA\nSimilarly, when presented with a complex medical question from MedQA, the distinct approaches of\nthe agents are evident. The RAG agent, as before, directly uses a truncated version of the lengthy\ninput as its search query, which is unlikely to yield specific, actionable information. ReAct and\nSearch-o1 engage in query reasoning, first hypothesizing that these symptoms suggest a possible\ndiagnosis of serotonin syndrome and then deciding to search for information on the treatment of\nserotonin syndrome. While this is more targeted than RAG, Re2Search demonstrates a more refined\nprocess by engaging in answer reasoning and reasoning reflection. It posits that the symptoms are\nsuggestive of a cholinergic syndrome. Recognizing the need to confirm the relationship between the\npatient\u2019s existing conditions and the suspected syndrome, it generates a highly specific query about\nthe relationship between constipation, fibromyalgia, and cholinergic syndrome. This demonstrates\nRe2Search\u2019s ability to align its search strategy with the nuances of constructing a well-supported\nanswer, thereby improving the precision of its information retrieval in a complex diagnostic scenario.\n21\n\n(a) RAG (b) ReAct / Search-o1\nthese symptoms ... suggest\na possible diagnosis of\nserotonin syndrome.\nQuery: What are the appropriate\ntreatments for serotonin\nsyndrome?\nThese symptoms ... are\nsuggestive of a cholinergic\nsyndrome\nQuery: What is the relationship\nbetween constipation,\nfibromyalgia, and cholinergic\nsyndrome?\nQuestion: A 25-year-old man with a past medical history of constipation and fibromyalgia ... generalized\nmalaise and severe diarrhea ... rhinorrhea, lacrimation, and piloerection ... pupils are dilated ... scars are\nnoted in the antecubital fossa ... Which could be an appropriate treatment for this patient's symptoms?\nQuery: A 25-year-old man ...\nquery reasoning\nstep 1\nstep 2\nanswer reasoning  & reflection\nstep 1\nstep 2\nquery generation query generation\ndirect pass\nwe need to search for\ninformation on the\ntreatment of serotonin\nsyndrome\nthe most appropriate\ntreatment ... an\nanticholinergic medication\n(c) ReSearch\nFigure 7: Comparison of different agent architectures in handling a multi-hop question from MedQA.\nG.3 Critic-Guided Action Selection in MedQA\nTo illustrate how the trained critic improves decision-making, we examine a case from the MedQA\ndataset (Table 6). The model is tasked with identifying the mechanism of action of the most likely\nanti-diabetic medication for a 60-year-old patient presenting with symptoms indicative of fluid\noverload.\nFor the generation of the first action, the agent initially generates a broad and less actionable query\nabout heart failure, which, while relevant, does not directly contribute to identifying the correct\nmedication. Meanwhile, it also provides another candidate action with the query \u201cWhat are common\nclasses of oral anti-diabetic medications?\", which leads to retrieving a structured list of relevant drug\ncategories. The process reward model estimates the quality of these two candidates and identifies the\nsecond candidate as the better action.\nAs the reasoning progresses, the process reward model continues to refine action selection, identifying\nthe best queries to resolve missing information efficiently. The rewarded queries ultimately guide\nthe agent toward correctly inferring that the medication is most likely to be Biguanides (metformin),\nwhich acts by increasing gene expression of GLUT-4. This case demonstrates how process reward\nmodels enhance decision quality by selecting queries that effectively bridge knowledge gaps, leading\nto more precise reasoning and higher answer accuracy.\nH Prompt Templates\nWe provide structured prompt templates for history knowledge summarization and action generation\nin our proposed Re2Search agent. The template in Figure 8 ensures that retrieved documents are\nsummarized concisely and factually for follow-up queries. Figure 9 shows the prompt template used\nby Re2Search to align the answer construction with query formulation. The prompt of using GPT-4o\nfor process reward data annotation is presented in Figure 10.\n22\n\nTable 6: An example of how PRM helps identify high-quality actions that lead to correct answers.\nThe unverified claims in the presented example are highlighted.\nMedQA Question\nQuestion: A 60-year-old woman presents to a physician for worsening shortness of breath and increasing\nabdominal distention over the last 3 months. She says that the shortness of breath is worse on exertion and\nimproves with rest. While she could previously walk to the nearby store for her groceries, she now has to\ndrive because she gets \u201dwinded\u201d on the way. The patient was diagnosed with diabetes 5 years ago and is\ncompliant with her medications. The medical history is otherwise unremarkable. The physical examination\nreveals gross ascites and visibly engorged periumbilical veins. Bilateral pitting edema is noted around\nthe ankles. The finger-prick blood glucose level is 100 mg/dL. What is the mechanism of action of the\nanti-diabetic medication this patient is most likely taking? A. Binding to the alpha subunit of the insulin\nreceptor, B. Closure of ATP-sensitive K-channels in the pancreatic beta-cell,C. Increased gene expression of\nGLUT-4, D. Glucagon-like peptide-1 receptor agonist\nAnswer (invisible): C\nAction 1\n[Candidate 1 \u2013 Reasoning] ...\n[Candidate 1 \u2013 Action]\nQuery: What is the primary concern with the patient\u2019s condition, specifically whether heart failure is\nconfirmed based on symptoms and physical examination findings? /thumbs-down\n[Candidate 2 \u2013 Reasoning]\n1. The patient presents with worsening shortness of breath, increasing abdominal distention, and gross\nascites. These symptoms, along with visibly engorged periumbilical veins and bilateral pitting edema, are\nindicative of fluid overload and congestive heart failure.\n2. The patient\u2019s history of diabetes, which is compliant with medication, suggests that her anti-diabetic\nmedication is likely a type of oral hypoglycemic agent rather than insulin.\n3. Considering the patient\u2019s age (60 years) and symptoms, it is unlikely that she is taking a glucagon-like\npeptide-1 (GLP-1) receptor agonist, which is typically used in younger patients with type 2 diabetes and has\na different side-effect profile.\n4. With the remaining options, \u2019Binding to the alpha subunit of the insulin receptor\u2019 and \u2019Closure of\nATP-sensitive K-channels in the pancreatic beta-cell\u2019, we can narrow down the possibilities. \u2019Binding to the\nalpha subunit of the insulin receptor\u2019 is more characteristic of insulin receptor agonists or insulin itself, which\nis unlikely given the patient\u2019s age and the oral nature of her medication. Therefore,\u2019Closure of ATP-sensitive\nK-channels in the pancreatic beta-cell\u2019 is the most plausible mechanism of action.\n5. This mechanism is consistent with the sulfonylureas class of medications, which are commonly used in\npatients with type 2 diabetes.\n[Candidate 2 \u2013 Action]\nQuery: What are common classes of oral anti-diabetic medications? /thumbs-up\nEnvironment Feedback 1\n[+ Documents]\nCommon classes of oral anti-diabetic medications include: 1. Sulfonylureas, 2. Dipeptidyl Peptidase IV\n(DPP IV) inhibitors (gliptins), 3. Thiazolidinediones, 4. Meglitinides, 5. Sodium-glucose co-transporter 2\n(SGLT 2) inhibitors, 6. Alpha-glucosidase inhibitors, 7. Biguanides (metformin).\nAction 2\n[Candidate 1 \u2013 Reasoning] ...\n[Candidate 1 \u2013 Action]\nQuery: Is metformin commonly prescribed for type 2 diabetes and known to cause lactic acidosis as a side\neffect? /thumbs-down\n[Candidate 2 \u2013 Reasoning] ...\n[Candidate 2 \u2013 Action]\nQuery: What is the mechanism of action of Biguanides (metformin)? /thumbs-up\nEnvironment Feedback 2\n[+ Documents]\nThe mechanism of action of Biguanides (metformin) is to activate the enzyme AMP-activated protein kinase\n(AMPK) and reduce hepatic glucose production.\nAction 3\n[Candidate 1 \u2013 Reasoning] ... [Candidate 1 \u2013 Action] Answer: B /thumbs-down\n[Candidate 2 \u2013 Reasoning] ... [Candidate 2 \u2013 Action] Answer: C /thumbs-up\n23\n\nPrompt template for history knowledge summarization in Search-o1 and Re2Search\nYou are a helpful assistant tasked with answering a follow-up query using the relevant\ndocuments provided.\n### Relevant Documents\n{{documents}}\n### Context\nOriginal question: {{question}}\n### Follow-up Query\n{{query}}\nAnswer the follow-up query succinctly, using only the information from the documents. When\nthe documents do not provide sufficient information, explicitly point this out instead of making\nup facts. Do not include unrelated or excessive details in the response.\nFigure 8: Template used for history knowledge summarization in Search-o1 and Re2Search.\nPrompt template for generating actions using the Re2Search agent\nYou are a helpful assistant. Your task is to answer a given question following user instructions.\u2019\n### Information-seeking History\n{{history}}\n### Original Question\n{{question}}\nYour output must include three sections:\n1. **### Step-by-step Reasoning**:\n- Think step-by-step and then answer the question.\n2. **### Unverified Claim Identification**:\n- Identify if there are claims in the step-by-step reasoning section that are not grounded in the\ninformation-seeking history section.\n- If yes, summarize the first piece of missing information as an atomic query to search in an\nexternal knowledge base.\n- If no, clearly state that no further query is needed.\n3. **### Structured Output**:\n- Present your predicted answer and generated query (if applicable) in the following JSON\nformat:\n\u201c\u2018json\n{\n\u201cpredicted_answer\": \u201cProvide a single letter (for multiple-choice questions), digit, word, or\nshort phrase here.\",\n\u201cgenerated_query\": \u201cProvide an entity, question, or statement to be searched in an external\nknowledge base. Output \\\u201cNone\\\" if no query is generated.\",\n}\n\u201c\u2018\nFigure 9: Template used to generate actions for the Re2Search agent.\n24\n\nPrompt template for ranking candidate actions with GPT-4o\nYou are a decision-evaluation assistant. Your task is to rank the proposed actions from the\nmost appropriate to the least appropriate as the next step in a sequential decision-making\nprocess aimed at solving a given question.\n### Original Question:\n{{question}}\n### Information-Seeking History:\n{{curr_history}}\n### Proposed Next Actions:\n{{actions_text}}\n### Important Assumption\nThe agent has no prior knowledge about the subject matter. It must rely solely on the\ninformation-seeking history provided to evaluate and answer the original question. Assump-\ntions not explicitly supported by the history must not influence the ranking of proposed actions.\n### Evaluation Criteria for Appropriateness\n1. **Sufficiency Check**:\n- Determine whether the available information is sufficient to directly answer the original\nquestion. If not, the proposed action to \u201cAnswer\u201d is inappropriate.\n- Prioritize queries that gather specific, missing information essential to solving the question.\n- If the history already contains all necessary information, then \u201cAnswer\u201d is the most\nappropriate action, and the correct answer should be ranked highest.\n2. **Utility Check**:\n- Queries must be precise, actionable, and directly relevant to solving the question.\n- Prioritize foundational queries that establish critical context or general knowledge necessary\nfor more specific follow-ups.\n- Rank overly narrow or prematurely specific queries lower if they presume knowledge not yet\navailable.\n- Avoid irrelevant queries that do not contribute to solving the original question.\n3. **Redundancy Check**:\n- Queries that duplicate information already covered in the history or repeat previous queries\nshould be ranked lower.\n- Proposed actions must add new value to the decision-making process by seeking new or\nclarifying missing information.\n### Expected Output Format\n- Output the indices of the ranked actions in JSON format: \u201c\u2018json{\u201cranked_indices\u201d: [list of\nindices]}\u201d\u2019.\n- Rank actions from most appropriate to least appropriate based on the evaluation criteria\nabove.\n- Do not provide additional explanations or reasoning.\u201d\u2019\nFigure 10: Template used by GPT-4o to rank action candidates given the state.\n25",
  "authors": [
    "Guangzhi Xiong",
    "Qiao Jin",
    "Xiao Wang",
    "Yin Fang",
    "Haolin Liu",
    "Yifan Yang",
    "Fangyuan Chen",
    "Zhixing Song",
    "Dengyu Wang",
    "Minjia Zhang",
    "Zhiyong Lu",
    "Aidong Zhang"
  ],
  "summary": "Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.",
  "pdf_url": "https://arxiv.org/pdf/2502.13957v2",
  "entry_id": "http://arxiv.org/abs/2502.13957v2",
  "published": "2025-02-19",
  "updated": "2025-05-31",
  "comment": "Homepage: https://rag-gym.github.io; Code: https://github.com/RAG-Gym/RAG-Gym",
  "journal_ref": null,
  "doi": null,
  "primary_category": "cs.CL",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "links": [
    {
      "href": "https://arxiv.org/abs/2502.13957v2",
      "rel": "alternate",
      "title": null
    },
    {
      "href": "https://arxiv.org/pdf/2502.13957v2",
      "rel": "related",
      "title": "pdf"
    }
  ],
  "full_text": "arXiv:2502.13957v2  [cs.CL]  31 May 2025\nRAG-Gym: Systematic Optimization of Language\nAgents for Retrieval-Augmented Generation\nGuangzhi Xiong\u22171, Qiao Jin\u22172, Xiao Wang3, Yin Fang2, Haolin Liu1, Yifan Yang2, Fangyuan\nChen4, Zhixing Song5, Dengyu Wang6, Minjia Zhang3, Zhiyong Lu\u20202, and Aidong Zhang\u20201\n1University of Virginia, 2National Institutes of Health, 3University of Illinois at Urbana Champaign,\n4Dana-Farber Cancer Institute, 5University of Alabama at Birmingham, 6Yale School of Medicine\nAbstract\nRetrieval-augmented generation (RAG) has shown great promise for knowledge-\nintensive tasks and recently advanced with agentic RAG, where language agents\nengage in multi-round interactions with external knowledge sources for adaptive\ninformation retrieval. However, existing agentic RAG methods often depend on\nad-hoc prompt engineering and lack a unified optimization framework. We in-\ntroduce RAG-Gym, a comprehensive platform that systematically explores three\noptimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic\ntraining. For prompt engineering, we propose Re2Search, a novel agent incorporat-\ning reasoning reflection that significantly outperforms standard prompts. In actor\ntuning, we evaluate three popular post-training algorithms with fine-grained process\nsupervision and identify direct preference optimization as the most effective. We\nfurther demonstrate that a trained critic can enhance inference by selecting higher-\nquality intermediate reasoning steps. Together, these findings lead to the optimized\nRe2Search++ agent, which surpasses most recent methods like Search-R1 by a\nrelative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact\nof different reward sources and analyze scaling properties in training and inference,\noffering practical insights for agentic RAG optimization. The project homepage is\navailable at https://rag-gym.github.io/.\n1 Introduction\nLarge language models (LLMs) often struggle with knowledge-intensive questions when lacking\nsufficient or up-to-date domain knowledge, leading to inaccurate responses or hallucinations [ 97,\n59, 28]. Retrieval-augmented generation (RAG) addresses this by grounding outputs in relevant\ninformation from information retrieval (IR) systems, improving both accuracy and verifiability of\nanswers [42, 18]. Agentic pipelines such as ReAct [ 91] enhances conventional RAG by allowing\nLLMs to actively generate search queries and interact with IR systems in multiple rounds, which has\nbeen shown to be more effective in solving complex tasks that need multi-hop reasoning [91, 4, 65].\nHowever, most existing agentic RAG methods focus on prompt engineering [73, 4, 31, 54], which\ndemands substantial manual effort and often fails to generalize across tasks [40, 70, 2].\nMeanwhile, although various LLM post-training algorithms have been developed to enhance down-\nstream performance, they are not directly suited for agentic RAG, where the model must dynamically\nadjust its token-generation strategy in response to newly retrieved context during the reasoning\nprocess. Recent works have adapted reinforcement learning with outcome-based rewards for agentic\nRAG [69, 33, 8]. However, by overlooking process-level supervision, these approaches risk generat-\ning suboptimal intermediate search actions and exhibit limited generalization on unseen data. Given\n\u2217Equal contribution. \u2020Co-correspondence.\nPreprint. Under review.\n\nthat the retrieval steps fundamentally shape the reasoning trajectory and ultimately influence the final\nanswer, providing fine-grained supervision over these intermediate steps is essential for optimizing\nagentic RAG. Nevertheless, systematic analyses on how to optimize the language agent and identify\nbest practices for enhancing overall agentic RAG performance are still lacking.\nIn this work, we present RAG-Gym, a systematic framework that enhances agentic RAG along three\ndimensions: prompt engineering, actor tuning, and critic training. We review and compare the func-\ntional components of existing agentic RAG pipelines (see Table 1) and introduce a novel agent design\nRe2Search that leverages reasoning reflection to improve performance. Our comprehensive experi-\nments across three widely used LLM post-training algorithms reveal that fine-grained, process-level\nsupervision substantially boosts performance, particularly when both positive and negative feedback\nare integrated. Furthermore, we show that training a critic to evaluate intermediate steps yields\nadditional gains across diverse LLMs. By integrating these insights, our optimized Re2Search++\nagent achieves superior performance than existing methods on challenging knowledge-intensive tasks\n(+ 3.2%\u223c11.6% in average F1), especially on unseen datasets (+ 8.5%\u223c24.7%). We also discuss\nreward sources as well as the training and inference scaling properties of agentic RAG, providing\npractical guidelines for future optimization. Our key contributions are summarized as follows:\n\u2022 We introduceRAG-Gym, a comprehensive framework that integrates advanced prompt engineering,\nactor tuning, and critic training to enhance agentic RAG.\n\u2022 Our extensive experiments uncover best practices across these dimensions and lead to the develop-\nment of the optimized agent Re2Search++, which consistently outperforms existing methods on\nchallenging knowledge-intensive tasks.\n\u2022 We provide a detailed analysis of reward sources as well as training and inference scaling properties,\noffering actionable insights for future advancements in agentic RAG.\n2 RAG-Gym Framework\nTo facilitate fine-grained process-level supervision and systematic evaluation of optimization methods\nfor agentic RAG, we introduce the RAG-Gym framework. RAG-Gym formulates knowledge-intensive\nquestion answering as a high-level MDP with well-defined intermediate actions, and provides a\nmodular approach for optimizing language agents across three key components. An overview of\nRAG-Gym is presented in Figure 1.\n2.1 Knowledge-intensive Question Answering as Markov Decision Process\nWhile sequential token generation in LLMs can be modeled as an MDP [43, 49, 93], the integration\nof interactions with the IR environment introduces complex and inconsistent state transitions across\nagent architectures. To address this, we propose a hierarchical MDP formulation in RAG-Gym that\nunifies diverse agentic RAG designs. At the high level, agentic RAG is represented as a sequence of\nreasoning steps that interact with an IR system, while at the low level, each action involves sequential\ntoken generation by LLMs. Below, we formally define the components of the high-level MDP.\nStates. For the agentic RAG process of a given question Q, we define the state st at time step\nt to be a set consisting of the original question Q and the information-seeking history Ht. The\ninformation-seeking history is a sequence of search queries q1, \u00b7\u00b7\u00b7 , qt\u22121 and their corresponding sets\nof retrieved documents D1, \u00b7\u00b7\u00b7 , Dt\u22121, and is used to augment the agent\u2019s knowledge for answering\nthe original question. The initial state is defined as s1 = (Q, H1), where H1 is an empty set.\nActions. Although agents may employ various strategies to reason about the current state and generate\ndifferent token sequences, RAG-Gym standardizes these outputs by defining a common macro-action\nspace. At each time step t, the action at is either a search query or a predicted answer to the original\nquestion. While the detailed generated token sequences may differ among agent designs, they must\nalways be semantically equivalent to a designated macro-action within the context of agentic RAG.\nEnvironment. The high-level MDP environment in RAG-Gym is powered by an IR system, which\nis central to the agentic RAG approach. At each time step t, if the agent\u2019s action at is a search\nquery qt, the IR system returns a corresponding set of documents Dt. The state is then updated\nfrom st = (Q, Ht) to st+1 = (Q, Ht \u222a {(qt, Dt)}). Conversely, if at predicts an answer to Q, the\n2\n\nProcess \nReward\nProcess \nReward\nProcess \nReward\nAnswer \nGeneration\nQuestion\nReasoning\nRetrieval \nAugmentation\nDocument \nSummarization\nReasoning \nReflection\nQuery \nGeneration\nWhat is the date of death of\nAlberto De Martino?\n\u2026The date of death of Alberto\nDe Martino is 2 June 2015\u2026\n \nWhat is the date of death of the\ndirector of film Holocaust 2000?\n \nWho is the director of the film\n\u2018Holocaust 2000\u2019?\n \n\u2026 director of the film \u2018Holocaust\n2000\u2019 is Alberto De Martino\u2026\nQuery:\nDocuments: Agent\nAction(s)\nWhat is the date of birth\nof Alberto De Martino?\nWhat is the date of death\nof Alberto De Martino?\nAlberto De Martino\u2019s date\nof death is 1990. \nState\nQuestion: ...\nQuery: ...\nDocuments: ...\nQuery:\nDocuments:\nQuery:\nQuery:\nAnswer:\nState\nEnvironment\n Environment\nAction\nAction\nAction\nAction\nAction\nStateAction\nCritic\nProcess \nReward\nProcess \nReward\nState\nActorState Action\nPrompt Engineering Actor Tuning Critic Training\nWhat is the date of death\nof Alberto De Martino?\nQuery:\nAction\nQuestion:\nState Action\nSupervised Fine-tuning\nState\nAction\nDirect Preference Optimization\nAction\nState Action\nProximal Policy Optimization\nProcess \nReward\nModel\nInput Question: What is the date of death of the director of film Holocaust 2000?\nHotpotQA F144.91% 55.22% 60.19%41.09%\nReSearch Direct Preference Optimization Critic\nFigure 1: Overview of the RAG-Gym framework. RAG-Gym employs a modular design, comprising\nprompt engineering, actor tuning, and critic training, to systematically optimize agentic RAG perfor-\nmance. By leveraging all three components, RAG-Gym improves the F1 score of the ReAct agent on\nHotpotQA from 41.09% to 60.19%.\nepisode terminates. To maintain stable and reproducible state transitions, the configuration of the IR\nsystem (e.g., the number of returned documents) remains constant throughout.\nRewards. For the high-level MDP, the immediate reward for a state-action pair(st, at) is defined as\nzero when at is a search query, and as the correctness of the predicted answer when at is an answer.\nMoreover, by formulating knowledge-intensive QA as a high-level MDP, we can directly assess the\nquality of intermediate actions, with process-level rewards derived from various sources (e.g., human\nannotations, LLM evaluations, or rollouts). This enables both the evaluation of intermediate actions\nand the fine-grained supervision of language agents through process-level feedback.\n2.2 Systematic Optimization of Agentic Retrieval-augmented Generation\nWith the high-level MDP formulation, RAG-Gym optimizes the agentic RAG system through three\nkey components: (1) prompt engineering, which refines the language agent\u2019s structure and operational\ndesign; (2) actor tuning, which adjusts the LLM parameters to improve decision-making; and (3)\ncritic training, which develops an external verifier to assess the quality of generated macro-actions.\n2.2.1 Prompt Engineering\nThe first aspect of optimizing agentic RAG is crafting effective prompts that guide the language\nmodel in generating the appropriate actions. The system prompt defines the agent\u2019s functional\ncapabilities when processing a given state. RAG-Gym summarizes the essential functions into six\ndistinct categories:\n\u2022 Answer generation: The agent produces a final answer to the question.\n\u2022 Question reasoning: The agent outlines reasoning steps before providing the answer.\n\u2022 Retrieval augmentation: The agent incorporates retrieved content to enhance its answer.\n\u2022 Query generation: The agent formulates queries to search for relevant documents.\n\u2022 Document summarization: The agent condenses retrieved content to extract key information.\n\u2022 Reasoning reflection: The agent reviews its reasoning to identify any unverified claims.\n3\n\nWhile the first five components have already been employed in existing agent architectures, the final\ncomponent reasoning reflection is a novel addition by RAG-Gym. Inspired by recent advancements in\nreasoning models in which the models can reflect on their own reasoning process for self-correction\n[19], the newly introduced reasoning reflection directs the agent to scrutinize its reasoning process\nand identify claims that are unsupported by the information seeking history, thereby linking search\nquery generation to answer reasoning to produce more precise and relevant queries.\nCombining reasoning reflection with other existing components, we propose a new agent architecture\ncalled Re2Search, which stands for Reasoning, Reflection, and Search. A Re 2Search agent first\nreasons about all available information to construct an answer to the original question. It then\nreflects on its reasoning process to identify unverified claims that lack sufficient justification based\non available evidence. These unverified claims form the basis for generating the next search query\nthat is designed to retrieve the missing information required for constructing the answer. Table 1\nsummarizes the presence or absence of these components in several existing agent architectures,\nincluding Direct, CoT [81], RAG [42], ReAct [91], Search-o1 [44], and our proposed Re2Search,\neach enabling different LLM capabilities through prompting.\nTable 1: A comparative overview of agent architectures based on their functional components.\nComponent Direct CoT [81] RAG [42] ReAct [91] Search-o1 [44] Re 2Search\nAnswer Generation \u2714 \u2714 \u2714 \u2714 \u2714 \u2714\nQuestion Reasoning \u2717 \u2714 \u2714 \u2714 \u2714 \u2714\nRetrieval Augmentation \u2717 \u2717 \u2714 \u2714 \u2714 \u2714\nQuery Generation \u2717 \u2717 \u2717 \u2714 \u2714 \u2714\nDocument Summarization \u2717 \u2717 \u2717 \u2717 \u2714 \u2714\nReasoning Reflection \u2717 \u2717 \u2717 \u2717 \u2717 \u2714\n2.2.2 Actor Tuning\nThe second aspect of optimizing agentic RAG is tuning LLM parameters to directly enhance reasoning\ncapability. Decomposing knowledge-intensive QA into intermediate steps, the high-level MDP in\nRAG-Gym enables the targeted optimization of language agents by focusing on the generated action\nat each step, reducing the task to standard text generation. This streamlines the training process and\nfacilitates the application of various LLM post-training algorithms to enhance agent performance.\nProcess Reward Data Collection. As discussed in our high-level MDP definition, the process reward\nfor intermediate actions can be derived from multiple sources, including human annotations, LLM\nevaluations, or rollouts. In our implementation, we focus on collecting process reward data using\nadvanced LLMs such as GPT-4o [1]. Specifically, we sample trajectories from an untuned agent and\nobtain process reward annotations from GPT-4o, while filtering out trajectories that do not result\nin a correct final answer using the outcome reward. This strategy enables us to efficiently gather\nhigh-quality process reward data, which is subsequently used to optimize the LLMs for agentic RAG.\nFurther details on alternative process reward sources can be found in Section 4.1, with additional\ninformation about the data collection pipeline provided in Appendix E.\nProcess-based Training Algorithms.\nLet D denote the process reward dataset, which consists of tuples (s, a+, a\u2212), where s is a state, a+\nis a preferred (high-quality) action, and a\u2212 is a less-preferred (lower-quality) action. Each action is\nannotated based on the quality of the generated query or predicted answer. We assign the preference\nlabel to the entire token sequence produced when reasoning about the state, thereby reducing process-\nbased actor tuning to a standard text generation problem. RAG-Gym implements and compares three\nwidely used LLM post-training algorithms:\n\u2022 Supervised fine-tuning (SFT) [52]: This method uses high-quality intermediate actions to train\nlanguage agents by maximizing the log-likelihood of preferred actions (a+) conditioned on their\nrespective states s.\n\u2022 Direct preference optimization (DPO) [56]: This approach employs a contrastive learning\nframework that utilizes both preferred ( a+) and unpreferred ( a\u2212) actions. The DPO objective\nencourages the agent to increase the likelihood of preferred actions while decreasing that of\nunpreferred actions.\n4\n\n\u2022 Proximal policy optimization (PPO) [60]: This is an online reinforcement learning algorithm for\npolicy optimization. The collected data D is first used to train a process reward model r\u03d5(s, a).\nPPO then optimizes the agent to maximize the process reward of newly generated actions, while\nconstraining policy updates to ensure stability.\n2.2.3 Critic Training\nThe third aspect of optimizing agentic RAG involves training a critic, denoted as r\u03d5, to act as an\nexternal evaluator of generated actions. The critic is designed to predict process rewards for a given\nstate-action pair (s, a). Its training objective employs a contrastive loss that distinguishes preferred\nactions from less-preferred ones, following the preference modeling approach widely used in LLM\nalignment and reward modeling [47, 52]:\nLcritic(\u03d5) =\u2212E(s,a+,a\u2212)\u223cD\nh\nlog \u03c3\n\u0000\nr\u03d5(s, a+) \u2212 r\u03d5(s, a\u2212)\n\u0001i\n, (1)\nwhere \u03c3 is the sigmoid function and D denotes the process reward dataset containing both preferred\n(a+) and less-preferred (a\u2212) actions.\nWhile process reward modeling has been studied in the context of math reasoning [62, 46], its appli-\ncation to agentic RAG for knowledge-intensive question answering remains largely underexplored.\nIn RAG-Gym, our process-level critic is tailored to evaluate intermediate actions such as search\nqueries, rather than only final answers. This approach enables more fine-grained and actionable\nfeedback, facilitating the optimization of agentic RAG systems through process-level supervision.\nOnce trained, the critic provides targeted feedback on generated actions, guiding the language agent\nto make decisions that are more likely to lead to successful outcomes.\n3 Main Results\n3.1 Experimental Settings\nTo assess the performance of various agents on knowledge-intensive QA tasks and evaluate the\nbenefits of different optimization methods in RAG-Gym, we consider four datasets that are both\nknowledge- and reasoning-intensive, spanning general and medical domains. Specifically, we use\nHotpotQA [90], 2WikiMultihopQA [21], and Bamboogle [ 54], which are popular multi-hop QA\ndatasets constructed from Wikipedia, as well as the MedQA dataset [34], which consists of medical\nexam questions that require specialized domain knowledge and complex reasoning. Following prior\nwork [61], HotpotQA, 2WikiMultihopQA, and Bamboogle are evaluated using Exact Match (EM)\nand F1 scores, while the multi-choice MedQA dataset is assessed with accuracy (Acc). We also\ncompute the average EM and F1 scores across different tasks, treating accuracy as equivalent to both\nmetrics in the multi-choice evaluation setting. For actor and critic training in RAG-Gym, 1k questions\nwere sampled from the HotpotQA and MedQA training sets for process reward data collection. To\ntest the generalizability of the tuned agents, 2WikiMultihopQA and Bamboogle were evaluated using\nLLMs trained on HotpotQA. More implementation details can be found in Appendices C, E, H.\n3.2 Performance Improvements by Prompt Engineering and Actor Tuning\nTable 2 presents a performance comparison of various agents and their tuned versions using different\nactor tuning algorithms in RAG-Gym. The results indicate that the Re 2Search agent consistently\noutperforms other agents in both zero-shot and actor-tuned settings. Furthermore, when comparing\nTable 2 with Table 1, which details the functional components of each agent, it can be observed\nthat more components generally leads to improved performance. This observation validates the\neffectiveness of the summarized functions in RAG-Gym, as well as the design of the Re2Search agent,\nwhich incorporates all identified components, including our newly proposed reasoning reflection.\nAdditional case studies of our proposed Re2Search agent are provided in Appendices G.1 and G.2.\nBy comparing different process supervision approaches for actor tuning, we observe that process\nsupervision consistently enhances agent performance relative to the zero-shot learning (ZSL) baseline.\nThis improvement underscores the critical role of process supervision in refining agentic RAG.\nNotably, for Direct, CoT, and RAG agents, where tuning focuses solely on answer generation, SFT\nslightly outperforms both DPO and PPO. In contrast, for ReAct, Search-o1, and Re2Search agents,\n5\n\nTable 2: Agent performance with Llama-3.1-8B backbone. Highest scores are bolded.\nMethod Agent HotpotQA 2Wiki Bamboogle MedQA Average\nEM F1 EM F1 EM F1 Acc EM F1\nZero-shot\nLearning\nDirect 21.10 27.93 24.10 27.68 9.60 14.89 61.82 29.16 33.08\nCoT 27.10 35.17 25.70 30.08 37.60 49.50 69.60 40.00 46.09\nRAG 38.30 48.57 32.00 36.91 22.40 33.73 66.85 39.89 46.51\nReAct 30.70 41.09 28.90 35.03 32.00 41.35 62.37 38.49 44.96\nSearch-o1 35.30 47.33 34.00 41.29 44.80 52.50 66.14 45.06 51.82\nRe2Search 34.00 44.91 41.50 49.06 44.80 55.33 70.31 47.65 54.90\nRAG-Gym\nSupervised\nFine-tuning\nDirect 22.80 31.67 28.00 33.17 20.00 27.21 63.63 33.61 38.92\nCoT 26.50 35.60 27.30 32.10 42.40 53.89 69.68 41.47 47.82\nRAG 41.50 52.26 38.00 42.74 28.80 40.76 67.79 44.02 50.89\nReAct 35.50 46.06 31.00 36.79 34.40 44.17 66.69 41.90 48.43\nSearch-o1 38.20 50.02 39.00 45.91 46.40 57.18 67.64 47.81 55.19\nRe2Search 37.60 49.16 44.00 50.54 44.80 56.78 69.52 48.98 56.50\nRAG-Gym\nDirect\nPreference\nOptimization\nDirect 20.80 28.79 25.20 29.45 12.00 20.67 62.37 30.09 35.32\nCoT 26.30 35.06 28.20 32.84 40.80 51.67 71.33 41.66 47.73\nRAG 38.00 49.38 37.60 42.88 28.80 39.57 67.79 43.05 49.91\nReAct 33.00 43.96 32.20 39.24 44.80 54.35 68.89 44.72 51.61\nSearch-o1 42.20 54.34 44.10 52.66 42.40 55.59 70.23 49.73 58.21\nRe2Search 42.20 55.22 44.30 51.36 48.00 56.57 72.11 51.65 58.82\nRAG-Gym\nProximal\nPolicy\nOptimization\nDirect 19.20 26.17 25.60 28.84 7.20 12.17 61.12 28.28 32.08\nCoT 25.50 33.68 24.20 29.02 43.20 52.54 68.50 40.35 45.94\nRAG 37.70 47.60 32.00 36.29 28.80 40.24 68.03 41.63 41.44\nReAct 35.80 47.56 33.20 40.06 36.80 46.79 67.32 43.28 50.43\nSearch-o1 38.30 50.24 32.60 39.34 50.40 59.92 70.15 47.86 54.91\nRe2Search 38.40 50.30 41.40 48.06 49.60 62.06 71.72 50.28 58.04\nwhere the tuning process also involves generating high-quality queries, DPO and PPO surpass\nSFT, with DPO demonstrating a slight edge over PPO on most tasks. These findings highlight the\nimportance of utilizing both positive and negative samples during training, especially for agents that\nrequire complex, multi-step reasoning with environmental feedback. Furthermore, the tuned agents\ntend to generate more search queries during inference, as elaborated in Appendix F.\n3.3 Performance Improvements by Critic Training\nFigure 2 illustrates the performance improvements achieved through critic training. The label \u201cWith\nCritic\u201d indicates that an external critic evaluates 10 sampled actions at each step to select the best one.\nIn our experiments, all agents except for \u201cDirect\u201d consistently benefit from critic training. Moreover,\nthese performance gains transfer to actors using different LLMs. As shown in the figure, not only does\nthe original Llama-3.1-8B benefit from the trained critic, but both the DPO-tuned Llama-3.1-8B and\nGPT-4o-mini also experience significant improvements across all datasets using the same critic. This\nhighlights the potential of employing trained critics as a plug-and-play module to enhance agentic\nRAG performance, particularly for proprietary LLMs where direct fine-tuning is not feasible. A case\nstudy of using trained critics during inference is provided in Appendix G.3.\n3.4 Comparisons with Outcome Supervision Methods\nCombining the findings from previous sections, we introduce Re2Search++, an optimized agent that\nintegrates the best choices from each optimization direction. Built on Re2Search and tuned with DPO\nwhile utilizing a trained critic for action selection, Re2Search++ is evaluated against recent methods\nsuch as Search-R1 [33] and R1-Searcher [69], which rely on outcome supervision via reinforcement\nlearning (RL) with over 8k training questions. As these methods primarily focus on general-domain\nquestions, we exclude MedQA from this evaluation for a fair comparison. Table 3 shows that\nRe2Search++ achieves performance comparable to that of the RL-tuned agents on the datasets used\nfor their training (HotpotQA for Search-R1; HotpotQA and 2WikiMultihopQA for R1-Searcher),\nwhile significantly outperforming them on unseen datasets and achieving the best performance on\naverage. This result underscores the overfitting issues of RL-based outcome supervision methods\nand highlights the robustness and generalizability of Re2Search++ through its fine-grained process\nsupervision on intermediate steps.\n6\n\nWithout Critic With Critic\n20\n40\n60\n80F1 / Accuracy\n33.08 32.45\nDirect\nWithout Critic With Critic\n40\n60\n80\n46.09 49.02\nCoT\nWithout Critic With Critic\n40\n60\n80\n46.51\n55.64\nRAG\nWithout Critic With Critic\n40\n60\n80\n44.96\n56.47\nReAct\nWithout Critic With Critic\n40\n60\n80F1 / Accuracy\n51.81\n61.04\nSearch-o1\nWithout Critic With Critic\n40\n60\n80\n54.73\n62.41\nRe\u00b2Search\nWithout Critic With Critic\n40\n60\n80\n58.81\n64.12\nRe\u00b2Search (Llama-3.1-8B-DPO)\nWithout Critic With Critic\n40\n60\n80\n61.06\n65.30\nRe\u00b2Search (GPT-4o-mini)\nAvg. Without Critic Avg. With Critic HotpotQA F1 2WikiMultihopQA F1 Bamboogle F1 MedQA Accuracy\nFigure 2: Performance improvements across various agents with critics.\nTable 3: Comparison of Re 2Search++ and other methods. Shading indicates in-domain model\nperformance. CEM represents the \u201cCover Exact Match\u201d used in [69].\nLLM Method HotpotQA 2WikiMultihopQA Bamboogle Average\nEM CEM F1 EM CEM F1 EM CEM F1 EM CEM F1\nLlama-3.1-8B\nReAct 30.70 38.40 41.09 28.90 38.00 35.03 32.00 36.80 41.35 30.57 37.73 39.16Search-o1 35.30 43.80 47.33 34.00 45.80 41.29 44.80 48.80 52.50 38.03 46.13 47.04R1-Searcher 44.90 50.40 56.88 48.70 51.30 54.24 38.40 40.80 53.21 44.00 47.50 54.78\nRe2Search++46.50 57.80 60.19 48.90 60.50 56.85 55.20 63.20 66.37 50.20 60.50 61.14\nQwen-2.5-7B\nReAct 36.00 40.10 45.84 38.60 44.50 45.02 35.20 38.40 44.94 36.60 41.00 45.27Search-o1 40.70 46.60 52.15 38.90 46.20 45.79 40.80 44.80 52.91 40.17 45.87 50.28Search-R1 44.90 49.40 57.30 43.90 47.80 50.07 40.80 41.60 51.69 43.20 46.27 53.02R1-Searcher 46.80 53.70 59.61 48.80 55.00 55.36 44.80 48.00 54.01 46.80 52.23 56.33\nRe2Search++44.40 50.30 56.47 47.00 56.50 54.35 52.94 56.30 63.51 48.11 54.37 58.11\n4 Analysis and Discussion\n4.1 Comparison of Different Reward Sources\nAs discussed in Section 2, the process reward can be collected from different sources. This section\nfocuses on the evaluation of the effectiveness of these sources in guiding the agent\u2019s action selection\ntoward correct answers, as well as their alignment with human preferences, which are often considered\nto have the highest quality for process annotation [ 98]. Specifically, we compare the GPT-4o\nannotations with Llama-3.1-8B, as well as the rollout-based annotations using Math-Shepherd [77].\nWe collect process annotations from human experts on MedQA to examine the alignment between\nthe trained reward models and human preferences.\nTable 4: Comparison of various reward sources. ORM/PRM denotes the outcome/process reward\nmodel. Outcome sources are labeled for PRMs due to the trajectory filtering in RAG-Gym.\nType Outcome\nSource\nProcess\nSource\nHotpotQA\n(EM / F1)\n2Wiki\n(EM / F1)\nBamboogle\n(EM / F1)\nMedQA\n(Acc / Agree)\nORM Truth \u2013 41.10 / 53.35 47.70 / 55.59 43.20 / 57.46 66.77 / \u2013\nPRM (Random) \u2013 \u2013 32.20 / 42.83 35.70 / 42.00 38.40 / 47.86 68.26 / 50.00\nPRM (Rollout) Truth Rollout 39.60 / 51.85 42.94 / 49.57 48.80 / 56.05 68.34 / 71.03\nPRM (Llama) Truth Llama-3.1-8B 40.30 / 51.74 40.70 / 48.22 44.80 / 54.36 68.50 / 65.99\nPRM (GPT) Truth GPT-4o 44.10 / 56.84 50.20 / 57.94 51.20 / 63.15 71.96 / 85.85\n7\n\nThe results are shown in Table 4. The reward model trained with GPT-4o annotations delivers the\nhighest performance across all datasets, effectively providing accurate, fine-grained process rewards\nfor agent optimization. Moreover, it exhibits the best alignment with human preferences, achieving\nan agreement rate of 85.85% with human annotators. In contrast, although rollouts and Llama-3.1-8B\nannotations improve action selection relative to a process reward model with random selections, they\nare generally less effective than GPT-4o annotations and sometimes even bring inferior outcomes on\ngeneral-domain questions. This result underscores the limitations of current rollout-based methods,\noriginally designed for math reasoning, in the context of complex reasoning and search tasks, and\nhighlights the need for tailored approaches in agentic RAG.\n4.2 Training Time Scaling\nFor the evaluation of training sample size and its impacts on the performance of Re2Search agents,\nwe conducted experiments using critics trained on varying numbers of instances, ranging from 250 to\n1000 questions. The results, presented in Figure 3, show how the agent\u2019s performance scales with\nthe availability of more training data across four datasets. In general, the performance of Re2Search\nimproves with an increasing number of training samples, but the gains tend to converge as the sample\nsize grows. Notably, there is a sharp improvement in F1 scores on HotpotQA, 2WikiMultihopQA,\nand Bamboogle when comparing the ZSL baseline to process reward models trained on 250 samples,\nshowing that even a small amount of process reward data can yield significant performance gains.\nHowever, the improvements become less pronounced on HotpotQA and 2WikiMultihopQA when\nincreasing the training samples from 500 to 1000, indicating diminishing returns as the model\napproaches a saturation point in its learning from additional data.\n250 500 750 1000\n#Sample\n43\n50\n57F1 (%)\nZSL\nHotpotQA\n250 500 750 1000\n#Sample\n47\n53\n59F1 (%)\nZSL\n2WikiMultihopQA\n250 500 750 1000\n#Sample\n57\n61\n65F1 (%)\nZSL\nBamboogle\n250 500 750 1000\n#Sample\n68\n70\n72Acc (%)\nZSL\nMedQA\nFigure 3: Performance of Re2Search agents with critics trained on different numbers of samples.\nFor MedQA, which involves complex reasoning and information-seeking tasks requiring domain-\nspecific knowledge, a different trend is observed. With only 250 training samples, the performance\nslightly drops below the ZSL baseline, highlighting the challenges of capturing intricate domain-\nspecific processes with limited training data. As the sample size increases, however, the performance\ngradually recovers and eventually surpasses the ZSL baseline, achieving the highest accuracy of\n71.72% with 1000 samples. This underscores the importance of sufficient training data in capturing\nthe nuanced reasoning and query-generation processes required for specialized tasks.\n4.3 Inference Time Scaling\nSince trained critics optimize action-taking by identifying high-quality actions from the generated\ncandidates during inference, we explored how the agent performance changes with the increasing\nnumber of sampled actions at each time step. Figure 4 displays the results of our inference time\nscaling study, with Re2Search as the tested agent. We observe a consistent trend across multiple\nbenchmarks, where increasing the number of sampled actions generally improves performance.\nSpecifically, for HotpotQA and Bamboogle, the F1 score continues to rise as more actions are\nsampled, demonstrating the benefits of expanding the candidate set to enable better action selection at\neach step. However, performance gains gradually diminish, indicating that the agent reaches a point\nwhere additional sampled actions contribute less to improvement. This suggests that while action\nsampling is beneficial, there is a limit to how much additional sampling enhances decision-making.\n8\n\n5 10 15 20\n#Action\n43\n53\n63F1 (%)\nZSL\nHotpotQA\n5 10 15 20\n#Action\n47\n53\n59F1 (%)\nZSL\n2WikiMultihopQA\n5 10 15 20\n#Action\n56\n61\n66F1 (%)\nZSL\nBamboogle\n5 10 15 20\n#Action\n70\n72\n74Acc (%)\nZSL\nMedQA\nFigure 4: Performance of Re2Search agents with different numbers of actions sampled per step.\n5 Related Work\n5.1 Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large\nlanguage models (LLMs) on knowledge-intensive tasks. A typical RAG framework comprises two\ncore components: a retriever, which selects relevant documents from a large corpus, and a generator,\nwhich synthesizes information from these documents to produce coherent and contextually appropriate\nresponses [42, 18, 9]. RAG has demonstrated strong performance across diverse domains, including\nopen-domain question answering [ 37, 25, 7, 88, 63], fact-checking [ 78, 66], and summarization\n[3]. Subsequent research has focused on improving both the retriever\u2019s ability to select relevant\ndocuments [95, 51, 89, 30, 32] and the generator\u2019s capacity to effectively utilize retrieved information\n[15, 82, 80], thereby boosting overall system performance [26, 96, 36, 94]. Nevertheless, most RAG\npipelines still rely on a single retrieval step, which can be inadequate for complex queries that require\nsynthesizing information from multiple sources.\n5.2 Multi-hop Question Answering\nMulti-hop question answering (QA) tasks require systems to synthesize information from multiple,\ndiverse sources to produce accurate answers [ 90, 21]. These tasks highlight the limitations of\nconventional RAG architectures, where a single retrieval step often fails to capture the comprehensive\ncontext needed for complex queries. To address this, language agents such as ReAct [91, 73, 4, 31, 54]\nhave been proposed, interleaving reasoning and retrieval to dynamically accumulate relevant evidence\n[57, 79, 38, 61], which has shown promise in improving LLM performance [ 76, 64, 92, 27, 29].\nHowever, most of these methods still rely heavily on prompt engineering, which can be fragile and\nmay not effectively optimize language agents for knowledge-intensive tasks [ 40, 70, 2]. Recent\nstudies have explored reinforcement learning (RL) to optimize language agents for multi-hop QA\n[69, 33, 8, 17, 55], but these approaches risk generating suboptimal intermediate search actions and\nshow limited generalization to unseen data, as demonstrated in our experiments. Other concurrent\nwork investigates process-level supervision [22, 12, 45, 71], but typically focuses on specific agent\narchitectures and a narrow set of supervision methods, offering limited insight into the systematic\noptimization of language agents.\n5.3 Post-training of Large Language Models\nBeyond the foundational knowledge acquired during pre-training, post-training methods are essential\nfor refining Large Language Models (LLMs) and aligning them with specific downstream tasks and\ndesired behaviors. Supervised Fine-Tuning (SFT) adapts models using curated instruction-response\npairs to promote task-specific capabilities [ 52, 10]. While SFT enhances instruction-following,\nfurther alignment with nuanced human preferences is often achieved through Reinforcement Learning\nfrom Human Feedback (RLHF) [52, 6, 5], typically implemented via Proximal Policy Optimization\n(PPO) [60]. More recently, critic-free approaches such as Direct Preference Optimization (DPO)\nhave emerged as streamlined alternatives [56, 50, 14, 87], directly optimizing the LLM policy based\non reward annotations and bypassing the need for a separately trained reward model. Although\nthese techniques bring strong gains on text generation benchmarks, their integration into agentic\nRAG pipelines, where models must dynamically interact with retrieval systems and adapt reasoning\nstrategies to evolving contexts, remains underexplored.\n9\n\n6 Conclusion\nThis work presents RAG-Gym as a unified and extensible framework for systematically optimizing\nagentic RAG along the axes of prompt engineering, actor tuning, and critic training. Through\nextensive empirical analysis, we demonstrate that integrating reasoning reflection, process-level direct\npreference optimization, and critic-guided inference yields substantial improvements over existing\napproaches. We hope RAG-Gym will serve as a foundation for further advances in robust, adaptive,\nand interpretable retrieval-augmented language agents.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[2] Victor Akinwande, Yiding Jiang, Dylan Sam, and J Zico Kolter. Understanding prompt\nengineering may not require rethinking generalization. arXiv preprint arXiv:2310.03957, 2023.\n[3] Chenxin An, Ming Zhong, Zhichao Geng, Jianqiang Yang, and Xipeng Qiu. Retrievalsum: A\nretrieval enhanced framework for abstractive summarization. arXiv preprint arXiv:2109.07943,\n2021.\n[4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learn-\ning to retrieve, generate, and critique through self-reflection. In The Twelfth International\nConference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\nReview.net, 2024. URL https://openreview.net/forum?id=hSyW5go0v8.\n[5] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a\nlaboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n[6] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022.\n[7] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\nMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\net al. Improving language models by retrieving from trillions of tokens. In International\nconference on machine learning, pages 2206\u20132240. PMLR, 2022.\n[8] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z\nPan, Wen Zhang, Huajun Chen, Fan Yang, et al. Research: Learning to reason with search for\nllms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025.\n[9] Mingyue Cheng, Yucong Luo, Jie Ouyang, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang,\nJiawei Cao, Jie Ma, et al. A survey on knowledge-oriented retrieval-augmented generation.\narXiv preprint arXiv:2503.10677, 2025.\n[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. Journal of Machine Learning Research, 25(70):1\u201353, 2024.\n[11] Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. Reciprocal rank fusion outper-\nforms condorcet and individual rank learning methods. In Proceedings of the 32nd international\nACM SIGIR conference on Research and development in information retrieval, pages 758\u2013759,\n2009.\n[12] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen.\nProgressive multimodal reasoning via active retrieval. arXiv preprint arXiv:2412.14835, 2024.\n10\n\n[13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783, 2024.\n[14] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:\nModel alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.\n[15] Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. Enhancing\nnoise robustness of retrieval-augmented language models with adaptive adversarial training.\narXiv preprint arXiv:2405.20978, 2024.\n[16] Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, and Yanghua Xiao. Reward\nshaping to mitigate reward hacking in rlhf. arXiv preprint arXiv:2502.18770, 2025.\n[17] Jingsheng Gao, Linxu Li, Weiyuan Li, Yuzhuo Fu, and Bin Dai. Smartrag: Jointly learn\nrag-related tasks from the environment feedback. arXiv preprint arXiv:2410.18141, 2024.\n[18] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\nand Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv\npreprint arXiv:2312.10997, 2023.\n[19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n[20] Binglan Han, Teo Susnjak, and Anuradha Mathrani. Automating systematic literature reviews\nwith retrieval-augmented generation: A comprehensive overview. Applied Sciences, 14(19):\n9103, 2024.\n[21] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the\n28th International Conference on Computational Linguistics, pages 6609\u20136625, 2020.\n[22] Sheryl Hsu, Omar Khattab, Chelsea Finn, and Archit Sharma. Grounding by trying: Llms with\nreinforcement learning-enhanced retrieval. arXiv preprint arXiv:2410.23214, 2024.\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[24] Jian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. Openrlhf: An\neasy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143,\n2024.\n[25] Gautier Izacard and \u00c9douard Grave. Leveraging passage retrieval with generative models for\nopen domain question answering. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume, pages 874\u2013880, 2021.\n[26] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning\nwith retrieval augmented language models. Journal of Machine Learning Research, 24(251):\n1\u201343, 2023.\n[27] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong-Cheol Park. Adaptive-\nrag: Learning to adapt retrieval-augmented large language models through question complexity.\nIn 2024 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 7036\u20137050. Association for Computational\nLinguistics, 2024.\n[28] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.\nACM Computing Surveys, 55(12):1\u201338, 2023.\n11\n\n[29] Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, and Jiawei\nHan. Ras: Retrieval-and-structuring for knowledge-intensive llm generation. arXiv preprint\narXiv:2502.10996, 2025.\n[30] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng\nSun, and Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large\nlanguage models via reinforcement learning. arXiv preprint arXiv:2503.00223, 2025.\n[31] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969\u20137992,\n2023.\n[32] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented\ngeneration with long-context llms. arXiv preprint arXiv:2406.15319, 2024.\n[33] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Za-\nmani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with\nreinforcement learning. arXiv preprint arXiv:2503.09516, 2025.\n[34] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\ndisease does this patient have? a large-scale open domain question answering dataset from\nmedical exams. Applied Sciences, 11(14):6421, 2021.\n[35] Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John Wilbur, and\nZhiyong Lu. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs\nfor zero-shot biomedical information retrieval. Bioinformatics, 39(11):btad651, 2023.\n[36] Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, and Jun\nZhao. Rag-rewardbench: Benchmarking reward models in retrieval augmented generation for\npreference alignment. arXiv preprint arXiv:2412.13746, 2024.\n[37] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 6769\u20136781, 2020.\n[38] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and\nAshish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks.\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?\nid=_nGgzQjzaRy.\n[39] Jakub L\u00e1la, Odhran O\u2019Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G Rodriques, and\nAndrew D White. Paperqa: Retrieval-augmented generative agent for scientific research. arXiv\npreprint arXiv:2312.07559, 2023.\n[40] Divya Lamba. The role of prompt engineering in improving language understanding and\ngeneration. International Journal For Multidisciplinary Research, 2024. URL https://api.\nsemanticscholar.org/CorpusID:274939741.\n[41] Guido Lang and Tan G\u00fcrpinar. Ai-powered learning support: A study of retrieval-augmented\ngeneration (rag) chatbot effectiveness in an online course. Information Systems Education\nJournal, 23(2), 2025.\n[42] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. Advances in Neural Information Processing\nSystems, 33:9459\u20139474, 2020.\n[43] Dongheng Li, Yongchang Hao, and Lili Mou. Llmr: Knowledge distillation with a large\nlanguage model-induced reward. In Proceedings of the 2024 Joint International Conference on\nComputational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages\n10657\u201310664, 2024.\n12\n\n[44] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and\nZhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint\narXiv:2501.05366, 2025.\n[45] Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. Can we\nfurther elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving\nchallenging tasks. arXiv preprint arXiv:2410.01428, 2024.\n[46] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy\nLee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step.\nIn The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?\nid=v8L0pN6EOi.\n[47] Fei Liu et al. Learning to summarize from human feedback. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 583\u2013592, 2020.\n[48] Siru Liu, Allison B McCoy, and Adam Wright. Improving large language model applications\nin biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and\nclinical development guidelines. Journal of the American Medical Informatics Association ,\npage ocaf008, 2025.\n[49] Hao Ma, Tianyi Hu, Zhiqiang Pu, Liu Boyin, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolv-\ning with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement\nlearning. Advances in Neural Information Processing Systems, 37:15497\u201315525, 2024.\n[50] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a\nreference-free reward. Advances in Neural Information Processing Systems, 37:124198\u2013124235,\n2024.\n[51] Thang Nguyen, Peter Chin, and Yu-Wing Tai. Reward-rag: Enhancing rag with reward driven\nsupervision. arXiv preprint arXiv:2410.03780, 2024.\n[52] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in neural information processing systems,\n35:27730\u201327744, 2022.\n[53] Nicholas Pipitone and Ghita Houir Alami. Legalbench-rag: A benchmark for retrieval-\naugmented generation in the legal domain. arXiv preprint arXiv:2408.10343, 2024.\n[54] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\nMeasuring and narrowing the compositionality gap in language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023, pages 5687\u20135711, 2023.\n[55] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T\u00fcr, Gokhan\nTur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958,\n2025.\n[56] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\nAdvances in Neural Information Processing Systems, 36:53728\u201353741, 2023.\n[57] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown,\nand Yoav Shoham. In-context retrieval-augmented language models. Transactions of the\nAssociation for Computational Linguistics, 11:1316\u20131331, 2023.\n[58] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and\nbeyond. Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389, 2009.\n[59] Satya S Sahoo, Joseph M Plasek, Hua Xu, \u00d6zlem Uzuner, Trevor Cohen, Meliha Yetisgen,\nHongfang Liu, St\u00e9phane Meystre, and Yanshan Wang. Large language models for biomedicine:\nfoundations, opportunities, challenges, and best practices. Journal of the American Medical\nInformatics Association, page ocae074, 2024.\n13\n\n[60] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[61] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.\nEnhancing retrieval-augmented large language models with iterative retrieval-generation synergy.\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248\u20139274,\n2023.\n[62] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n[63] Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, and Ninghao\nLiu. Searchrag: Can search engines be helpful for llm-based medical question answering?\narXiv preprint arXiv:2502.13233, 2025.\n[64] Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun\nRen. Generate-then-ground in retrieval-augmented generation for multi-hop question answering.\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 7339\u20137353, 2024.\n[65] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\nReflexion: Language agents with verbal reinforcement learning.Advances in Neural Information\nProcessing Systems, 36, 2024.\n[66] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmenta-\ntion reduces hallucination in conversation. In Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 3784\u20133803, 2021.\n[67] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and\ncharacterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460\u2013\n9471, 2022.\n[68] Michael D Skarlinski, Sam Cox, Jon M Laurent, James D Braza, Michaela Hinks, Michael J\nHammerling, Manvitha Ponnapati, Samuel G Rodriques, and Andrew D White. Language\nagents achieve superhuman synthesis of scientific knowledge. arXiv preprint arXiv:2409.13740,\n2024.\n[69] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang,\nand Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2503.05592, 2025.\n[70] Hari Subramonyam, Divy Thakkar, Andrew Ku, Juergen Dieber, and Anoop K Sinha. Prototyp-\ning with prompts: Emerging approaches and challenges in generative ai design for collaborative\nsoftware teams. In Proceedings of the 2025 CHI Conference on Human Factors in Computing\nSystems, pages 1\u201322, 2025.\n[71] Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang,\nSong Yang, and Han Li. Rearter: Retrieval-augmented reasoning with trustworthy process\nrewarding. arXiv preprint arXiv:2501.07861, 2025.\n[72] Jakub Swacha and Micha\u0142 Gracel. Retrieval-augmented generation (rag) chatbots for education:\nA survey of applications. Applied Sciences, 15(8):4234, 2025.\n[73] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving\nretrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 10014\u201310037, 2023.\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n14\n\n[75] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan\nLambert, Shengyi Huang, Kashif Rasul, and Quentin Gallou\u00e9dec. Trl: Transformer reinforce-\nment learning. https://github.com/huggingface/trl, 2020.\n[76] Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge\nRong, and Zhang Xiong. Knowledge-driven cot: Exploring faithful reasoning in llms for\nknowledge-intensive question answering. arXiv preprint arXiv:2308.13259, 2023.\n[77] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and\nZhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations.\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 9426\u20139439, 2024.\n[78] Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim\nDas, and Preslav Nakov. Factuality of large language models: A survey. In Proceedings of the\n2024 Conference on Empirical Methods in Natural Language Processing, pages 19519\u201319529,\n2024.\n[79] Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. Rat: Retrieval\naugmented thoughts elicit context-aware reasoning in long-horizon generation. arXiv preprint\narXiv:2403.05313, 2024.\n[80] Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot,\nYuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing\nretrieval augmented generation through drafting. arXiv preprint arXiv:2407.08223, 2024.\n[81] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems, 35:24824\u201324837, 2022.\n[82] Zhepei Wei, Wei-Lin Chen, and Yu Meng. InstructRAG: Instructing retrieval-augmented\ngeneration via self-synthesized rationales. In The Thirteenth International Conference on\nLearning Representations, 2025. URL https://openreview.net/forum?id=P1qhkp8gQT.\n[83] Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie,\nIkechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, and Bruno Fleisch. Cbr-rag: case-\nbased reasoning for retrieval augmented generation in llms for legal question answering. In\nInternational Conference on Case-Based Reasoning, pages 445\u2013460. Springer, 2024.\n[84] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources\nto advance general chinese embedding, 2023.\n[85] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented\ngeneration for medicine. In Findings of the Association for Computational Linguistics ACL\n2024, pages 6233\u20136251, 2024.\n[86] Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, and Aidong Zhang. Im-\nproving retrieval-augmented generation in medicine with iterative follow-up questions. In\nBiocomputing 2025: Proceedings of the Pacific Symposium, pages 199\u2013214. World Scientific,\n2024.\n[87] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Ken-\nton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries\nof llm performance in machine translation. In International Conference on Machine Learning,\npages 55204\u201355224. PMLR, 2024.\n[88] Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang\nLi, Joyce C Ho, Carl Yang, et al. Simrag: Self-improving retrieval-augmented generation for\nadapting large language models to specialized domains. arXiv preprint arXiv:2410.17952,\n2024.\n15\n\n[89] Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May Dongmei Wang, Joyce C.\nHo, Chao Zhang, and Carl Yang. BMRetriever: Tuning large language models as better\nbiomedical text retrievers. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors,\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,\npages 22234\u201322254, Miami, Florida, USA, November 2024. Association for Computational\nLinguistics. doi: 10.18653/v1/2024.emnlp-main.1241. URL https://aclanthology.org/\n2024.emnlp-main.1241/.\n[90] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii, editors,\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npages 2369\u20132380, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\n[91] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. In International Conference\non Learning Representations (ICLR), 2023.\n[92] Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation\nfor large language models. 2024. URL https://arxiv.org/abs/2411.19443.\n[93] Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boull\u00e9,\nand Ievgen Redko. Large language models as markov chains. arXiv preprint arXiv:2410.02724,\n2024.\n[94] Hanning Zhang, Juntong Song, Juno Zhu, Yuanhao Wu, Tong Zhang, and Cheng Niu. Rag-\nreward: Optimizing rag with reward modeling and rlhf. arXiv preprint arXiv:2501.13264,\n2025.\n[95] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to\naugment large language models. arXiv preprint arXiv:2310.07554, 2023.\n[96] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and\nJoseph E Gonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint\narXiv:2403.10131, 2024.\n[97] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo\nZhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: a survey on hallucination in\nlarge language models. arXiv preprint arXiv:2309.01219, 2023.\n[98] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng\nLiu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in\nmathematical reasoning. arXiv preprint arXiv:2501.07301, 2025.\n16\n\nA Limitations and Future Work\nDespite the strengths of RAG-Gym, several limitations remain. First, our framework relies on\nhigh-quality process reward judgments to supervise intermediate agent actions. Obtaining such\nfine-grained annotations for complex reasoning or domain-specific scenarios can be challenging.\nSecond, as with other reward modeling approaches, there is an inherent risk of reward hacking: agents\nmay learn to exploit imperfections or biases in the reward model, optimizing for the reward signal\nrather than genuine task performance [67, 16]. Third, while our experiments focus on knowledge-\nintensive question answering, the generalizability of RAG-Gym to other task types (e.g., dialogue,\nsummarization, or planning) remains to be systematically evaluated.\nWhile RAG-Gym serves as a pilot study of when and how process supervision works for agentic\nRAG, several promising directions remain for future work. First, developing more scalable and\ncost-effective annotation strategies for process reward modeling is essential, especially for complex or\nspecialized domains. Since existing rollout-based methods such as Math-Shepherd [77] did not yield\nsignificant gains in our experiments (Table 4), new approaches are needed to facilitate efficient and\nhigh-quality process reward collection. Second, the design and training of process reward judges can\nbe further refined to improve robustness and reduce susceptibility to reward hacking. Third, extending\nRAG-Gym to a broader range of agentic tasks beyond knowledge-intensive question answering such\nas dialogue will help assess its generalizability and reveal new challenges, particularly in settings\nwhere outcome rewards are ambiguous and process supervision is even more critical.\nB Broader Impacts\nRAG-Gym systematically evalutes different optimization approaches for retrieval-augmented lan-\nguage agents, which has the potential for wide-ranging societal benefits and risks. By enabling\nhigh-quality intermediate steps with process-level supervsion, our framework can improve the relia-\nbility of AI assistants in knowledge-intensive domains such as education [72, 41], healthcare [48, 86],\nscientific research [20, 39, 68], and legal analysis [ 83, 53]. Also, process-level actor tuning and\ncritic-guided inference may help reduce hallucinations and increase transparency, supporting more\ntrustworthy AI deployments.\nHowever, these advances also raise important considerations. The reliance on high-quality process\nreward annotations may introduce biases if the annotation sources are not representative or contain\nsystematic errors. Reward hacking remains a risk, as agents may learn to exploit weaknesses in the\nreward model, potentially leading to unintended behaviors or misinformation.\nC Dataset Descriptions\nIn this section, we provide detailed descriptions of the datasets used in our experiments, including\nHotpotQA [90], 2WikiMultihopQA [21], Bamboogle [54], and MedQA [34].\nHotpotQA. HotpotQA is a large-scale, multi-hop question-answering dataset that requires rea-\nsoning across multiple documents. It consists of questions that explicitly demand retrieving and\nsynthesizing information from different sources. The dataset provides both distractor and supporting\ndocuments, allowing evaluation of models\u2019 ability to filter relevant information effectively. As the\nanswers to the test questions in HotpotQA are not publicly available we took a subsample from its\nvalidation set (7,405 instances) as previous research did [91, 44]. The last 1,000 validation questions\nwere selected for the agent evaluation on HotpotQA. The first 1,000 questions were used as the\ntraining data for process supervision.\n2WikiMultihopQA. 2WikiMultihopQA is another multi-hop question-answering dataset con-\nstructed from Wikipedia. 2WikiMultihopQA focuses on high-quality reasoning paths by selecting\nsupporting documents more systematically. The dataset contains questions that require reasoning\nacross different Wikipedia pages, ensuring a diverse range of factual and inferential challenges. The\nlast 1000 questions in the development set (12,576 question in total) were used for agent evaluation.\nBamboogle. Bamboogle is a manually constructed dataset designed to evaluate compositional\nreasoning and adversarial robustness. It consists of 2-hop questions written by researchers, where\n17\n\nboth supporting facts exist in Wikipedia but are structured to be challenging for retrieval-based\nsystems. Unlike automatically generated datasets like 2WikiMultihopQA and Musique, Bamboogle\nquestions do not follow fixed templates, increasing their variability. We used the whole test set with\n125 questions for the evaluation of agents on Bamboogle.\nMedQA. MedQA is a medical question-answering dataset sourced from professional medical\nexams such as the USMLE (United States Medical Licensing Examination). It requires domain-\nspecific knowledge and reasoning to answer multiple-choice medical questions. We focused on the\nEnglish split of MedQA with 1,273 USMLE-style test questions. A subset of 1,000 questions was\nsampled from the training set (10,178 questions) for the optimization of various agents.\nD Baseline Descriptions\nHere are the detailed descriptions of various baseline agents that we implemented in the experiments.\nDirect. The Direct agent represents the simplest baseline, where the language model is prompted\nto output the predicted answer immediately, without any explicit intermediate reasoning or search\nsteps. This approach tests the model\u2019s ability to answer questions in a single step, relying solely on\nits internal knowledge and without leveraging external retrieval or multi-step reasoning.\nCoT [81]. The Chain-of-Thought (CoT) agent encourages the model to generate a step-by-step\nreasoning process before producing the final answer, but still does so in a single iteration. The agent\nis prompted to articulate its reasoning explicitly, which can help with complex questions by making\nthe model\u2019s thought process transparent and potentially improving answer accuracy. However, CoT\ndoes not incorporate external retrieval or iterative search.\nRAG [42]. The Retrieval-Augmented Generation (RAG) agent augments the language model with a\nretrieval step. At the first iteration, the agent issues the original question as a search query to retrieve\nrelevant documents. In the subsequent step, it reasons about the updated state, which includes the\nretrieved information, and generates a predicted answer. This approach leverages external knowledge\nbut does not perform multi-hop or iterative search.\nReAct [91]. The ReAct agent combines reasoning and acting by allowing the model to interleave\nnatural language reasoning with actions, such as issuing search queries or providing answers. At each\nstep, the agent reasons about the current state and decides whether to search for more information or\nto answer the question. This enables multi-step, interactive information-seeking and supports more\ncomplex reasoning chains.\nSearch-o1 [44]. The Search-o1 agent extends the ReAct framework by introducing a knowledge\nsummarization step before reasoning. For each search query, the agent reasons about the retrieved\ndocuments and briefly summarize the useful information as the direct answer to the search query,\nforming query-answer pairs that are used as input for subsequent reasoning steps. This approach\nreplaces the use of raw documents with structured summaries, potentially improving reasoning\nefficiency. Search-o1 can be viewed as a special case of ReAct where retrieval is performed via RAG\nand the agent operates on summarized knowledge rather than full documents.\nE Implementation Details\nIn our experiments, we selected Llama-3.1-8B-Instruct [13] as the base LLM for the implementations\nof various language agents, due to its context length of 128k tokens and its availability of open-source\nparameters. The critic is also trained based on the same Llama-3.1-8B-Instruct, same as the actor.\nWe involved GPT-4o-mini and Qwen-2.5-7B-Instruct to show the transferability of the trained critic\n(Figure 2) and the generalizability of RAG-Gym (Table 3) to other LLMs.\nE.1 Details of Process Data Collection\nTo evaluate intermediate reasoning and search steps in RAG-Gym, we design a process reward\nfunction that assesses queries based on three key criteria:\n18\n\n\u2022 If the retrieval history already contains sufficient information, answering should be the preferred\naction instead of searching further.\n\u2022 Queries should also be precise, actionable, and foundational to solving the question while avoiding\nunnecessary details.\n\u2022 Queries should introduce new, useful information rather than repeating past searches.\nThese criteria ensure that queries are efficient, targeted, and contribute meaningfully to constructing\nthe final answer.\nThe data collection pipeline begins with trajectory sampling, where the language agent generates a se-\nquence of actions based on its current policy. At each step in the trajectory, multiple candidate actions\nare proposed, and the best action is selected according to predefined evaluation criteria. To streamline\nthe annotation process and ensure consistency, we employ a ranking-based evaluation framework\nrather than assigning numerical scores. The selected action is then executed, and the trajectory\ntransitions to the next state. This process is repeated iteratively until the trajectory terminates.\nTo ensure quality, only sampled trajectories that result in a correct final answer are retained, as\ndetermined by the outcome reward. This filtering guarantees that the selected actions not only align\nwith the process reward criteria but also contribute to successful task completion. To address the\nchallenges of slow and costly human annotation, we leverage LLMs such as GPT-4o to annotate\nthe sampled trajectories. As demonstrated in our experiments (Table 4), annotations generated by\nGPT-4o exhibit high reliability, closely aligning with domain expert judgments. This approach\nenables scalable and efficient data collection, making it feasible to gather high-quality process reward\ndata at scale.\nFigure 5: Pipeline of the process data collection in RAG-Gym. Process reward data is collected by\nrandomly sampling action candidates at each time step and using an external annotator (e.g., GPT-4o)\nto select the best one. The episode is terminated when the agent generates a final answer.\nFor the implementation of the IR environment, we select Wikipedia as the supporting corpus for the\nretrieval of relevant information for questions from HotpotQA, 2WikiMultihopQA, and Bamboogle.\nFor the environment of solving MedQA questions, we use a combination of medical textbooks and\nStatPearls which were pre-processed in MedRAG [ 85]. For all tasks, we used both lexical and\nsemantic retrievers whose results were merged with Reciprocal Rank Fusion [11]. BM25 [58] and\nBGE-Base [84] were used for HotpotQA, 2WikiMultihopQA, and Bamboogle, while in MedQA, we\nselected BM25 and MedCPT [35]. A set of 32 documents will be retrieved for each search query.\nE.2 Details of LLM Post-training\nFor the actor tuning, we employed Low-Rank Adaptation (LoRA) [ 23] in the implementaion of\nsupervised fine-tuning (SFT) [52] and direct preference optimization (DPO) [56] [60] with r = 256\nand alpha = 512on all attention components in the transformers architecture [74]. SFT and DPO\n19\n\nwere implemented using the TRL package [75]. For proximal policy optimization (PPO), we used\nthe OpenRLHF package [24] with full-parameter tuning. Detailed hyperparameter settings for SFT,\nDPO, and PPO can be found in our source code. For the tuning of Search-o1 and Re2Search agents,\nonly the LLM for action reasoning is trained while the one for history knowledge summarization\nremains untuned.\nE.3 Details of LLM Inference\nAll results of zero-shot learning (ZSL), supervised fine-tuning (SFT), direct preference optimization\n(DPO), and proximal policy optimization (PPO) are generated with a temperature of 0.0. For the\nevaluation of agents with a critic, we employed a temperature of 1.0 with 10 different actions sampled\nfor each step in the information-seeking trajectory. Algorithm 1 presents our algorithm of using the\ntrained process reward model to guide the action selection during inference. All experiments were\nconducted on NVIDIA A100 and A6000 GPUs.\nAlgorithm 1 PRM-Guided Inference with Best-of-N Selection\n1. Input: Original question Q, actor \u03c0\u03b8, critic r\u03d5, number of candidate actions N, maximum steps\nT, information retrieval function IR.\n2. Initialize state S \u2190 (Q, H1 = \u2205).\n3. For t = 1to T:\n(a) Generate N candidate actions: aq, \u00b7\u00b7\u00b7 , aN \u223c \u03c0f(\u03b8)(\u00b7|S).\n(b) Compute process rewards and select the best action: a\u2217 \u2190 arg maxa\u2208{a1,\u00b7\u00b7\u00b7,aN} r\u03d5(S, a).\n(c) If a\u2217 is a search query:\ni. Retrieve documents: D \u2190 IR(a\u2217).\nii. Update state: S \u2190 (Q, Ht+1 = Ht \u222a {(a\u2217, D)}).\n(d) If a\u2217 is a final answer:\ni. Return a\u2217 and terminate the process.\n4. End For\nF Study on the Number of Search Queries\nIn addition to the results presented in Table 2, we further analyzed the number of search queries\ngenerated by Re2Search agents across different datasets. Table 5 reports the minimum, maximum,\nand mean number of search queries issued. The maximum value is capped at 10, reflecting the upper\nlimit of iterations allowed per question in our experiments. The results show that tuned agents (SFT,\nDPO, and PPO) consistently generate more search queries than the zero-shot agent (ZSL), indicating\nthat fine-tuning encourages more extensive information-seeking behavior, which aligns with their\nimproved performance.\nTable 5: Minimum, maximum, and mean number of search queries generated by Re2Search agents\nfor each dataset.\nHotpotQA 2Wiki Bamboogle MedQA\nMin Max Mean Min Max Mean Min Max Mean Min Max Mean\nZSL 0.0 9.0 1.5 0.0 9.0 3.4 0.0 9.0 1.0 0.0 9.0 0.4\nSFT 0.0 9.0 2.1 0.0 9.0 3.8 0.0 9.0 1.9 0.0 9.0 0.6\nDPO 0.0 9.0 3.2 0.0 9.0 4.5 0.0 9.0 3.4 0.0 9.0 2.2\nPPO 0.0 9.0 4.6 0.0 9.0 5.6 0.0 9.0 2.7 0.0 9.0 5.6\n20\n\nG Case Studies\nG.1 Comparison of Agent Designs on Bamboogle\nWe analyze the reasoning and search behaviors of RAG, ReAct, Search-o1, and Re2Search using an\nexample from the Bamboogle dataset. As shown in Figure 7, given the question \u201cWhat was the father\nof the last surviving Canadian father of Confederation?\", the three agents show distinct behaviors\nwhen generating the first action.\nThe RAG agent directly passes the question as a search query without decomposition, relying entirely\non retrieval to infer the answer. This often leads to ineffective searches that fail to retrieve necessary\nintermediate facts. ReAct and Search-o1 improve upon this by engaging in stepwise query reasoning,\nfirst identifying the need to determine the last surviving Canadian father of Confederation before\nissuing a search query. However, the generated query, \u201cList of Canadian fathers of Confederation\u201d,\nretrieves broad information rather than directly resolving the missing knowledge.\nIn contrast, Re2Search explicitly integrates answer reasoning with search. It first constructs a potential\nanswer, identifying an unverified claim that William Lyon Mackenzie King is among the last surviving\nCanadian fathers of Confederation. Recognizing the missing evidence, it formulates a targeted query,\n\u201cWho is the last surviving Canadian father of Confederation?\u201d, to resolve the uncertainty. This\napproach ensures that retrieval is aligned with answer construction, minimizing unnecessary queries\nand improving information efficiency. The case study illustrates how Re2Search effectively refines\nthe search process by linking query generation to specific knowledge gaps.\nanswer reasoning  & reflection\n(a) RAG (b) ReAct / Search-o1 (c) ReSearch\n1.  Need to identify the\nlast surviving Canadian\nfather of Confederation.\n2.  Start by searching for\nthe list of Canadian\nfathers of Confederation.\nQuery: List of Canadian\nfathers of Confederation\n1. William Lyon\nMackenzie King is among\nthe last Canadian father\nof Confederation\n2. Mackenzie King's\nfather was James\nMackenzie\nQuery: Who is the last\nsurviving Canadian father\nof Confederation?\nQuestion: What was the father of\nthe last surviving Canadian\nfather of Confederation?\nQuestion: What was the father of\nthe last surviving Canadian\nfather of Confederation?\nQuestion: What was the father of\nthe last surviving Canadian\nfather of Confederation?\nQuery: What was the\nfather of the last surviving\nCanadian father of\nConfederation?\nquery reasoning\nstep 1\nstep 2\nstep 1\nstep 2\nquery generation query generation\ndirect pass\nFigure 6: Comparison of different agent architectures in handling a multi-hop question from Bam-\nboogle.\nG.2 Comparison of Agent Designs on MedQA\nSimilarly, when presented with a complex medical question from MedQA, the distinct approaches of\nthe agents are evident. The RAG agent, as before, directly uses a truncated version of the lengthy\ninput as its search query, which is unlikely to yield specific, actionable information. ReAct and\nSearch-o1 engage in query reasoning, first hypothesizing that these symptoms suggest a possible\ndiagnosis of serotonin syndrome and then deciding to search for information on the treatment of\nserotonin syndrome. While this is more targeted than RAG, Re2Search demonstrates a more refined\nprocess by engaging in answer reasoning and reasoning reflection. It posits that the symptoms are\nsuggestive of a cholinergic syndrome. Recognizing the need to confirm the relationship between the\npatient\u2019s existing conditions and the suspected syndrome, it generates a highly specific query about\nthe relationship between constipation, fibromyalgia, and cholinergic syndrome. This demonstrates\nRe2Search\u2019s ability to align its search strategy with the nuances of constructing a well-supported\nanswer, thereby improving the precision of its information retrieval in a complex diagnostic scenario.\n21\n\n(a) RAG (b) ReAct / Search-o1\nthese symptoms ... suggest\na possible diagnosis of\nserotonin syndrome.\nQuery: What are the appropriate\ntreatments for serotonin\nsyndrome?\nThese symptoms ... are\nsuggestive of a cholinergic\nsyndrome\nQuery: What is the relationship\nbetween constipation,\nfibromyalgia, and cholinergic\nsyndrome?\nQuestion: A 25-year-old man with a past medical history of constipation and fibromyalgia ... generalized\nmalaise and severe diarrhea ... rhinorrhea, lacrimation, and piloerection ... pupils are dilated ... scars are\nnoted in the antecubital fossa ... Which could be an appropriate treatment for this patient's symptoms?\nQuery: A 25-year-old man ...\nquery reasoning\nstep 1\nstep 2\nanswer reasoning  & reflection\nstep 1\nstep 2\nquery generation query generation\ndirect pass\nwe need to search for\ninformation on the\ntreatment of serotonin\nsyndrome\nthe most appropriate\ntreatment ... an\nanticholinergic medication\n(c) ReSearch\nFigure 7: Comparison of different agent architectures in handling a multi-hop question from MedQA.\nG.3 Critic-Guided Action Selection in MedQA\nTo illustrate how the trained critic improves decision-making, we examine a case from the MedQA\ndataset (Table 6). The model is tasked with identifying the mechanism of action of the most likely\nanti-diabetic medication for a 60-year-old patient presenting with symptoms indicative of fluid\noverload.\nFor the generation of the first action, the agent initially generates a broad and less actionable query\nabout heart failure, which, while relevant, does not directly contribute to identifying the correct\nmedication. Meanwhile, it also provides another candidate action with the query \u201cWhat are common\nclasses of oral anti-diabetic medications?\", which leads to retrieving a structured list of relevant drug\ncategories. The process reward model estimates the quality of these two candidates and identifies the\nsecond candidate as the better action.\nAs the reasoning progresses, the process reward model continues to refine action selection, identifying\nthe best queries to resolve missing information efficiently. The rewarded queries ultimately guide\nthe agent toward correctly inferring that the medication is most likely to be Biguanides (metformin),\nwhich acts by increasing gene expression of GLUT-4. This case demonstrates how process reward\nmodels enhance decision quality by selecting queries that effectively bridge knowledge gaps, leading\nto more precise reasoning and higher answer accuracy.\nH Prompt Templates\nWe provide structured prompt templates for history knowledge summarization and action generation\nin our proposed Re2Search agent. The template in Figure 8 ensures that retrieved documents are\nsummarized concisely and factually for follow-up queries. Figure 9 shows the prompt template used\nby Re2Search to align the answer construction with query formulation. The prompt of using GPT-4o\nfor process reward data annotation is presented in Figure 10.\n22\n\nTable 6: An example of how PRM helps identify high-quality actions that lead to correct answers.\nThe unverified claims in the presented example are highlighted.\nMedQA Question\nQuestion: A 60-year-old woman presents to a physician for worsening shortness of breath and increasing\nabdominal distention over the last 3 months. She says that the shortness of breath is worse on exertion and\nimproves with rest. While she could previously walk to the nearby store for her groceries, she now has to\ndrive because she gets \u201dwinded\u201d on the way. The patient was diagnosed with diabetes 5 years ago and is\ncompliant with her medications. The medical history is otherwise unremarkable. The physical examination\nreveals gross ascites and visibly engorged periumbilical veins. Bilateral pitting edema is noted around\nthe ankles. The finger-prick blood glucose level is 100 mg/dL. What is the mechanism of action of the\nanti-diabetic medication this patient is most likely taking? A. Binding to the alpha subunit of the insulin\nreceptor, B. Closure of ATP-sensitive K-channels in the pancreatic beta-cell,C. Increased gene expression of\nGLUT-4, D. Glucagon-like peptide-1 receptor agonist\nAnswer (invisible): C\nAction 1\n[Candidate 1 \u2013 Reasoning] ...\n[Candidate 1 \u2013 Action]\nQuery: What is the primary concern with the patient\u2019s condition, specifically whether heart failure is\nconfirmed based on symptoms and physical examination findings? /thumbs-down\n[Candidate 2 \u2013 Reasoning]\n1. The patient presents with worsening shortness of breath, increasing abdominal distention, and gross\nascites. These symptoms, along with visibly engorged periumbilical veins and bilateral pitting edema, are\nindicative of fluid overload and congestive heart failure.\n2. The patient\u2019s history of diabetes, which is compliant with medication, suggests that her anti-diabetic\nmedication is likely a type of oral hypoglycemic agent rather than insulin.\n3. Considering the patient\u2019s age (60 years) and symptoms, it is unlikely that she is taking a glucagon-like\npeptide-1 (GLP-1) receptor agonist, which is typically used in younger patients with type 2 diabetes and has\na different side-effect profile.\n4. With the remaining options, \u2019Binding to the alpha subunit of the insulin receptor\u2019 and \u2019Closure of\nATP-sensitive K-channels in the pancreatic beta-cell\u2019, we can narrow down the possibilities. \u2019Binding to the\nalpha subunit of the insulin receptor\u2019 is more characteristic of insulin receptor agonists or insulin itself, which\nis unlikely given the patient\u2019s age and the oral nature of her medication. Therefore,\u2019Closure of ATP-sensitive\nK-channels in the pancreatic beta-cell\u2019 is the most plausible mechanism of action.\n5. This mechanism is consistent with the sulfonylureas class of medications, which are commonly used in\npatients with type 2 diabetes.\n[Candidate 2 \u2013 Action]\nQuery: What are common classes of oral anti-diabetic medications? /thumbs-up\nEnvironment Feedback 1\n[+ Documents]\nCommon classes of oral anti-diabetic medications include: 1. Sulfonylureas, 2. Dipeptidyl Peptidase IV\n(DPP IV) inhibitors (gliptins), 3. Thiazolidinediones, 4. Meglitinides, 5. Sodium-glucose co-transporter 2\n(SGLT 2) inhibitors, 6. Alpha-glucosidase inhibitors, 7. Biguanides (metformin).\nAction 2\n[Candidate 1 \u2013 Reasoning] ...\n[Candidate 1 \u2013 Action]\nQuery: Is metformin commonly prescribed for type 2 diabetes and known to cause lactic acidosis as a side\neffect? /thumbs-down\n[Candidate 2 \u2013 Reasoning] ...\n[Candidate 2 \u2013 Action]\nQuery: What is the mechanism of action of Biguanides (metformin)? /thumbs-up\nEnvironment Feedback 2\n[+ Documents]\nThe mechanism of action of Biguanides (metformin) is to activate the enzyme AMP-activated protein kinase\n(AMPK) and reduce hepatic glucose production.\nAction 3\n[Candidate 1 \u2013 Reasoning] ... [Candidate 1 \u2013 Action] Answer: B /thumbs-down\n[Candidate 2 \u2013 Reasoning] ... [Candidate 2 \u2013 Action] Answer: C /thumbs-up\n23\n\nPrompt template for history knowledge summarization in Search-o1 and Re2Search\nYou are a helpful assistant tasked with answering a follow-up query using the relevant\ndocuments provided.\n### Relevant Documents\n{{documents}}\n### Context\nOriginal question: {{question}}\n### Follow-up Query\n{{query}}\nAnswer the follow-up query succinctly, using only the information from the documents. When\nthe documents do not provide sufficient information, explicitly point this out instead of making\nup facts. Do not include unrelated or excessive details in the response.\nFigure 8: Template used for history knowledge summarization in Search-o1 and Re2Search.\nPrompt template for generating actions using the Re2Search agent\nYou are a helpful assistant. Your task is to answer a given question following user instructions.\u2019\n### Information-seeking History\n{{history}}\n### Original Question\n{{question}}\nYour output must include three sections:\n1. **### Step-by-step Reasoning**:\n- Think step-by-step and then answer the question.\n2. **### Unverified Claim Identification**:\n- Identify if there are claims in the step-by-step reasoning section that are not grounded in the\ninformation-seeking history section.\n- If yes, summarize the first piece of missing information as an atomic query to search in an\nexternal knowledge base.\n- If no, clearly state that no further query is needed.\n3. **### Structured Output**:\n- Present your predicted answer and generated query (if applicable) in the following JSON\nformat:\n\u201c\u2018json\n{\n\u201cpredicted_answer\": \u201cProvide a single letter (for multiple-choice questions), digit, word, or\nshort phrase here.\",\n\u201cgenerated_query\": \u201cProvide an entity, question, or statement to be searched in an external\nknowledge base. Output \\\u201cNone\\\" if no query is generated.\",\n}\n\u201c\u2018\nFigure 9: Template used to generate actions for the Re2Search agent.\n24\n\nPrompt template for ranking candidate actions with GPT-4o\nYou are a decision-evaluation assistant. Your task is to rank the proposed actions from the\nmost appropriate to the least appropriate as the next step in a sequential decision-making\nprocess aimed at solving a given question.\n### Original Question:\n{{question}}\n### Information-Seeking History:\n{{curr_history}}\n### Proposed Next Actions:\n{{actions_text}}\n### Important Assumption\nThe agent has no prior knowledge about the subject matter. It must rely solely on the\ninformation-seeking history provided to evaluate and answer the original question. Assump-\ntions not explicitly supported by the history must not influence the ranking of proposed actions.\n### Evaluation Criteria for Appropriateness\n1. **Sufficiency Check**:\n- Determine whether the available information is sufficient to directly answer the original\nquestion. If not, the proposed action to \u201cAnswer\u201d is inappropriate.\n- Prioritize queries that gather specific, missing information essential to solving the question.\n- If the history already contains all necessary information, then \u201cAnswer\u201d is the most\nappropriate action, and the correct answer should be ranked highest.\n2. **Utility Check**:\n- Queries must be precise, actionable, and directly relevant to solving the question.\n- Prioritize foundational queries that establish critical context or general knowledge necessary\nfor more specific follow-ups.\n- Rank overly narrow or prematurely specific queries lower if they presume knowledge not yet\navailable.\n- Avoid irrelevant queries that do not contribute to solving the original question.\n3. **Redundancy Check**:\n- Queries that duplicate information already covered in the history or repeat previous queries\nshould be ranked lower.\n- Proposed actions must add new value to the decision-making process by seeking new or\nclarifying missing information.\n### Expected Output Format\n- Output the indices of the ranked actions in JSON format: \u201c\u2018json{\u201cranked_indices\u201d: [list of\nindices]}\u201d\u2019.\n- Rank actions from most appropriate to least appropriate based on the evaluation criteria\nabove.\n- Do not provide additional explanations or reasoning.\u201d\u2019\nFigure 10: Template used by GPT-4o to rank action candidates given the state.\n25",
  "full_text_length": 89103,
  "link_pdf": "https://arxiv.org/pdf/2502.13957v2",
  "paper_id": "2502.13957v2"
}