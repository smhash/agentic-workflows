{
  "source": "arxiv",
  "query": "Cycle Generative Adversarial Networks",
  "fetched_at": "2025-11-21T18:06:45.454724",
  "title": "Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently",
  "url": "http://arxiv.org/abs/1702.06762v2",
  "content": "Workshop track - ICLR 2017\nSTYLE TRANSFER GENERATIVE ADVERSARIAL NET-\nWORKS : L EARNING TO PLAY CHESS DIFFERENTLY\nMuthuraman Chidambaram & Yanjun Qi\nDepartment of Computer Science\nUniversity of Virginia\nCharlottesville, V A 22903, USA\n{mc4xf,yanjun}@virginia.edu\nABSTRACT\nThe idea of style transfer has largely only been explored in image-based tasks,\nwhich we attribute in part to the speci\ufb01c nature of loss functions used for style\ntransfer. We propose a general formulation of style transfer as an extension of\ngenerative adversarial networks, by using a discriminator to regularize a genera-\ntor with an otherwise separate loss function. We apply our approach to the task\nof learning to play chess in the style of a speci\ufb01c player, and present empirical\nevidence for the viability of our approach.\n1 I NTRODUCTION\nGatys et al. (2015) showed that a convolutional neural network (CNN) model could be trained to\ntransfer the unique styles present in human art onto other images. However, the style transfer loss\nused in their paper, as well the losses used in the follow-up work of Ulayanov et al. (2016) and\nJohnson et al. (2016), were speci\ufb01c to image-based tasks. This makes it dif\ufb01cult to extend their\nwork on style transfer to other tasks where unique human styles are present, such as playing games.\nMotivated by this problem, we present a general framework for style transfer, which we term style\ntransfer generative adversarial networks (STGANs) as an extension of the generative adversarial\nnetworks (GANs) described by Goodfellow et al. (2014). Our proposed framework consists of a\ngenerator G, which learns to perform a given task, and a discriminator D, which learns to predict\nwhether the same task was performed in a speci\ufb01c style. These two models are trained in an adver-\nsarial fashion by using the discriminator to regularize the generator, so that the generator learns to\nperform the given task in a way that is consistent with the style designated by the discriminator.\nIn this paper, we examine an application of STGANs to the task of learning to play chess in the style\n(made precise in appendix) of a designated player. Essentially, a generator is trained to evaluate\nchess board positions, and is then combined with a search function to generate moves. A discrimi-\nnator is trained to distinguish the moves selected using the generator from the moves of a designated\nplayer, and is used to bias the generator\u2019s evaluations towards the style of the designated player.\n2 STGAN M ODEL\nThe key difference between our proposed STGAN model and the GAN model is that the generator\nloss in our model is not structured purely in terms of the discriminator. Instead, the generator loss\nis de\ufb01ned to be speci\ufb01c to the given task, which we take to be generating optimal chess board\nevaluations in this paper. We then de\ufb01ne a style transfer generator loss by regularizing the original\ngenerator loss using the discriminator.\n2.1 G ENERATOR\nWe structured our generator to be similar to the Deep Pink model described by Bernhardsson (2014).\nThe generator Gis thus a fully connected feedforward neural network with a 768 unit-wide input\nlayer, two 2048 unit-wide hidden layers with ReLU activations, and a single linear output unit. The\ngenerator takes as input a chess board, which is represented as a 768 element vector corresponding\nto the locations of the 12 different chess pieces, and outputs a real number as an evaluation. Positive\n1\narXiv:1702.06762v2  [cs.LG]  7 May 2017\n\nWorkshop track - ICLR 2017\nFigure 1: The generator network learns G(xG) =G(yG), as well as G(yG) > G(rG) if yG corre-\nsponds to a move made by white and G(yG) <G(rG) otherwise.\nevaluations signify that the board is in white\u2019s favor, while negative evaluations signify that the\nboard is in black\u2019s favor. We train the generator using triplets of chess boards (xG,yG,rG) taken\nfrom games played by top chess players, where xG is an initial board, yG is the board after a player\nhas made a move on xG, and rG is the board after a random move has been made on xG. The\nfunction Gis learned such that G(xG) =G(yG) and G(yG) > G(rG) if it is white\u2019s turn to move\nand G(yG) <G(rG) if it is black\u2019s turn to move. The assumption made here is that the generator is\nbeing trained on boards taken from masters\u2019 games, so board evaluations should not change much\nafter a move has been played (neither player gives the other a signi\ufb01cant advantage). Consequently,\na random non-master move is considered to be much worse, so the board evaluation should become\nmore positive if the random move was played by black and more negative if the random move was\nplayed by white. We formulate the generator loss J(G)(\u03b8G) as:\nJ(G)(\u03b8G) =\u22121\nm\nm\u2211\ni=1\n[ log(\u03c3(G(x(i)\nG ) \u2212G(y(i)\nG ))) + log(\u03c3(G(y(i)\nG ) \u2212G(x(i)\nG )))\n+ log(\u03c3(pi(G(y(i)\nG ) \u2212G(r(i)\nG ))))]\n(1)\nWhere m is the batch size, \u03c3 is the sigmoid function, and pi is 1 if it is white\u2019s turn to move\non the input board, and -1 otherwise. The terms log(\u03c3(G(x(i)\nG ) \u2212G(y(i)\nG ))) and log(\u03c3(G(y(i)\nG ) \u2212\nG(x(i)\nG ))) enforce the inequalities G(xG) > G(yG) and G(yG) > G(xG), thereby attempting to\nlearn G(xG) =G(yG). The term log(\u03c3(pi(G(y(i)\nG ) \u2212G(r(i)\nG )))) enforces the inequality G(yG) >\nG(rG) if it is white\u2019s turn to move, andG(yG) <G(rG) if it is black\u2019s turn to move.\n2.2 D ISCRIMINATOR\nThe discriminator, which learns a function D, is set up identically to the generator, save for a 1536\nunit-wide input layer and a sigmoid output. The discriminator takes as input a valid chess move,\nwhich is represented as the concatenation of the vector representations of a pair of boards, and out-\nputs the probability that the move was played by a designated player. Training is done using pairs of\nsequential boards (xD,yD) taken from the games of a designated player, as well as fake move pairs\n(xD,M(xD)) generated by selecting moves using the generator G. The board M(xD) is chosen\nusing the negamax search described by Campbell & Marsland (1983) with a search depth of one\nand the generator as the board evaluation function. The discriminator is optimized by maximizing\nD((xD,yD)) and minimizing D((xD,M(xD)), which corresponds to minimizing the following\ndiscriminator loss J(D)(\u03b8D):\nJ(D)(\u03b8D) =\u22121\nm\nm\u2211\ni=1\nD((x(i)\nD ,y(i)\nD )) + 1\nm\nm\u2211\ni=1\nD((x(i)\nD ,M(x(i)\nD )) (2)\nHere we have opted to structure the discriminator loss after the loss described by Arjovsky et al.\n(2017) for training Wasserstein GANs (WGANs).\n2.3 S TYLE TRANSFER\nStyle transfer is done by using the discriminator to regularize the generator. This is achieved by\nde\ufb01ning a style transfer generator loss J(G)\nST (\u03b8G) as:\n2\n\nWorkshop track - ICLR 2017\nMove Baseline, k= 0 Style Transfer, k= 1 Style Transfer, k= 2\nf8e7 0.710 0.565 0.325\nd7d5 0.703 0.603 0.479\nTable 1: Negamax move evaluations produced by the baseline and style transfer generator networks\n(a) Initial board\n (b) Baseline (f8e7)\n (c) Style (d7d5)\n (d) Tal (d7d5)\nFigure 2: Positions reached after queen\u2019s pawn opening sequence of moves\nJ(G)\nST (\u03b8G) =J(G)(\u03b8G) \u2212 1\nm\nm\u2211\ni=1\nkD((x(i)\nD ,M(x(i)\nD )) (3)\nWhere k is a hyperparameter that controls the level of in\ufb02uence the style designated by the dis-\ncriminator should have on the generator. Since certain boards x(i)\nG may not be represented in the\ndiscriminator\u2019s training data, we choose to use initial boardsx(i)\nD for the regularization term.\n2.4 T RAINING\nThe discriminator and the generator are updated simultaneously by gradient descent on J(D) and\nJ(G)\n(ST ), but the discriminator is updated 5 times for each generator update, as described by Arjovsky\net al. (2017) in the WGAN paper. Examples taken from the most recent training batch of the dis-\ncriminator are used for regularization in each generator update. The discriminator\u2019s weights are also\nclamped to be in the range [\u22120.01,0.01], once again consistent with the WGAN approach.\n3 R ESULTS\nTraining data for the generator was obtained by extracting all standard chess games played in 2016\nbetween players with ratings above 2000 from the FICS games database. For the discriminator, we\nchose to predict the style of late chess grandmaster Mikhail Tal, and extracted his 2431 available\ngames from PGN Mentor as training data.\nWe trained multiple generator networks with varying values of the regularization parameter k, with\nk= 0being treated as the baseline. Due to the cost of having to perform a negamax search for each\ngenerated move during training, all networks were trained for 10 epochs with only 100 batches of\nsize 64 sampled from the training data in each epoch. After training, each network was tested by\ngenerating move sequences (once again using a negamax search with depth one) in response to the\nqueen\u2019s pawn opening sequence of moves.\nFigure 2 shows the positions reached by the generator networks after white plays the queen\u2019s pawn\nopening sequence of moves (d2d4, c2c4, g1f3), as well as an actual position commonly reached by\nTal for the same sequence. The style transfer networks end in the same position as Tal, whereas the\nbaseline network reaches a position never played by Tal within the data. Table 1 shows the difference\nin move evaluations between the networks, and it can be seen that the \ufb01nal Tal move (d7d5) becomes\nmore favored as kis increased. It should be noted that the style transfer networks still learn that the\nmove f8e7 is a good move in the last position (positive negamax evaluation), so they are not simply\nover\ufb01tting to Tal\u2019s moves.\n3\n\nWorkshop track - ICLR 2017\nREFERENCES\nMartin Arjovsky, Soumith Chintala, and L \u00b4eon Bottou. Wasserstein gan, 2017. URL https:\n//arxiv.org/pdf/1701.07875v1.pdf.\nErik Bernhardsson. Deep learning for... chess, 2014. URL https://erikbern.com/2014/\n11/29/deep-learning-for-chess/ .\nMartin S. Campbell and T.A. Marsland. A comparison of minimax tree search algorithms.Arti\ufb01cial\nIntelligence, 20:347\u2013367, 1983.\nLeon A. Gatys, Alexander S. Ecker, and Matthias Bethge. A neural algorithm of artistic style, 2015.\nURL https://arxiv.org/pdf/1508.06576.pdf.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Big Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets.Advances in Neural Processing\nSystems, 27:2672\u20132680, 2014.\nJustin Johnson, Alexandre Alahi, and Fei-Fei Li. Perceptual losses for real-time style transfer and\nsuper-resolution. European Conference on Computer Vision, pp. 694\u2013711, 2016.\nDmitry Ulayanov, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky. Texture networks: Feed-\nforward synthesis of textures and stylized images. Journal of Machine Learning Research , 48,\n2016.\nJohn L. Watson. Queen\u2019s Gambit: Chigorin Defence. B.T. Batsford, 1981.\n4\n\nWorkshop track - ICLR 2017\n4 A PPENDIX\n4.1 B ACKGROUND : G ENERATIVE ADVERSARIAL NETWORKS\nThe STGAN model is an extension of the generative adversarial network (GAN) framework put forth\nby Goodfellow et al. (2014). The GAN framework consists of a discriminatorDwith parameters \u03b8D,\nwhich attempts to determine the probability that an input is real, and a generator Gwith parameters\n\u03b8G, which attempts to generate input data from noise that is intended to \u201cfool\u201d the discriminator.\nG is de\ufb01ned to be a differentiable function which transforms a random sample z drawn from a\nprior distribution into an input sample G(z) for the discriminator. The discriminator function Dis\nthen trained to maximize the probability of input samples xdrawn from a data distribution Pdata\nand minimize the probability of generated input samples G(z) by minimizing the following loss\nJ(D)(\u03b8D):\nJ(D)(\u03b8D) =\u22121\nm\nm\u2211\ni=1\n[log(D(x(i))) + log(1\u2212D(G(z(i))))] (4)\nWhere mcorresponds to the batch size. The generator is trained simultaneously to generate samples\nthat are indistinguishable from actual samples drawn from Pdata by minimizing the loss:\nJ(G)(\u03b8G) = 1\nm\nm\u2211\ni=1\nlog(1 \u2212D(G(z(i)))) (5)\n4.2 D EFINING AND MOTIVATING STYLE TRANSFER\nIt is very dif\ufb01cult to de\ufb01ne \u201dstyles\u201d for games such as chess. While a human can assess a player\nas aggressive or defensive in a qualitative manner, we are not aware of any quantitative metrics\nfor measuring such characteristics in chess. In this paper, we take \u201dstyle\u201d to mean a favoritism\ntowards certain positions, speci\ufb01cally within opening sequences in chess. We then assess style by\nconsidering a common opening sequence of moves for the player with the white pieces and then\nobserving the sequence of response moves played by the style transfer and baseline models. If the\nsequence of response moves falls into an opening repertoire commonly used by the player whose\nstyle we were attempting to transfer, we consider the style transfer to be successful. We choose\nto use a de\ufb01nition of style transfer that is based on opening sequences of moves due to the higher\ndegree of subjectivity in measuring style within the later portions of chess games, which we think\nwould be an interesting area of future research.\nFurthermore, it is also important to understand why style transfer provides value in a game such as\nchess. There exist many chess-playing AIs that can play well above the grandmaster level, but we\nnote that these AIs use a \ufb01xed play style based on tree search and position heuristics. Thus, they\ndo not provide an easy way for players to train against different types of opponents. We believe\nthat successfully emulating the style of speci\ufb01c players would provide signi\ufb01cant pedagogical value\nto those looking to further their chess skills, as they would be able to practice against a variety of\ndifferent openings and preferred styles. We use style transfer as opposed to simply training a neural\nnetwork on the games of an individual player due to the fact that an individual player\u2019s game data\ndoes not typically cover enough board positions to train a pro\ufb01cient model (i.e. the model plays very\npoor moves in positions not represented in its data).\nFinally, while our example use case of the STGAN framework is chess, we believe that the frame-\nwork can be applied to other domains. For example, for image style transfer, one could construct\nan STGAN in which the generator is an image generation model and the discriminator is trained on\nimages corresponding to a speci\ufb01c style (i.e. Picasso).\n4.3 F URTHER EXPERIMENTS : M IKHAIL CHIGORIN\nWhile our main results focused on transferring the style of the famous chess player Mikhail Tal\nto our baseline model, we also ran experiments with another notable player: Mikhail Chigorin.\n5\n\nWorkshop track - ICLR 2017\n(a) 1. d2d4 g8f6\n (b) 2. c2c4 b8c6\n (c) 3. g1f3 d7d5\nFigure 3: Response move sequence played by Chigorin style transfer model\n(a) 1. d2d4 d7d5\n (b) 2. c2c4 b8c6\n (c) 3. g1f3 c8g4\nFigure 4: Classical sequence of the Chigorin defense\nChigorin is famous for popularizing a speci\ufb01c response to the queen\u2019s pawn opening in chess, which\nis the eponymous Chigorin defense (Watson, 1981). Similar to the approach we used for Tal, we\nextracted Chigorin\u2019s 688 available games from PGN Mentor and used them as training data for the\ndiscriminator in the STGAN framework.\nGiven the classical queen\u2019s pawn opening move sequence described in the main results section, the\nChigorin style transfer model ( k = 1) responded with the move sequence shown in Figure 3. The\nclassical sequence of moves played in the Chigorin defense is shown in Figure 4. It can be seen that\nthe \ufb01nal position reached by the style transfer model represents a variation of the Chigorin defense,\nwhich seems to have been a result of the baseline model heavily favoring the \ufb01rst move g8f6 due to\nits representation in the master games used for training.\n6",
  "authors": [
    "Muthuraman Chidambaram",
    "Yanjun Qi"
  ],
  "summary": "The idea of style transfer has largely only been explored in image-based tasks, which we attribute in part to the specific nature of loss functions used for style transfer. We propose a general formulation of style transfer as an extension of generative adversarial networks, by using a discriminator to regularize a generator with an otherwise separate loss function. We apply our approach to the task of learning to play chess in the style of a specific player, and present empirical evidence for the viability of our approach.",
  "pdf_url": "https://arxiv.org/pdf/1702.06762v2",
  "entry_id": "http://arxiv.org/abs/1702.06762v2",
  "published": "2017-02-22",
  "updated": "2017-05-07",
  "comment": "style transfer, Generative Adversarial Networks",
  "journal_ref": null,
  "doi": null,
  "primary_category": "cs.LG",
  "categories": [
    "cs.LG"
  ],
  "links": [
    {
      "href": "https://arxiv.org/abs/1702.06762v2",
      "rel": "alternate",
      "title": null
    },
    {
      "href": "https://arxiv.org/pdf/1702.06762v2",
      "rel": "related",
      "title": "pdf"
    }
  ],
  "full_text": "Workshop track - ICLR 2017\nSTYLE TRANSFER GENERATIVE ADVERSARIAL NET-\nWORKS : L EARNING TO PLAY CHESS DIFFERENTLY\nMuthuraman Chidambaram & Yanjun Qi\nDepartment of Computer Science\nUniversity of Virginia\nCharlottesville, V A 22903, USA\n{mc4xf,yanjun}@virginia.edu\nABSTRACT\nThe idea of style transfer has largely only been explored in image-based tasks,\nwhich we attribute in part to the speci\ufb01c nature of loss functions used for style\ntransfer. We propose a general formulation of style transfer as an extension of\ngenerative adversarial networks, by using a discriminator to regularize a genera-\ntor with an otherwise separate loss function. We apply our approach to the task\nof learning to play chess in the style of a speci\ufb01c player, and present empirical\nevidence for the viability of our approach.\n1 I NTRODUCTION\nGatys et al. (2015) showed that a convolutional neural network (CNN) model could be trained to\ntransfer the unique styles present in human art onto other images. However, the style transfer loss\nused in their paper, as well the losses used in the follow-up work of Ulayanov et al. (2016) and\nJohnson et al. (2016), were speci\ufb01c to image-based tasks. This makes it dif\ufb01cult to extend their\nwork on style transfer to other tasks where unique human styles are present, such as playing games.\nMotivated by this problem, we present a general framework for style transfer, which we term style\ntransfer generative adversarial networks (STGANs) as an extension of the generative adversarial\nnetworks (GANs) described by Goodfellow et al. (2014). Our proposed framework consists of a\ngenerator G, which learns to perform a given task, and a discriminator D, which learns to predict\nwhether the same task was performed in a speci\ufb01c style. These two models are trained in an adver-\nsarial fashion by using the discriminator to regularize the generator, so that the generator learns to\nperform the given task in a way that is consistent with the style designated by the discriminator.\nIn this paper, we examine an application of STGANs to the task of learning to play chess in the style\n(made precise in appendix) of a designated player. Essentially, a generator is trained to evaluate\nchess board positions, and is then combined with a search function to generate moves. A discrimi-\nnator is trained to distinguish the moves selected using the generator from the moves of a designated\nplayer, and is used to bias the generator\u2019s evaluations towards the style of the designated player.\n2 STGAN M ODEL\nThe key difference between our proposed STGAN model and the GAN model is that the generator\nloss in our model is not structured purely in terms of the discriminator. Instead, the generator loss\nis de\ufb01ned to be speci\ufb01c to the given task, which we take to be generating optimal chess board\nevaluations in this paper. We then de\ufb01ne a style transfer generator loss by regularizing the original\ngenerator loss using the discriminator.\n2.1 G ENERATOR\nWe structured our generator to be similar to the Deep Pink model described by Bernhardsson (2014).\nThe generator Gis thus a fully connected feedforward neural network with a 768 unit-wide input\nlayer, two 2048 unit-wide hidden layers with ReLU activations, and a single linear output unit. The\ngenerator takes as input a chess board, which is represented as a 768 element vector corresponding\nto the locations of the 12 different chess pieces, and outputs a real number as an evaluation. Positive\n1\narXiv:1702.06762v2  [cs.LG]  7 May 2017\n\nWorkshop track - ICLR 2017\nFigure 1: The generator network learns G(xG) =G(yG), as well as G(yG) > G(rG) if yG corre-\nsponds to a move made by white and G(yG) <G(rG) otherwise.\nevaluations signify that the board is in white\u2019s favor, while negative evaluations signify that the\nboard is in black\u2019s favor. We train the generator using triplets of chess boards (xG,yG,rG) taken\nfrom games played by top chess players, where xG is an initial board, yG is the board after a player\nhas made a move on xG, and rG is the board after a random move has been made on xG. The\nfunction Gis learned such that G(xG) =G(yG) and G(yG) > G(rG) if it is white\u2019s turn to move\nand G(yG) <G(rG) if it is black\u2019s turn to move. The assumption made here is that the generator is\nbeing trained on boards taken from masters\u2019 games, so board evaluations should not change much\nafter a move has been played (neither player gives the other a signi\ufb01cant advantage). Consequently,\na random non-master move is considered to be much worse, so the board evaluation should become\nmore positive if the random move was played by black and more negative if the random move was\nplayed by white. We formulate the generator loss J(G)(\u03b8G) as:\nJ(G)(\u03b8G) =\u22121\nm\nm\u2211\ni=1\n[ log(\u03c3(G(x(i)\nG ) \u2212G(y(i)\nG ))) + log(\u03c3(G(y(i)\nG ) \u2212G(x(i)\nG )))\n+ log(\u03c3(pi(G(y(i)\nG ) \u2212G(r(i)\nG ))))]\n(1)\nWhere m is the batch size, \u03c3 is the sigmoid function, and pi is 1 if it is white\u2019s turn to move\non the input board, and -1 otherwise. The terms log(\u03c3(G(x(i)\nG ) \u2212G(y(i)\nG ))) and log(\u03c3(G(y(i)\nG ) \u2212\nG(x(i)\nG ))) enforce the inequalities G(xG) > G(yG) and G(yG) > G(xG), thereby attempting to\nlearn G(xG) =G(yG). The term log(\u03c3(pi(G(y(i)\nG ) \u2212G(r(i)\nG )))) enforces the inequality G(yG) >\nG(rG) if it is white\u2019s turn to move, andG(yG) <G(rG) if it is black\u2019s turn to move.\n2.2 D ISCRIMINATOR\nThe discriminator, which learns a function D, is set up identically to the generator, save for a 1536\nunit-wide input layer and a sigmoid output. The discriminator takes as input a valid chess move,\nwhich is represented as the concatenation of the vector representations of a pair of boards, and out-\nputs the probability that the move was played by a designated player. Training is done using pairs of\nsequential boards (xD,yD) taken from the games of a designated player, as well as fake move pairs\n(xD,M(xD)) generated by selecting moves using the generator G. The board M(xD) is chosen\nusing the negamax search described by Campbell & Marsland (1983) with a search depth of one\nand the generator as the board evaluation function. The discriminator is optimized by maximizing\nD((xD,yD)) and minimizing D((xD,M(xD)), which corresponds to minimizing the following\ndiscriminator loss J(D)(\u03b8D):\nJ(D)(\u03b8D) =\u22121\nm\nm\u2211\ni=1\nD((x(i)\nD ,y(i)\nD )) + 1\nm\nm\u2211\ni=1\nD((x(i)\nD ,M(x(i)\nD )) (2)\nHere we have opted to structure the discriminator loss after the loss described by Arjovsky et al.\n(2017) for training Wasserstein GANs (WGANs).\n2.3 S TYLE TRANSFER\nStyle transfer is done by using the discriminator to regularize the generator. This is achieved by\nde\ufb01ning a style transfer generator loss J(G)\nST (\u03b8G) as:\n2\n\nWorkshop track - ICLR 2017\nMove Baseline, k= 0 Style Transfer, k= 1 Style Transfer, k= 2\nf8e7 0.710 0.565 0.325\nd7d5 0.703 0.603 0.479\nTable 1: Negamax move evaluations produced by the baseline and style transfer generator networks\n(a) Initial board\n (b) Baseline (f8e7)\n (c) Style (d7d5)\n (d) Tal (d7d5)\nFigure 2: Positions reached after queen\u2019s pawn opening sequence of moves\nJ(G)\nST (\u03b8G) =J(G)(\u03b8G) \u2212 1\nm\nm\u2211\ni=1\nkD((x(i)\nD ,M(x(i)\nD )) (3)\nWhere k is a hyperparameter that controls the level of in\ufb02uence the style designated by the dis-\ncriminator should have on the generator. Since certain boards x(i)\nG may not be represented in the\ndiscriminator\u2019s training data, we choose to use initial boardsx(i)\nD for the regularization term.\n2.4 T RAINING\nThe discriminator and the generator are updated simultaneously by gradient descent on J(D) and\nJ(G)\n(ST ), but the discriminator is updated 5 times for each generator update, as described by Arjovsky\net al. (2017) in the WGAN paper. Examples taken from the most recent training batch of the dis-\ncriminator are used for regularization in each generator update. The discriminator\u2019s weights are also\nclamped to be in the range [\u22120.01,0.01], once again consistent with the WGAN approach.\n3 R ESULTS\nTraining data for the generator was obtained by extracting all standard chess games played in 2016\nbetween players with ratings above 2000 from the FICS games database. For the discriminator, we\nchose to predict the style of late chess grandmaster Mikhail Tal, and extracted his 2431 available\ngames from PGN Mentor as training data.\nWe trained multiple generator networks with varying values of the regularization parameter k, with\nk= 0being treated as the baseline. Due to the cost of having to perform a negamax search for each\ngenerated move during training, all networks were trained for 10 epochs with only 100 batches of\nsize 64 sampled from the training data in each epoch. After training, each network was tested by\ngenerating move sequences (once again using a negamax search with depth one) in response to the\nqueen\u2019s pawn opening sequence of moves.\nFigure 2 shows the positions reached by the generator networks after white plays the queen\u2019s pawn\nopening sequence of moves (d2d4, c2c4, g1f3), as well as an actual position commonly reached by\nTal for the same sequence. The style transfer networks end in the same position as Tal, whereas the\nbaseline network reaches a position never played by Tal within the data. Table 1 shows the difference\nin move evaluations between the networks, and it can be seen that the \ufb01nal Tal move (d7d5) becomes\nmore favored as kis increased. It should be noted that the style transfer networks still learn that the\nmove f8e7 is a good move in the last position (positive negamax evaluation), so they are not simply\nover\ufb01tting to Tal\u2019s moves.\n3\n\nWorkshop track - ICLR 2017\nREFERENCES\nMartin Arjovsky, Soumith Chintala, and L \u00b4eon Bottou. Wasserstein gan, 2017. URL https:\n//arxiv.org/pdf/1701.07875v1.pdf.\nErik Bernhardsson. Deep learning for... chess, 2014. URL https://erikbern.com/2014/\n11/29/deep-learning-for-chess/ .\nMartin S. Campbell and T.A. Marsland. A comparison of minimax tree search algorithms.Arti\ufb01cial\nIntelligence, 20:347\u2013367, 1983.\nLeon A. Gatys, Alexander S. Ecker, and Matthias Bethge. A neural algorithm of artistic style, 2015.\nURL https://arxiv.org/pdf/1508.06576.pdf.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Big Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets.Advances in Neural Processing\nSystems, 27:2672\u20132680, 2014.\nJustin Johnson, Alexandre Alahi, and Fei-Fei Li. Perceptual losses for real-time style transfer and\nsuper-resolution. European Conference on Computer Vision, pp. 694\u2013711, 2016.\nDmitry Ulayanov, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky. Texture networks: Feed-\nforward synthesis of textures and stylized images. Journal of Machine Learning Research , 48,\n2016.\nJohn L. Watson. Queen\u2019s Gambit: Chigorin Defence. B.T. Batsford, 1981.\n4\n\nWorkshop track - ICLR 2017\n4 A PPENDIX\n4.1 B ACKGROUND : G ENERATIVE ADVERSARIAL NETWORKS\nThe STGAN model is an extension of the generative adversarial network (GAN) framework put forth\nby Goodfellow et al. (2014). The GAN framework consists of a discriminatorDwith parameters \u03b8D,\nwhich attempts to determine the probability that an input is real, and a generator Gwith parameters\n\u03b8G, which attempts to generate input data from noise that is intended to \u201cfool\u201d the discriminator.\nG is de\ufb01ned to be a differentiable function which transforms a random sample z drawn from a\nprior distribution into an input sample G(z) for the discriminator. The discriminator function Dis\nthen trained to maximize the probability of input samples xdrawn from a data distribution Pdata\nand minimize the probability of generated input samples G(z) by minimizing the following loss\nJ(D)(\u03b8D):\nJ(D)(\u03b8D) =\u22121\nm\nm\u2211\ni=1\n[log(D(x(i))) + log(1\u2212D(G(z(i))))] (4)\nWhere mcorresponds to the batch size. The generator is trained simultaneously to generate samples\nthat are indistinguishable from actual samples drawn from Pdata by minimizing the loss:\nJ(G)(\u03b8G) = 1\nm\nm\u2211\ni=1\nlog(1 \u2212D(G(z(i)))) (5)\n4.2 D EFINING AND MOTIVATING STYLE TRANSFER\nIt is very dif\ufb01cult to de\ufb01ne \u201dstyles\u201d for games such as chess. While a human can assess a player\nas aggressive or defensive in a qualitative manner, we are not aware of any quantitative metrics\nfor measuring such characteristics in chess. In this paper, we take \u201dstyle\u201d to mean a favoritism\ntowards certain positions, speci\ufb01cally within opening sequences in chess. We then assess style by\nconsidering a common opening sequence of moves for the player with the white pieces and then\nobserving the sequence of response moves played by the style transfer and baseline models. If the\nsequence of response moves falls into an opening repertoire commonly used by the player whose\nstyle we were attempting to transfer, we consider the style transfer to be successful. We choose\nto use a de\ufb01nition of style transfer that is based on opening sequences of moves due to the higher\ndegree of subjectivity in measuring style within the later portions of chess games, which we think\nwould be an interesting area of future research.\nFurthermore, it is also important to understand why style transfer provides value in a game such as\nchess. There exist many chess-playing AIs that can play well above the grandmaster level, but we\nnote that these AIs use a \ufb01xed play style based on tree search and position heuristics. Thus, they\ndo not provide an easy way for players to train against different types of opponents. We believe\nthat successfully emulating the style of speci\ufb01c players would provide signi\ufb01cant pedagogical value\nto those looking to further their chess skills, as they would be able to practice against a variety of\ndifferent openings and preferred styles. We use style transfer as opposed to simply training a neural\nnetwork on the games of an individual player due to the fact that an individual player\u2019s game data\ndoes not typically cover enough board positions to train a pro\ufb01cient model (i.e. the model plays very\npoor moves in positions not represented in its data).\nFinally, while our example use case of the STGAN framework is chess, we believe that the frame-\nwork can be applied to other domains. For example, for image style transfer, one could construct\nan STGAN in which the generator is an image generation model and the discriminator is trained on\nimages corresponding to a speci\ufb01c style (i.e. Picasso).\n4.3 F URTHER EXPERIMENTS : M IKHAIL CHIGORIN\nWhile our main results focused on transferring the style of the famous chess player Mikhail Tal\nto our baseline model, we also ran experiments with another notable player: Mikhail Chigorin.\n5\n\nWorkshop track - ICLR 2017\n(a) 1. d2d4 g8f6\n (b) 2. c2c4 b8c6\n (c) 3. g1f3 d7d5\nFigure 3: Response move sequence played by Chigorin style transfer model\n(a) 1. d2d4 d7d5\n (b) 2. c2c4 b8c6\n (c) 3. g1f3 c8g4\nFigure 4: Classical sequence of the Chigorin defense\nChigorin is famous for popularizing a speci\ufb01c response to the queen\u2019s pawn opening in chess, which\nis the eponymous Chigorin defense (Watson, 1981). Similar to the approach we used for Tal, we\nextracted Chigorin\u2019s 688 available games from PGN Mentor and used them as training data for the\ndiscriminator in the STGAN framework.\nGiven the classical queen\u2019s pawn opening move sequence described in the main results section, the\nChigorin style transfer model ( k = 1) responded with the move sequence shown in Figure 3. The\nclassical sequence of moves played in the Chigorin defense is shown in Figure 4. It can be seen that\nthe \ufb01nal position reached by the style transfer model represents a variation of the Chigorin defense,\nwhich seems to have been a result of the baseline model heavily favoring the \ufb01rst move g8f6 due to\nits representation in the master games used for training.\n6",
  "full_text_length": 15560,
  "link_pdf": "https://arxiv.org/pdf/1702.06762v2",
  "paper_id": "1702.06762v2"
}